\documentclass[DynamicalBook]{subfiles}
\begin{document}
%




\setcounter{chapter}{0}%Just finished 0.


%------------ Chapter ------------%
\chapter{Deterministic Systems}\label{chapter.1}

%-------- Section --------%
\section{Introduction}\label{sec.chap1_intro}

Here's a basic fact of life: \emph{things change}. And how things change most
often depends on how they currently are. This is the basic idea underlying all the various notions of \emph{dynamical
  system} that we will see in this book.

\begin{informal}\label{inf.dynam_sys}
  A \emph{dynamical system} consists of:
  \begin{itemize}
  \item a notion of how things can be, called the \emph{states}, and
  \item a notion of how things will change given how they are, called the \emph{dynamics}.
  \end{itemize}
  The dynamics of a system might also depend on some free \emph{parameters} or \emph{inputs} that are imported from the environment, and
  we will often be interested in some particular \emph{variables} of the
  state that are \emph{exposed} or \emph{output} to the environment. 
\end{informal}

You and I are big, complicated dynamical systems. Our bodies and minds are in
some particular configuration, and over time this configuration changes. We can
sense things --- seeing, touching, tasting --- and what we sense affects how our
bodies and mind changes. Seeing a scary snake can make me recoil and feel fear,
but seeing a cute snake plushie can make me go over and start to pet it.
Some parts of me are also put back into the environment, like the expression on
my face. But not all of me is exposed in that way --- some things just go on in
my head.

This is the basic model of a dynamical system we will be working with in this
book. But to make the above informal definition precise, we need to answer
a number of questions:
\begin{itemize}
  \item What should a state be, really? Do we just have an abstract set of
    states, or could there be a continuum of states? Maybe there are some other
    structures that states can enter into which have to be respected by the
    dynamics, but aren't determined by them? \jaz{With this last sentence, I'm
      thinking of ``states as polynomial comonad aka category''. Not sure how to
    phrase it right.}
  \item What does it mean to change? Do we want to know precisely which state
    will be next if we know how things are? Or, maybe we will only have a guess
    at which state will come next? Or, maybe we'll just say how a state is
    tending to change, but not where it will end up?
  \item Do we always take in the same sort of parameters, or does it depend on
    how our system is placed in its environment? Should the dynamics vary
    continuously (or linearly, or some other way) in the choice of parameters? 
\end{itemize}

Different people have decided on different answers to these questions for
different purposes. Here are some of the most widespread ways to answer those
questions:
\begin{enumerate}
  \item We'll assume the states form a discrete set, and that if we know the
    current state and our parameters, we know exactly what the next state will
    be. Such a system generally called a \emph{Moore machine}.
  \item We'll assume the states form a continuum, but that we only know how a
    state is tending to change, not what the ``next'' state will be. Such a
    system is generally called a \emph{system of
      differential equations} --- the differential equations tells us the way the
    derivatives of the state variables, which way they are tending.
  \item We'll assume the states form a discrete set, but that we only have a
    guess at which state will follow from the current state. Such a system is generally
    called a \emph{Markov process}, or a \emph{Markov decision process}.
\end{enumerate}

We will call a way of answering these questions the \emph{doctrine} of dynamical
systems we are working in.
\begin{informal}
  A \emph{doctrine} of dynamical systems is a particular way to answer the following
  questions about what it means to be a dynamical system:
  \begin{itemize}
  \item What does it mean to be a state?
  \item How should the output vary with the state --- discretely,
    continuously, linearly?
  \item Can the kinds of input a
    system takes in depend on what it's putting out, and how do they depend on it?
  \item What sorts of changes are possible in a given state?
  \item How should the way the state changes vary with the input?
  \end{itemize}
\end{informal}

Moore machines, differential equations, and Markov decision processes are each
dynamical systems understood in a different doctrine.
\begin{enumerate}
  \item A Moore machine is a dynamical system in a \emph{discrete and
      deterministic} doctrine.
  \item A system of differential equations is a dynamical system in a
    \emph{differential} doctrine.
  \item A Markov decision process is a dynamical system in a \emph{stochastic} doctrine.
\end{enumerate}

In most cases, mathematicians have assumed that that the kinds of parameters our systems take in
never change --- that our system will always interface with
    its environment in the same way. However, this assumption is quite
    restrictive; after all, I change the way I interface with my environment all
    the time. Every time I turn and face a new direction, I open myself up to
    new inputs. There are variations on all of the above doctrines which allow for the
    kinds of input to depend on what the system is putting out, but in this book
    we will take a deep dive into the discrete and deterministic variant.

\begin{enumerate}
  \item[4.] A \emph{dependent system} is a dynamical system in a
    (discrete, deterministic, and) \emph{dependent} doctrine.
\end{enumerate}

The dynamical systems we will see in this book are \emph{open} in the sense that
they take in inputs from their environment and expose outputs back to their
environment. Because of this, our systems can interact with eachother. One
system can take what the other system outputs as part of its input, and the
other can take what the first outputs as part of its input. For example, when we
have a conversation, I take what I hear from you and use it to change how I
feel, and from those feelings I generate some speech which I output to the
world. You then take what I've said and do the same thing.

\begin{center}
  \jaz{Some wiring diagram of a conversation}
\end{center}

We call this way of putting together dynamical systems to make more complex
systems \emph{composition}.
\begin{informal}
  \emph{Composition} is the process by which some things are brought together to
  form bigger things.
\end{informal}

Functions can be composed by $g \circ f(x) = g(f(x))$, and dynamical systems
can be composed by plugging in the variables of the states of some into the
parameters of others.
  
This book is all about composing dynamical systems.\jaz{We should also tell the
  story that categories for doing the mathematics of dependent systems --- ie
  you need category theory to talk about dependent systems effectively.} Because of this, we will use
the abstract language of composition: \emph{category theory}.
\begin{informal}
\emph{Category theory} is the abstract study of composition.
\end{informal}




%---- Subsection ----%
\subsection{Category Theory}

We'll be using the language of category theory quite freely in this book, and so
we'll expect you to know the basics. These are the notions we will expect you to
be familiar with:
\begin{itemize}
\item What a category is.
\item What an isomorphism is.
\item What a functor is.
\item What a natural transformation is.
\item What a terminal and an initial object are.
\item What a product and a coproduct are.
\item What a monad is, and it will help if you also know what a comomad is.
  \item What a monoidal category is.
\end{itemize}

Good introductions to category theory abound. One place to start is \emph{An invitation to applied category theory} \cite{fong2019seven}.

We will be using cartesian categories quite a bit in the first few chapters.
\begin{definition}\label{def.cartesian_category}
  A category $\cat{C}$ is \emph{cartesian} if every two objects $A$ and $B$ in
  $\cat{C}$ have a product $A \times B$, and $\cat{C}$ has a terminal object
  $\ord{1}$. Equivalently, $\cat{C}$ is cartesian if for any finite set $I$ and
  $I$-indexed family $A_{(-)} : I \to \cat{C}$ of objects, there is a product
  $\prod_{i \in I} A_i$ in $\cat{C}$.

  A functor $F : \cat{C} \to \cat{D}$ between cartesian categories is said to be
  \emph{cartesian} if it preserves products and terminal objects, i.e.\ the
  map $(F\pi_A,\, F\pi_B) : F(A \times B) \to FA \times FB$ is an isomorphism
  for all $A$ and $B$, and the terminal morphism $F\ord{1} \to \ord{1}$ is an
  isomorphism. 
\end{definition}

We will also use some more advanced category theory, like indexed
categories, double categories, and toposes. However, you don't need to know them up front; we will introduce these concepts
as we use them.

While we're at it, here's some notation we'll use repeatedly throughout the book. The $n$th ordinal is denoted $\ord{n}$. It is defined to be the set
\[
\ord{n}\coloneqq\{1,2,\ldots,n\}.
\]
So $\ord{0}$ is the empty set, $\ord{1}$ is a one-element set, etc.

%-------- Section --------%
\section{Deterministic systems}\label{sec.deterministic_system}

In this chapter, we will see the idea from \cref{inf.dynam_sys} in its
most distilled form: we will know exactly how things are, and exactly how they
will change. That is, we'll be working in the \emph{deterministic} doctrine of
dynamical systems,
for which time steps forward in \emph{discrete} increments.


A paradigmatic example of this sort of dynamical system is a clock.
\[
\begin{tikzpicture}[line cap=rect,line width=3pt]
\filldraw [fill=cyan] (0,0) circle [radius=2cm];
\foreach \angle [count=\xi] in {60,30,...,-270}
{
  \draw[line width=1pt] (\angle:1.8cm) -- (\angle:2cm);
  \node[font=\large] at (\angle:1.36cm) {\textsf{\xi}};
}
\foreach \angle in {0,90,180,270}
  \draw[line width=2pt] (\angle:1.6cm) -- (\angle:2cm);
\draw (0,0) -- (120:0.8cm);
\end{tikzpicture}
\]

Suppose that our clock has just an hour hand for now. Then we may collect all
the way things can be for the clock into a set of hours:
$$\Set{Hour} := \{1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12\}.$$
This set $\Set{Hour}$ is the set of \emph{states} of our clock system.

If we know what hour it is, we also know what hour is coming next. So, this system has the following dynamics:
%
% :CUSTOM-ID: problem-with-drawing-mapsto-nicely
%
\begin{align}\label{eqn.tick}
  \fun{tick} : \Set{Hour} &\to \Set{Hour} \\\nonumber
                t &\mapsto \begin{cases} t + 1 &\mbox{if $t < 12$}\\ 1 &\mbox{if $t = 12$}  \end{cases}
\end{align}

Here's a sample of the dynamics of the clock. Say we started at the 10 o'clock state:
$$10 \xmapsto{\fun{tick}} 11 \xmapsto{\fun{tick}} 12 \xmapsto{\fun{tick}} 1 \To{\fun{tick}} 2
\xmapsto{\fun{tick}} \cdots$$

Ok, it's not the most dynamic of systems, but we have to start somewhere. If we want to
refer to the whole system at once, we can box it up and draw it like this:

\begin{equation}\label{eqn.clock_system_box}
\begin{tikzpicture}[oriented WD, every fit/.style={inner xsep=\bbx, inner ysep=\bby}, bbx = 1cm, bby =.5cm, bb min width=1cm, bb port length=4pt, bb port sep=1, baseline=(X.center)]
	\node[bb={0}{1}, fill=blue!10] (X) {$\Sys{Clock}$};
	\draw[label] 
		node [right=2pt of X_out1] {$\Set{Hour}$}
		;
\end{tikzpicture}
\end{equation}
We imagine that the clock is going about its business inside the box, and
that is shows the hour it is currently displaying on the outgoing wire. This outgoing wire constitutes the clock's exposed variable, but we'll explain that more later.

One issue with our clock is that it doesn't tell us whether it is morning or
evening. Being morning or evening and going back and forth between them is another way that things might be and change, and hence we
can see it as its own two-state dynamical system with states
$$\Set{a.m./p.m.} = \{\const{a.m.}, \const{p.m.}\}.$$

However, rather than have this be an independent system, we want to consider it as a little addition to
our clock system, one that reads $\const{a.m.}$ or $\const{p.m.}$:
\begin{equation}\label{eqn.whole_clock}
\begin{tikzpicture}[line cap=rect,line width=3pt, baseline=(bl)]
\coordinate (bl) at (0,0);
\filldraw [fill=cyan] (0,0) circle [radius=2cm];
\foreach \angle [count=\xi] in {60,30,...,-270}
{
  \draw[line width=1pt] (\angle:1.8cm) -- (\angle:2cm);
  \node[font=\large] at (\angle:1.36cm) {\textsf{\xi}};
}
\foreach \angle in {0,90,180,270}
  \draw[line width=2pt] (\angle:1.6cm) -- (\angle:2cm);
\draw (0,0) -- (120:0.8cm);
\node[draw, fill] at (330:.7cm) {\small${ \color{white}\const{a.m.} }$};
\end{tikzpicture}
\end{equation}
To connect the meridian to the clock means that the way the meridian changes should be based on the hour:
\begin{align}\label{eqn.next}
  \fun{next} : \Set{a.m./p.m.} \times \Set{Hour} &\to \Set{a.m./p.m.} \\\nonumber
               (\const{a.m.}, t) &\mapsto \begin{cases} \const{p.m.} &\mbox{if $t = 11$}\\ \const{a.m.} &\mbox{otherwise}  \end{cases} \\\nonumber
               (\const{p.m.}, t) &\mapsto \begin{cases} \const{a.m.} &\mbox{if $t = 11$}\\ \const{p.m.} &\mbox{otherwise}  \end{cases}
\end{align}
If it is $\const{a.m.}$ and the clock reads 8, then it will still be
$\const{a.m.}$ at the next tick; but if it is $\const{a.m.}$ and the clock reads 11, then the next tick will switch the meridian to $\const{p.m.}$.

Again, the thing to note about the dynamics of the $\const{a.m.}/\const{p.m.}$ system
is that they depend on what hour it is. The hour is imported as a \emph{parameter} for the
dynamics of the meridian system. We can draw the meridian system as a box like this:
\begin{equation}\label{eqn.am_pm_system_box}
\begin{tikzpicture}[oriented WD, every fit/.style={inner xsep=\bbx, inner ysep=\bby}, bbx = 1cm, bby =.5cm, bb min width=1cm, bb port length=4pt, bb port sep=1, baseline=(X.center)]
	\node[bb={1}{1}, fill=blue!10] (X) {$\Sys{Meridian}$};
	\draw[label] 
		node [left=2pt of X_in1] {$\Set{Hour}$}
		node [right=2pt of X_out1] {$\Set{a.m./p.m.}$}
		;
\end{tikzpicture}
\end{equation}
We have the $\Set{a.m./p.m.}$ wire coming out, which carries the information of
whether it is $\const{a.m.}$ or $\const{p.m.}$, just like the clock. But we also
have a wire coming in, which carries the hour that we need as a parameter for
our dynamics.


We can now express our whole clock \eqref{eqn.whole_clock} by wiring together
our bare clock \eqref{eqn.clock_system_box} and the $\const{a.m.}/\const{p.m.}$ system:

\begin{equation}\label{eqn.whole_clock_system_box}
\begin{tikzpicture}[oriented WD, every fit/.style={inner xsep=\bbx, inner ysep=\bby}, bbx = .3cm, bby =.3cm, bb min width=.5cm, bb port length=2pt, bb port sep=1, baseline=(Y.center)]
	\node[bb={1}{1}, fill=blue!10] (X1) {$\Sys{Meridian}$};
  	\node[bb={0}{1}, fill=blue!10, below=2 of X1] (X2) {$\Sys{Clock}$};
	\node[bb={0}{2}, fit={($(X1.north west)+(-2,1)$) ($(X1.north east)+(2,1)$) ($(X2.south)+(0,-3)$)}] (Y) {};
  \node[above=0pt of Y.south] (Label) {$\Sys{ClockWithDisplay}$};
	\draw (X1_out1) to (Y_out1);
  \draw let \p1=(X1.south west), \p2=(X2.north east), \n1=\bbportlen, \n2=\bby in
    (X2_out1) to[in=0] (\x2 + \n1, \y2 + \n2) -- (\x1 - \n1, \y2 + \n2) to[out=180] (X1_in1);
  \draw (X2_out1) to (Y_out2);
	\draw[label] 
		node [right=2pt of Y_out1] {$\Set{a.m./p.m.}$}
		node [right=2pt of Y_out2] {$\Set{Hour}$}
		;
\end{tikzpicture}
\end{equation}

The resulting system has states
$$\Set{HoursWithDisplay} := \Set{Hour} \times \Set{a.m./p.m.}$$
each of which is a pair, e.g.\ $(11, \const{a.m.})$, consisting of an hour and a meridian reading.
They update in a combined way, by using the hour shown on the clock face as the
parameter we need for the $\Sys{Meridian}$ system; this is expressed by having a wire from the output of $\Sys{Clock}$ to the input of $\Sys{Meridian}$. In full, the
dynamics looks like this:
\begin{align*}
  \fun{tick'} :\Set{HoursWithDisplay} &\to \Set{HoursWithDisplay} \\
  (t, m) &\mapsto (\fun{tick}(t), \fun{next}(t, m))
\end{align*}
where $\fun{tick}$ and $\fun{next}$ are as in \eqref{eqn.tick} and \eqref{eqn.next}.

\begin{exercise}
  Expand the definition of the combined system out in full, and check that it
  really does behave like the clock with $\const{a.m.}/\const{p.m.}$ display should.
\end{exercise}

Now that we have a working clock, we can use it for systems that need to know
the time. For example, consider a diner that opens at $7 \const{a.m.}$ and
closes at $10 \const{p.m.}$. The states of this diner are
$$\Set{DinerState} = \{\const{open}, \const{closed}\}.$$
The diner's dynamics are then
\begin{align*}
  \fun{dinerDynamics} : \Set{DinerState} \times \Set{HoursWithDisplay} &\to \Set{DinerState} \\
  (\const{open}, (10, \const{p.m.})) &\mapsto \const{closed} \\
  (\const{closed}, (7, \const{a.m.})) &\mapsto \const{open} \\
  (s, (t, m)) &\mapsto s \quad\mbox{otherwise.} 
\end{align*}

Again, we can represent the diner by this box:
\begin{equation}\label{eqn.diner_system_box}
\begin{tikzpicture}[oriented WD, every fit/.style={inner xsep=\bbx, inner ysep=\bby}, bbx = 1cm, bby =.5cm, bb min width=1cm, bb port length=4pt, bb port sep=1, baseline=(X.center)]
	\node[bb={2}{1}, fill=blue!10] (X) {$\Sys{Diner}$};
	\draw[label] 
		node [left=2pt of X_in1] {$\Set{a.m./p.m.}$}
		node [left=2pt of X_in2] {$\Set{Hour}$}
		node [right=2pt of X_out1] {$\Set{DinerState}$}
		;
\end{tikzpicture}
\end{equation}
This time, we have two wires coming in, corresponding to the two parameters we
need for the diner system: the hour and the
meridien. 

Assuming that the diner has a clock on its wall which it uses to decide whether
to open or close, the full diner system would be given by wiring the clock with display into
those input wires:
\begin{equation}\label{eqn.diner_system_box}
\begin{tikzpicture}[oriented WD, every fit/.style={inner xsep=\bbx, inner ysep=\bby}, bbx = 1cm, bby =.5cm, bb min width=1cm, bb port length=4pt, bb port sep=1, baseline=(Outer.center)]
  \node[bb={0}{2}, fill=blue!10](Clock) {$\Sys{ClockWithDisplay}$};
  \node[bb={2}{1}, fill=blue!10, right=of Clock] (Diner) {$\Sys{Diner}$};
  
  \node[bb={0}{1}, fit={(Clock) (Diner)}] (Outer) {};

  \draw (Clock_out1) to (Diner_in1);
  \draw (Clock_out2) to (Diner_in2);
  \draw (Diner_out1) to (Outer_out1);

  \draw[label] node [right=2pt of Outer_out1] {$\Set{DinerState}$};
\end{tikzpicture}
\end{equation}
If we want to, we can peak into the clock with display and see that it is itself
made out of a clock wired to a display:
\begin{equation}\label{eqn.diner_system_box}
\begin{tikzpicture}[oriented WD, every fit/.style={inner xsep=\bbx, inner ysep=\bby}, bbx = 1cm, bby =.3cm, bb min width=1cm, bb port length=4pt, bb port sep=1, baseline=(Outer.center)]
	\node[bb={1}{1}, fill=blue!10] (X1) {$\Sys{Meridian}$};
  	\node[bb={0}{1}, fill=blue!10, below=2 of X1] (X2) {$\Sys{Clock}$};
	\node[dashed, bb={0}{2}, fit={($(X1.north west)+(0,1)$) ($(X1.north east)+(0,1)$) ($(X2.south)+(0,-3)$)}] (Clock) {};
  \node[above=0pt of Clock.south] (Label) {$\Sys{ClockWithDisplay}$};
	\draw (X1_out1) to (Clock_out1);
  \draw let \p1=(X1.south west), \p2=(X2.north east), \n1=\bbportlen, \n2=\bby in
    (X2_out1) to[in=0] (\x2 + \n1, \y2 + \n2) -- (\x1 - \n1, \y2 + \n2) to[out=180] (X1_in1);
  \draw (X2_out1) to (Clock_out2);
  \node[bb={2}{1}, fill=blue!10, right=of Clock] (Diner) {$\Sys{Diner}$};
  
  \node[bb={0}{1}, fit={(Clock) (Diner)}] (Outer) {};

  \draw (Clock_out1) to (Diner_in1);
  \draw (Clock_out2) to (Diner_in2);
  \draw (Diner_out1) to (Outer_out1);

  \draw[label] node [right=2pt of Outer_out1] {$\Set{DinerState}$};
\end{tikzpicture}
\end{equation}

These examples are simple, but it doesn't take much more to get to some truly
amazing phenomena. Consider this system: we have an infinite tape with a
read-head at some integer position. On this infinite tape, we will write the symbols
$a$, $b$, $c$, or $d$, or we will leave it blank: $\_$. Together, the state
of the tape and the position of the read-head have states pairs $(\fun{T}, n)$ consisting of a
function $\fun{T} : \zz \to \{a, b,c,d,\_\}$, telling us what symbol $\fun{T}(i)$ is found at
position $i$ of the tape, and a position $n$ of the read-head:
\begin{align*}
  \Set{Symbol} &= \{a, b,c,d,\_\}\\
  \Set{Tape} &= \Set{Symbol}^{\zz}\\
  \Set{Head} &= \zz
\end{align*}
The parameters that this system needs in order to change are a move-command and a
write-command. The move-command will be either move left or move right, encoded as $-1$ or $1$ respectively, and the write command will be one of the
symbols that can be written on the tape:
\[
  \Set{Move}=\{-1,1\}\qqand
  \Set{Write} = \{a,b,c,d,\_\}.
\]

The way this system changes is by writing the write command to the tape at the current
position, and then moving according to the move command. As a function, this is:
\begin{align*}
  \fun{execute} : \Set{Head} \times \Set{Tape} \times \Set{Move} \times \Set{Write} &\to \Set{Head} \times \Set{Tape}\\
  (n, i\mapsto \fun{T}(i), d, s) &\mapsto \left( n + d, i \mapsto \begin{cases} \fun{T}(i) &\mbox{if $i \neq n$} \\ s &\mbox{if $i = n$} \end{cases} \right).
\end{align*}

We can imagine that the system exposes the tape and the symbol under its read head. We can box this system up and draw it like so:
\begin{equation}\label{eqn.turing_box}
\begin{tikzpicture}[oriented WD, every fit/.style={inner xsep=\bbx, inner ysep=\bby}, bbx = 1cm, bby =.5cm, bb min width=1cm, bb port length=4pt, bb port sep=1, baseline=(Tape.center)]
\node[bb={2}{2}, fill=blue!10] (Tape) {$\Sys{Tape Machine}$};
\draw[label]
  node[left=2pt of Tape_in1] {$\Set{Move}$}
  node[left=2pt of Tape_in2] {$\Set{Write}$}
  node[right=2pt of Tape_out1] {$\Set{Tape}$}
  node[right=2pt of Tape_out2] {$\Set{Symbol}$}
;
\end{tikzpicture}
\end{equation}

Now, we need one more simple ingredient to get our system going; a mysterious system of the
form:
\begin{equation}\label{eqn.turing_box2}
\begin{tikzpicture}[oriented WD, every fit/.style={inner xsep=\bbx, inner ysep=\bby}, bbx = 1cm, bby =.5cm, bb min width=1cm, bb port length=4pt, bb port sep=1, baseline=(Box.center)]
\node[bb={1}{2}, fill=blue!10] (Box) {$\Sys{Mystery Box}$};
\draw[label]
  node[right=2pt of Box_out1] {$\Set{Move}$}
  node[right=2pt of Box_out2] {$\Set{Write}$}
  node[left=2pt of Box_in1] {$\Set{Symbol}$}
;
\end{tikzpicture}
\end{equation}

We can see that our mystery box will take in a symbol and put out a move command
and a write command. The way our mystery box behaves is rather mysterious. It has six states $S\cong\6$, and its update rule is given by the following table,
where the entry in the row $i$ and the column $s$ is written $(m,w):s'$ to express the
move command $m$, the write command $w$, and the next state $s'$ that our
mysterious system transitions to when input the symbol $i$ in state $s$:
\begin{equation}\label{eqn.turing_machine}
  \begin{tabular}{|c|c|c|c|c|c|c|}
    \hline
     & 1 & 2 & 3 & 4 & 5 & 6 \\\hline
    a & (-1, b):1 & (1, a):1 & (-1, b):3  & (1, b):2 & (-1, b):6 & (-1, b):4 \\\hline     
   b & (-1, a):1 & (1, a):2 & (-1, b):5  & (1, a):4 & (1, a):6 & (1, a): 5 \\\hline     
   c & (1, d):2 & (1, d):2 & (-1, c):5 & (1,d):4 & (1, c):5 & (1, a):1 \\\hline     
   d & (-1, c):1 & (1, a):5 & (-1, c):3 & (1,d):5 & (-1, b):3 & end \\\hline     
  \end{tabular}
\end{equation}
Mysterious indeed. But when we wire the two together, magic happens!

\begin{equation}\label{eqn.full_turing_machine}
\begin{tikzpicture}[oriented WD, every fit/.style={inner xsep=\bbx, inner ysep=\bby}, bbx = 1cm, bby =.5cm, bb min width=1cm, bb port length=2pt, bb port sep=1, baseline=(Outer.center)]
  \node[bb={1}{2}, fill=blue!10](Box) {$\Sys{Mystery Box}$};
  \node[bb={2}{2}, fill=blue!10, right=of Box] (Tape) {$\Sys{Tape Machine}$};
  
  \node[bb={0}{0}, fit={($(Tape.south east) + (0, -2)$) ($(Box.north west) + (0, 1)$)}] (Outer) {};
  \node[above=0pt of Outer.south] (Label) {$\Sys{Universal Turing Machine}$};

  \draw (Box_out1) to (Tape_in1);
  \draw (Box_out2) to (Tape_in2);

  \draw (Tape_out1) to (Outer.east|-Tape_out1);
  \draw (Tape_out2) to (Outer.east|-Tape_out2);

  \draw let \p1=(Tape.south east), \p2=(Box.south west), \n1=\bbportlen, \n2=\bby in
    (Tape_out2) to[in=0] (\x1 + \n1, \y1 - \n2 - 1) -- (\x2 - \n1, \y2 - \n2 - 1) to[out=180] (Box_in1);
\end{tikzpicture}
\end{equation}
This is a universal Turing machine, i.e.\ when we encode everything into this strange alphabet, it is capable of arbitrarily complex calculation! 
\slogan{Even simple systems can
have very interesting behavior when plugged in to the right environment.}

%%
%% :CUSTOM_ID:cite-this.turing_machine
%% 
That's a lot of informal definitions, we are ready for something precise:
\begin{definition}\label{def.deterministic_system}
  A \emph{deterministic system} $\Sys{S}$, also written as 
  $$\lens{\update{S}}{\expose{S}} : \lens{\State{S}}{\State{S}} \leftrightarrows \lens{\In{S}}{\Out{S}},$$ 
  consists of:
  \begin{itemize}
    \item a set $\State{S}$ of \emph{states};
    \item a set $\Out{S}$ of \emph{values for exposed variables}, or \emph{outputs}
      for short;
    \item a set $\In{S}$ of \emph{parameter values}, or \emph{inputs} for short;
    \item a function $\expose{S} : \State{S} \to \Out{S}$, the \emph{exposed variable of state} or
      \emph{expose} function, which takes a state to the output it yields; and
    \item a function $\update{S} : \State{S} \times \In{S} \to \State{S}$, the \emph{dynamics} or
      \emph{update} function which takes a state and a parameter and gives the
      next state.
  \end{itemize}
  We refer to the pair $\lens{\In{S}}{\Out{S}}$ of exposed variable and parameter values as
  the \emph{interface} of the system.

We can interpret this definition in any cartesian category $\cat{C}$; here, we
have have used the category $\Cat{Set}$ of sets.
\end{definition}

\begin{remark}
  Deterministic systems are also known as \emph{Moore machines} in the
  literature. If the output set is taken to be $\{\const{true},
  \const{false}\}$, then they are known as \emph{deterministic automata}.

  Often, these definitions also include a \emph{start state} $s_0 \in \State{S}$
  as part of the data. We don't do this.
\end{remark}

\begin{example}\label{ex.clock_system}
  The $\Sys{Clock}$ system can be seen as a deterministic system with:
  $$\lens{\fun{tick}}{\id} : \lens{\Set{Hour}}{\Set{Hour}} \fromto \lens{\{\ast\}}{\Set{Hour}}.$$
  In other words, it consists of
  \begin{itemize}
  \item State set $\State{Clock} = \Set{Hour} =\{1,2,\ldots,12\}$.
  \item Output set $\Out{Clock} = \Set{Hour}$.
  \item Input set $\In{Clock} = \{\ast\}$, a one element set.
  \item Readout function $\expose{Clock} = \id_{\Set{Hour}}$.
  \item update function $\update{Clock} : \Set{hours} \times \{\ast\} \to \Set{Hour}$
    defined by $\update{Clock}(t, \ast) = \fun{tick}(t)$.
  \end{itemize}
\end{example}

\begin{example}\label{ex.moore_machine}
  The term \emph{Moore machine} is often used for the mathematical notion of
  deterministic system we've just presented, but it is also used for actual,
  real-life circuits which are designed on that principle.

  For example, suppose that a wire carries the signals $\Set{Wire} =
  \{\const{high}, \const{low}\}$. We can see a deterministic system $\Sys{M}$
  with input $\In{M} = \Set{Wire}^n$ and $\Out{M} = \Set{Wire}^k$ as a circuit
  with $n$ incoming wires and $k$ outgoing wires.\footnote{Of course, the notion
  of ``incoming'' and ``outgoing'' wires are ways we think about the circuit in
  design terms. Circuits aren't actually directed in this way. We'll think about
undirected notions of system in \cref{chapter.2}.} The state then describes the
state of all the internal wires (and capacitors, etc.) in the circuit. We would wire up these systems by literally wiring them together. 

\jaz{I would like to add an example of an implementation of a Moore machine into
  a circuit.}
%%
%%  :CUSTOM-ID:moore-machine-example
%%
\end{example}

Note that when we say that a system doesn't have any parameters, as in \cref{ex.clock_system}, we don't take the
parameter set to be empty but instead take it to have a single dummy value $\{*\}$, the one-element ``hum of existence''. In other words, having ``no parameters'' really means that the parameters are unchanging, or that there is no way to change the value of the parameters.

Also, we are just exposing the whole state with the system in \cref{ex.clock_system}. There is nothing preventing our systems
from exposing their whole state, but often some aspects of the state are private, i.e.\ not exposed for use by other systems. 

\begin{exercise}\label{exc.clock_meridian_diner}
  Write out the clock and meridian systems from \eqref{eqn.tick} and \eqref{eqn.next} in terms of
  \cref{def.deterministic_system}. Really, this amounts to noticing which sets
  are the sets of states, which are the sets of inputs, and what (implicitly)
  are the sets of outputs.
\end{exercise}

\begin{example}[SIR model]\label{ex.SIR_model_discrete}
  The set of states for a deterministic system doesn't need to be finite. The $\Sys{SIR}$ model
  is an epimediological model used to study how a disease spreads through a
  population. ``SIR'' stands for ``susceptible'', ``infected'', and, rather
  ominously, ``removed''. This model is usually presented as a system of
  differential equations --- what we will call a differential system --- and we will see it in that form in the next chapter.
  But we can see a discrete approximation to this continuous model as a
  deterministic system.

  A state of the $\Sys{SIR}$ model is a choice of how many people are susceptible, how
  many are infected, and how many are removed. That is,
  $$\State{SIR} = \left\{\begin{bmatrix} s \\ i \\ r \end{bmatrix}\; \middle|\; s,\, i,\, r\in \rr\right\} \cong \rr^3.$$
  is a 3-place vector of real numbers. We will again expose the whole state, so
  $\Out{SIR} = \State{SIR}$ and $\expose{SIR} = \id$.

  The idea behind the $\Sys{SIR}$ model is that if a susceptible person comes in
  contact with an infected person, then they have a chance of becoming infected
  too. And, eventually, infected persons will be removed from the model, either
  by recovering (a gentler way to read the ``R'') or by dying. So we need two
  parameters: the rate $a$ of infection and the rate $b$ of removal:
  $$\In{SIR} = \left\{\begin{bmatrix} a \\ b \end{bmatrix}\, \middle|\, a,\, b \in \rr\right\} = \rr^2.$$

  Now, we can show how a population will develop according to this model by
  defining the update function:
  \begin{align}\label{eqn.SIR_model_discrete}
    \update{SIR} : \State{SIR} \times \In{SIR} &\to \State{SIR}\\
    \left( \begin{bmatrix} s\\ i\\ r\\ \end{bmatrix},\, \begin{bmatrix}a \\ b \end{bmatrix} \right) &\mapsto \begin{bmatrix}s - a s i \\ i + a s i - b  i \\ r + b i \end{bmatrix}
  \end{align}

\end{example}

\begin{example}\label{ex.transition_diagram_discrete}
  If a deterministic system has a small finite set of states, then we can draw it
entirely as a \emph{transition diagram}:

\[
\begin{tikzpicture}
	\node[draw] {
  \begin{tikzcd}[column sep=small]
  	\LMO{a}\ar[rr, dgreen, thick, bend left]\ar[loop left, thick, orange]&&
  	\LMO{b}\ar[ll, thick, orange, bend left]\ar[dl, bend left, thick, dgreen]\\&
  	\LMO{b} \ar[ul, thick, orange, bend left] \ar[loop left, thick, dgreen]
  \end{tikzcd}
  };
\end{tikzpicture}
\]
Note that every node has an orange and a green arrow emanating from it, but that there are no rules on how many arrows point to it.

This diagram describes the following system $\Sys{S}$:
  $$\lens{\update{S}}{\expose{S}} : \lens{\{1, 2, 3\}}{\{1, 2, 3\}} \leftrightarrows \lens{\{{\color{dgreen} \const{green},\, {\color{orange} \const{orange}}}\}}{\{a, b\}}.$$ 
That is, we have
\begin{itemize}
\item $\State{S} = \{1, 2, 3\}$.
\item $\In{S} = \{{\color{dgreen} \const{green},\, {\color{orange} \const{orange}}}\}$,
\item $\Out{S} = \{a, b\}$,
\item \[\begin{aligned}
        \expose{S} : \State{S} &\to \Out{S} \\
        1 &\mapsto a \\
        2 &\mapsto b \\
        3 &\mapsto b \\~\\~\\~\\
      \end{aligned} \quad\quad\quad\quad
 \begin{aligned}
        \update{S} : \State{S} \times \In{S} &\to \In{S} \\
        (1, {\color{dgreen} \const{green}}) &\mapsto 2 \\
        (1, {\color{orange} \const{orange}}) &\mapsto 1 \\
        (2, {\color{dgreen} \const{green}}) &\mapsto 3 \\
        (2, {\color{orange} \const{orange}}) &\mapsto 1 \\
        (3, {\color{dgreen} \const{green}}) &\mapsto 3 \\
        (3, {\color{orange} \const{orange}}) &\mapsto 1
      \end{aligned}
      \]
\end{itemize}

To draw a transition diagram of a system $\Sys{S}$, we draw each state $s \in
\State{S}$ as a bubble filled with the label $\expose{S}(s)$, and for each
parameter $i \in \In{S}$ we draw an arrow from $s$ to $\update{S}(s, i)$. For a
diagram like this to be a transition diagram, every node must have an edge
leaving it for each parameter.
\end{example}

\begin{exercise}
  Draw the $\Sys{Clock}$ system (\cref{ex.clock_system}) as a transition diagram.
\end{exercise}

\begin{example}[Deterministic Finite Automata]
  A \emph{deterministic finite automaton} (DFA) is a simple model of computation.
  Given our definition of deterministic system, DFAs are easy enough to define:
  they are just the deterministic systems whose output values are either
  $\const{true}$ or $\const{false}$. 

  This means that the exposed variable of state $\expose{S} : \State{S} \to
  \{\const{true},\, \const{false}\}$ is a boolean valued function. We say a
  state $s$ is an \emph{accept state} if $\expose{S}(s) = \const{true}$, and a
  \emph{reject state} if $\expose{S}(s) = \const{false}$.

  The idea is that a DFA is a question answering machine. Given a starting state
  $s_0$ and a sequence of input values $i_1, \ldots, i_n$, we get a sequence of
  states by $s_{t+1}:= \update{S}(s_t, i_t)$. The answer to the question is
  ``yes'' if $s_n$ is an accept state, and ``no'' if $s_n$ is a reject state.

\end{example}

There is an important special case of deterministic systems which appear very
commonly in the literature: the \emph{closed} systems. These are the systems
which have no parameters, and which expose no variables. They are closed off
from their environment, and can't be wired into any other systems.

As mentioned after \cref{ex.clock_system}, when we say ``no'' in this way --- no parameters, no variables --- we should be
careful with what we mean exactly. We mean that there is no \emph{variation} in
the parameters or variables, that they are trivial. That is, we make the
following definition.
\begin{definition}
  A deterministic system $\Sys{S}$ is \emph{closed} if both $\In{S}$ and
  $\Out{S}$ have only one element
  \[\In{S}\cong\{*\}\cong\Out{S}.\]
\end{definition}

\begin{exercise}
Show that to give a closed system
\[\lens{\update{S}}{\expose{S}}\colon\lens{\State{S}}{\State{S}}\fromto\lens{\{*\}}{\{*\}},\]
one just needs to choose a set $\State{S}$ and an update function $\update{S} :  \State{S} \to \State{S}$.
\end{exercise}

\begin{exercise}
What would happen to a system $\Sys{S}$ if its set of parameters or output values were actually empty sets? Let's find out.
\begin{enumerate}
	\item Suppose $\In{S}=\varnothing$. Explain the content of a deterministic system \[\lens{\update{S}}{\expose{S}}\colon\lens{\State{S}}{\State{S}}\fromto\lens{\varnothing}{\{*\}}.\]
	\item Suppose $\Out{S}=\varnothing$. Explain the content of a deterministic system \[\lens{\update{S}}{\expose{S}}\colon\lens{\State{S}}{\State{S}}\fromto\lens{\{*\}}{\varnothing}.
	\qedhere
\]	
\end{enumerate}
\end{exercise}


%-------- Section --------%
\section{Wiring together systems with lenses}\label{sec.wiring_discrete_systems}

In the last section, we saw the formal definition of deterministic systems and a
few examples of them. In this section, we'll see how to wire systems together---as we did in \cref{sec.chap1_intro} for the clock and the universal Turing machine---to
make more complex systems. We will do this using an interesting notion coming
from the world of function programming: a \emph{lens}.

%---- Subsection ----%
\subsection{Lenses and lens composition}\label{sec.lens_discrete}

A lens is a framework for bi-directional information passing.

\begin{definition}\label{def.lens}
  A \emph{lens} $\lens{f^{\sharp}}{f} : \lens{A^-}{A^+} \leftrightarrows
  \lens{B^-}{B^+}$ in a cartesian category $\cat{C}$ consists of:
  \begin{itemize}
  \item A \emph{passforward} map $f : A^+ \to B^+$, and
    \item a \emph{passback} map $f^{\sharp} : A^+ \times B^- \to A^-$.
  \end{itemize}
\end{definition}

The most useful thing about lenses is that they \emph{compose}.
\begin{definition}\label{def.lens_composition}
  Let $\lens{f^{\sharp}}{f} : \lens{A^-}{A^+} \leftrightarrows \lens{B^-}{B^+}$ and
  $\lens{g^{\sharp}}{g} : \lens{B^-}{B^+} \leftrightarrows \lens{C^-}{C^+}$ be lenses in
  a cartesian category $\cat{C}$. We define their composite
  $$\lens{g^{\sharp}}{g} \circ \lens{f^{\sharp}}{f}$$
  to have passforward $g \circ f$ and passback
  $$(a^+, c^-) \mapsto f^{\sharp}\left(a^+, g^{\sharp}(f(a^+), c^-)\right).$$
\end{definition}
Here's a picture so that you can see the information flow for the composite of lenses:%
\footnote{We draw this with a different style---green boxes, etc.---so that the reader will not confuse it with our usual wiring diagrams for systems. These are not dynamic in any way; everything below is just a set.}
\begin{equation}\label{eqn.lens_drawing}
\begin{tikzpicture}[oriented WD, bb small, bb port length=5pt, baseline=(f)]
	\node[bb={1}{1}, rounded corners=5pt, draw=dgreen] (f) {$f$};
	\node[bb={1}{1}, rounded corners=5pt, draw=dgreen, right=1.5 of f] (g) {$g\vphantom{f}$};
	\node[left=0 of f_in1] {$A^+$};
	\node[right=0 of g_out1] {$C^+$};
	\draw (f_out1) to node[above, font=\scriptsize] {$B^+$} (g_in1);
%
	\node[dot, right=6 of g] (dot) {};
	\draw (dot.180) to +(180:5pt) coordinate (dot_in1) node[left] {$A^+$};
	\node[bb={1}{1}, rounded corners=5pt, draw=dgreen] at ($(dot)+(1.5,-1.5)$) (f) {$f$};
	\node[bb={2}{1}, rounded corners=5pt, draw=dgreen, below right=-1 and 2 of f] (g') {$g\shp$};
	\draw (dot) to[out=-60, in=180] (f_in1);
	\draw (f_out1) to node[above=5pt, pos=.6, font=\scriptsize] {$B^+$} (g'_in1);
	\node[bb={2}{1}, rounded corners=5pt, draw=dgreen, above right=-1 and 2 of g'] (f') {$f\shp$};
	\draw (dot) to [out=60, in=180] (f'_in1);
	\draw (g'_out1) to[out=0, in=180] node[below=5pt, font=\scriptsize] {$B^-$} (f'_in2);
	\draw (g'_in2) to (g'_in2-|dot_in1) node[left] {$C^-$};
	\node[right=0 of f'_out1] {$A^-$};
\end{tikzpicture}
\end{equation}

This gives us a category of lenses in any cartesian category $\cat{C}$.

\begin{definition}\label{def.lens_category}
Let $\cat{C}$ be a cartesian category. Then the category $\Cat{Lens}_{\cat{C}}$
has:
\begin{itemize}
\item as objects, the pairs $\lens{A^-}{A^+}$ of objects in $\cat{C}$, which we will
  call \emph{arenas}.
\item as morphisms, the lenses $\lens{f^{\sharp}}{f} : \lens{A^-}{A^+} \leftrightarrows \lens{B^-}{B^+}$.
\item The identity lens is $\lens{\pi_2}{\id} : \lens{A^-}{A^+} \leftrightarrows
  \lens{A^-}{A^+}$, where $\pi_2 : A^+ \times A^- \to A^-$ is the projection.
\end{itemize}
\item Composition is given by lens composition as in \cref{def.lens_composition}.
\end{definition}

\begin{remark}
  The category of lenses is special among categories because it is named for its
  \emph{maps} (which are the lenses), rather than its objects (which are the
  arenas). This is because we will later meet another category, the
  \emph{category of charts} (See \cref{def.category_of_charts}), whose objects are
  the arenas but whose maps are not lenses. Finally, in
  \cref{def.double_category_of_arenas_discrete} we will meet a \emph{double
    category}\footnote{A double category is like a category with two different
    kinds of morphisms and a way for them to commute. See
    \cref{def.double_category} for the precise definition and the accompanying
    discussion.} $\Cat{Arena}_\cat{C}$ which combines these two categories whose objects
  are arenas and which \emph{is} named after its objects. In
  \cref{sec.double_category_of_arenas}, we will explain the name ``arena'' and
  its role in the theory of dynamical systems.
\end{remark}

\begin{exercise}
  \begin{enumerate}
  	\item Draw the composite of two lenses in the style of \eqref{eqn.lens_drawing}.
   	\item Check that $\Cat{Lens}_{\cat{C}}$ is actually a category. That is, check that
    lens composition is associative, and that the identity lens is an identity for it.
  \qedhere
  \end{enumerate}
\end{exercise}

Like any good categorical construction, $\Cat{Lens}_{\cat{C}}$ varies
functorially in its variable cartesian category $\cat{C}$.
\begin{proposition}[Functoriality of $\Cat{Lens}$]\label{prop.lens_functoriality}
  Every cartesian functor $F : \cat{C} \to \cat{D}$ induces a functor
  $\lens{F}{F} : \Cat{Lens}_{\cat{C}} \to \Cat{Lens}_{\cat{D}}$ given by
  $$\lens{F}{F}\lens{f^{\sharp}}{f} = \lens{Ff^{\sharp} \circ \mu\inv}{Ff}$$
  where $\mu = (F\pi_1,\, F\pi_2) : F(X \times Y) \xto{\sim} FX \times FY$ is
  the isomorphism witnessing that $F$ preserves products.
\end{proposition}
\begin{proof}[Proof Sketch.]
  Because lenses are defined just using the cartesian product, and $F$ preserves
  these products, it commutes with everything in sight.
\end{proof}

\begin{exercise}
 \begin{enumerate}
 	\item What does the functor $\lens FF: \Cat{Lens}_{\cat{C}} \to \Cat{Lens}_{\cat{D}}$ do on objects?
	\item Complete the proof of \cref{prop.lens_functoriality}, by showing that $\lens FF$ really is a functor.
	\qedhere
\end{enumerate}
\end{exercise}

%---- Subsection ----%
\subsection{Deterministic systems as lenses}

The reason we are interested in lenses and lens composition is because
deterministic systems are themselves lenses. As written in
\cref{def.deterministic_system}, a system $\Sys{S}$ is a lens
$$\lens{\update{S}}{\expose{S}} : \lens{\State{S}}{\State{S}} \leftrightarrows \lens{\In{S}}{\Out{S}}.$$
In fact, the deterministic systems are precisely the lenses whose input arena is of the form $\lens SS$. This means that we can compose
a system $\Sys{S}$ with a lens $\lens{f^{\sharp}}{f} : \lens{\In{S}}{\Out{S}}
\leftrightarrows \lens{I}{O}$ to get a new dynamical system
$$\lens{f^{\sharp}}{f} \circ \lens{\update{S}}{\expose{S}} :
\lens{\State{S}}{\State{S}} \leftrightarrows \lens{I}{O}$$
with a new interface!

We can use this observation to wire together different systems. We separate this into to phases: first we put two systems in parallel, then we wire them together using a lens. The first phase, combine two systems without having them interact, is achieved through what we call the \emph{parallel product} and denote $\otimes$. Two put two arenas $\lens{A_1}{B_1}$ and $\lens{A_2}{B_2}$ in parallel we just take their product in our cartesian category $\cat{C}$:
\[
\lens{A_1}{B_1}\otimes\lens{A_2}{B_2}\coloneqq\lens{A_1\times A_2}{B_1\times B_2}
\]
In \cref{def.lens_parallel} we define parallel product for morphisms in $\Cat{Lens}$, i.e.\ for general lenses.

\begin{definition}\label{def.lens_parallel}
  For lenses $\lens{f^{\sharp}}{f} : \lens{A_1}{B_2} \leftrightarrows \lens{C_1}{D_1}$ and
  $\lens{g^{\sharp}}{g} : \lens{A_2}{B_2} \leftrightarrows \lens{C_2}{D_2}$, we
  define their \emph{parallel product} $$\lens{f^{\sharp}}{f} \otimes
  \lens{g^{\sharp}}{g} : \lens{A_1 \times A_2}{B_1 \times B_2} \leftrightarrows
  \lens{C_1 \times C_2}{D_1 \times D_2}$$
  to have passforward $f \times g$ and passback
  $$((b_1, b_2), (c_1, c_2)) \mapsto (f^{\sharp}(b_1, c_2), g^{\sharp}(b_2, c_2)).$$
  In terms of morphisms, this is
  $$(B_1 \times B_2) \times (C_1 \times C_2) \xto{\sim} (B_1 \times C_1) \times
  (B_2 \times C_2) \xto{f^{\sharp} \times g^{\sharp}} A_1 \times A_2.$$

  Together with $\lens{\ord{1}}{\ord{1}}$, this gives $\Cat{Lens}_{\cat{C}}$ the
  structure of a monoidal category.
\end{definition}

\begin{remark}
  We will show a slick way to prove that the parallel product does indeed make
  $\Cat{Lens}_{\cat{C}}$ into a monoidal category in \cref{sec.indexed_double_category_of_systems}.
\end{remark}

\begin{exercise}
Show the parallel product of morphisms as in \cref{def.lens_parallel} using the string diagram notation from \eqref{eqn.lens_drawing}.
\end{exercise}


\begin{proposition}\label{prop.lens_functoriality_monoidal}
Let $F : \cat{C} \to \cat{D}$ be a cartesian functor. The induced functor
$\lens{F}{F} : \Cat{Lens}_{\cat{C}} \to \Cat{Lens}_{\cat{D}}$ is strong monoidal
with respect to the parallel product.
\end{proposition}
\begin{proof}
  Since $F$ preserves products, and \jaz{finish}.
\end{proof}

Given two dynamical systems $\Sys{S_1}$ and $\Sys{S_2}$, their parallel product
$\Sys{S_1} \otimes \Sys{S_2}$ is defined explicitly as follows:
\begin{itemize}
\item $\State{S_1 \otimes S_2} := \State{S_1} \times \State{S_2}$.
\item $\Out{S_1 \otimes S_2} := \Out{S_1} \times \Out{S_2}$.
\item $\In{S_1 \otimes S_2} := \In{S_1} \times \In{S_2}$.
\item $\expose{S_1 \otimes S_2}((s_1,\, s_2)) = (\expose{S_1}(s_1),\, \expose{S_2}(s_2))$.
\item $\update{S_1 \otimes S_2}((s_1,\, s_2),\, (i_1,\, i_2)) =
  (\update{S_1}(s_1,\, i_1),\, \update{S_2}(s_2,\, i_2))$.
\end{itemize}

This can be expressed as the following wiring diagram:
\begin{equation}\label{eqn.parallel_product_diagram}
\begin{tikzpicture}[oriented WD, every fit/.style={inner xsep=\bbx, inner ysep=\bby}, bbx = .3cm, bby =.3cm, bb min width=.5cm, bb port length=2pt, bb port sep=1, baseline=(Y.center)]
	\node[bb={1}{1}, fill=blue!10] (X1) {$\Sys{S_1}$};
  \node[bb={1}{1}, fill=blue!10, below=1.5 of X1] (X2) {$\Sys{S_2}$};
	\node[bb={0}{0}, fit={($(X1.north west)+(-2,1)$) ($(X1.north east)+(2,1)$) ($(X2.south)+(0,-2)$)}] (Y) {};
  \node[above=0pt of Y.south] (Label) {$\Sys{S_1 \otimes S_2}$};
  
  \draw[shorten <=-3pt] (X1_in1-|Y.west) to (X1_in1);
  \draw[shorten <=-3pt] (X2_in1-|Y.west) to (X2_in1);

  \draw[shorten >=-3pt] (X1_out1) to (Y.east|-X1_out1);
  \draw[shorten >=-3pt] (X2_out1) to (Y.east|-X2_out1);
\end{tikzpicture}
\end{equation}

If we imagine physically wiring together our boxes, the first thing we would
need to do is collect them together like this; then we can proceed to wire them.
We will do exactly this with our systems: first we will take their parallel
product, and then we compose it with a lens that represents the wiring diagram.

\begin{example}\label{ex.ClockWithDisplay}
 We can describe the $\Sys{ClockWithDisplay}$ system (reproduced below) as a
 composite of lenses.
\begin{equation}\label{eqn.clock_system_box2}
\begin{tikzpicture}[oriented WD, every fit/.style={inner xsep=\bbx, inner ysep=\bby}, bbx = .3cm, bby =.3cm, bb min width=.5cm, bb port length=2pt, bb port sep=1, baseline=(Y.center)]
	\node[bb={1}{1}, fill=blue!10] (X1) {$\Sys{Meridian}$};
  	\node[bb={0}{1}, fill=blue!10, below=2 of X1] (X2) {$\Sys{Clock}$};
	\node[bb={0}{2}, fit={($(X1.north west)+(-2,1)$) ($(X1.north east)+(2,1)$) ($(X2.south)+(0,-3)$)}] (Y) {};
  \node[above=0pt of Y.south] (Label) {$\Sys{ClockWithDisplay}$};
	\draw (X1_out1) to (Y_out1);
  \draw let \p1=(X1.south west), \p2=(X2.north east), \n1=\bbportlen, \n2=\bby in
    (X2_out1) to[in=0] (\x2 + \n1, \y2 + \n2) -- (\x1 - \n1, \y2 + \n2) to[out=180] (X1_in1);
  \draw (X2_out1) to (Y_out2);
	\draw[label] 
		node [right=2pt of Y_out1] {$\Set{a.m./p.m.}$}
		node [right=2pt of Y_out2] {$\Set{Hour}$}
		;
\end{tikzpicture}
\end{equation}

First, we take the parallel product of $\Sys{Meridian}$ and $\Sys{Clock}$ (see \cref{exc.clock_meridian_diner}) to get the system 
$$\Sys{Meridian} \otimes \Sys{Clock} : \lens{\Set{a.m./p.m.} \times \Set{Hour}}{\Set{a.m./p.m.} \times \Set{Hour}} \leftrightarrows \lens{\1\times\Set{Hour}}{\Set{a.m./p.m.} \times \Set{Hour}}.$$

Now, we will express the wiring pattern in \cref{eqn.clock_system_box2} as a lens
$$\lens{w^{\sharp}}{w} : \lens{\1\times\Set{Hour}}{\Set{a.m./p.m.} \times \Set{Hour}} \leftrightarrows \lens{\ord{1}}{\Set{a.m./p.m.} \times \Set{Hour}}.$$

We do this by setting
\begin{align*}
  w(m, h) &:= (m, h), \mbox{ and} \\
  w^{\sharp}((m, h), \ast) &:= (\ast, h). 
\end{align*}
Seen as a wiring diagram on its own, $\lens{w^{\sharp}}{w}$ looks like this:
\begin{equation}
\begin{tikzpicture}[oriented WD, every fit/.style={inner xsep=\bbx, inner ysep=\bby}, bbx = .3cm, bby =.3cm, bb min width=2cm, bb port length=0, bb port sep=2, baseline=(Y.center)]
  \node[bb={1}{2}, fill=blue!10]  (Mid) {$\Sys{Meridian}\otimes\Sys{Clock}$};

	\node[bb={0}{2}, fit={($(Mid.north west)+(-3,1)$) ($(Mid.north east)+(2,1)$) ($(Mid.south)+(0,-5)$)}] (Y) {};
  \node[above=0pt of Y.south] (Label) {$\lens{w^{\sharp}}{w}$};


  \draw (Mid_out1) to (Y_out1|-Mid_out1);
  \draw (Mid_out2) to (Y_out1|-Mid_out2);
  
  
  \draw let \p1=(Mid.south east), \p2=(Mid.south west), \n1=\bbportlen, \n2=\bby in
    (Mid_out2) to[out=0, in=0] (\x1 + \n1, \y1-\n2) -- (\x2 - \n1, \y1 - \n2) to[out=180, in=180] (Mid_in1);

	\draw[label] 
		node [right=2pt of Y_out1|-Mid_out1] {$\Set{a.m./p.m.}$}
		node [right=2pt of Y_out2|-Mid_out2] {$\Set{Hour}$}
		;
\end{tikzpicture}
\end{equation}


We can then see that 
$$\Sys{ClockWithDisplay} = \lens{w^{\sharp}}{w} \circ (\Sys{Meridian} \otimes
\Sys{Clock})$$
just like we wanted! In terms of wiring diagrams, this looks like:

\begin{equation}\label{eqn.clock_system_box3}
\begin{tikzpicture}[oriented WD, every fit/.style={inner xsep=\bbx, inner ysep=\bby}, bbx = .3cm, bby =.3cm, bb min width=.5cm, bb port length=2pt, bb port sep=1, baseline=(Y.center)]
	\node[bb={1}{1}, fill=blue!10] (X1) {$\Sys{Meridian}$};
  	\node[bb={0}{1}, fill=blue!10, below=2 of X1] (X2) {$\Sys{Clock}$};
	\node[bb={0}{2}, fit={($(X1.north west)+(-2,1)$) ($(X1.north east)+(2,1)$) ($(X2.south)+(0,-3)$)}] (Y) {};
  \node[above=0pt of Y.south] (Label) {$\Sys{ClockWithDisplay}$};
	\draw (X1_out1) to (Y_out1);
  \draw let \p1=(X1.south west), \p2=(X2.north east), \n1=\bbportlen, \n2=\bby in
    (X2_out1) to[in=0] (\x2 + \n1, \y2 + \n2) -- (\x1 - \n1, \y2 + \n2) to[out=180] (X1_in1);
  \draw (X2_out1) to (Y_out2);
	\draw[label] 
		node [right=2pt of Y_out1] {$\Set{a.m./p.m.}$}
		node [right=2pt of Y_out2] {$\Set{Hour}$}
		;
\end{tikzpicture}
\quad=\quad
\begin{tikzpicture}[oriented WD, every fit/.style={inner xsep=\bbx, inner ysep=\bby}, bbx = .3cm, bby =.3cm, bb min width=.5cm, bb port length=0, bb port sep=1, baseline=(Y.center)]
	\node[bb={1}{1}, fill=blue!10] (X1) {$\Sys{Meridian}$};
  	\node[bb={0}{1}, fill=blue!10, below=2 of X1] (X2) {$\Sys{Clock}$};

  \node[dashed, bb={1}{2}, fit={($(X1.north west)+(-2,1)$) ($(X1.north east)+(2,1)$) ($(X2.south)+(0,-3)$)}]  (Mid) {};
  \node[above=0pt of Mid.south] (Label) {$\Sys{Meridian} \otimes \Sys{Clock}$};

	\node[bb={0}{2}, fit={($(Mid.north west)+(-3,1)$) ($(Mid.north east)+(2,1)$) ($(Mid.south)+(0,-5)$)}] (Y) {};
  \node[above=0pt of Y.south] (Label) {$\lens{w^{\sharp}}{w}$};

	\draw (X1_out1) to (Mid_out1|-X1_out1);
  \draw (X2_out1) to (Mid_out2|-X2_out1) coordinate (Midout);
  \draw (Mid_in1|-X1_in1) coordinate (Midin) to (X1_in1);

  \draw (Mid_out1|-X1_out1) to (Y_out1|-X1_out1);
  \draw (Mid_out2|-X2_out1) to (Y_out1|-X2_out1);
  
  
  \draw let \p1=(Mid.south east), \p2=(Mid.south west), \n1=\bbportlen, \n2=\bby in
    (Midout) to[out=0, in=0] (\x1 + \n1, \y1-\n2) -- (\x2 - \n1, \y1 - \n2) to[out=180, in=180] (Midin);

	\draw[label] 
		node [right=2pt of Y_out1|-X1_out1] {$\Set{a.m./p.m.}$}
		node [right=2pt of Y_out2|-X2_out1] {$\Set{Hour}$}
		;
\end{tikzpicture}
\end{equation}
\end{example}

\paragraph{Wiring together transition diagrams.}

When a deterministic system is presented as a transition diagram (See
\cref{ex.transition_diagram_discrete}), its dynamics is given by reading the
input and following the arrow with that label, and then output the label on the
resulting node. When we wire together systems presented as transition diagrams,
the dynamics then involve reading the input labels of all inner systems, moving
along all the arrows with those labels, and then outputing the labels at each
state, possible into the input of another system.

\begin{exercise}\label{ex.wiring_transition_diagrams}
Here are two systems, $\Sys{S_1}$ and $\Sys{S_2}$ presented in terms of
transition diagrams. The task is calculate the transition diagram of a system
made by wiring them together.

 First, let $\Set{Colors}
= \{\Red, \Blue, \Green\}$ and let $\Set{Bool} = \{\const{true}, \const{false}\}$. Here is our first system
$\Sys{S_1}$, which has interface $\lens{\Set{Bool}}{\Set{Colors}}$:

\begin{equation}\label{eqn.wiring_transition_diagrams1}
\Sys{S_1} \coloneqq \begin{tikzpicture}[baseline=(bl)]
	\node[draw] (bl) {
  \begin{tikzcd}[column sep=small]
    \LMO{\Blue} \ar[loop left, "\const{false}"] \ar[rr, bend left, "\const{true}"] \ar[dd, leftarrow, bend right, "\true"'] &  & \LMO{\Red} \ar[loop right, "\true"] \ar[dd, bend left, "\const{false}" ]\\
    & & \\
    \LMO{\Blue} \ar[loop left, "\false"] \ar[rr, leftarrow, bend left, "\false"] \ar[rr, leftarrow, bend right, "\true"'] & & \LMO{\Green}
  \end{tikzcd}
  };
\end{tikzpicture}
\end{equation}

Our second system $\Sys{S_2}$ will have interface
$\lens{\Set{Colors}}{\Set{Bool}}$:

\begin{equation}\label{eqn.wiring_transition_diagrams2}
\Sys{S_2} \coloneqq \begin{tikzpicture}[baseline=(bl)]
	\node[draw] (bl) {
  \begin{tikzcd}[column sep=small]
    \LMO{\true} \ar[out=120, in=90, loop, red] \ar[in=210, out=250, loop, blue] \ar[rr, bend left = 10, dgreen] \ar[rr, leftarrow, red, bend right= 10] \ar[ddr, leftarrow, dgreen, bend right= 10] &  & \LMO{\false} \ar[loop right, dgreen] \ar[ddl, blue, bend left= 10] \ar[ddl,red, leftarrow, bend right= 10]  \\
    & & \\
    & \LMO{\true} \ar[out=300, in=240, loop, blue] & 
  \end{tikzcd}
  };
\end{tikzpicture}
\end{equation}

\begin{enumerate}
	\item Write down the transition diagram of the system obtained by connecting the above systems according to the following wiring diagram:
\[
\Sys{S} \coloneqq 
\begin{tikzpicture}[oriented WD, bbx = .3cm, bby =.3cm, bb min width=.5cm, bb port length=2pt, bb port sep=1, every fit/.style={inner xsep=\bbx, inner ysep=\bby}
, baseline=(Outer.center)]
  \node[bb={1}{1}, fill=blue!10] (S1) {$\Sys{S_1}$};
  \node[bb={1}{1}, fill=blue!10, right= of S1] (S2) {$\Sys{S_2}$};

  \node[bb={1}{1}, fit={(S1) (S2)}] (Outer) {};

  \draw (Outer_in1) to (S1_in1);
  \draw (S1_out1) to (S2_in1);
  \draw (S2_out1) to (Outer_out1);
\end{tikzpicture}
\]
	\item Explain how to understand the dynamics of this $\Sys{S}$ in terms of the component
systems $\Sys{S_1}$ and $\Sys{S_2}$.
\qedhere
\end{enumerate}
\end{exercise}

\begin{example}[Multi-city SIR models]\label{ex.multi_city_SIR_discrete}
  
  In \cref{ex.SIR_model_discrete}, we saw how a discrete version of the SIR
  model can be seen as a deterministic system. This model assumes there is a
  single population, but what if we wanted to study the spread of a disease
  through multiple cities at the same time? For this, we will need to use a
  mutli-city SIR model.

  To define our multi-city SIR model, we'll first assume that we have a map of
  all our cities, and that we know the rates of travel between cities. So, let's
  say we're looking at Boston and New York City, and that we have flow rates
  $f_{s,\Sys{Bos}\to \Sys{NYC}}$, $f_{i,\Sys{Bos}\to \Sys{NYC}}$ and
  $f_{r,\Sys{Bos}\to \Sys{NYC}}$, and similarly flow rates from NYC to Boston.
  We can draw a simple map as a labeled graph like this:
  \[
\begin{tikzpicture}
	\node[draw] {
  \begin{tikzcd}
    \Sys{Boston} \ar[r, bend left, "f_{\Sys{Bos} \to \Sys{NYC}}"]\ar[r, bend right, leftarrow, "f_{\Sys{NYC} \to \Sys{Bos}}"'] & \Sys{NYC}
  \end{tikzcd}
  };
\end{tikzpicture}
  \]
  Later, in \cref{sec.population_flow_graph}, we will see this sort of diagram
  as a \emph{population flow graph}. But for now, we'll just see how to turn it
  into a deterministic system.
  
  To define each city as a system, we will add parameters to each
  that express the incoming population. Since only one city is incoming to
  Boston, we will only need one set of these parameters: $(e_s, e_i, e_r)$
  corresponding to the inflow of each population class. 
  \begin{align*}
    \State{Boston} &:= \left\{\begin{bmatrix} s \\ i \\ r \end{bmatrix}\, \middle|\, s,\, i,\, r\in \rr\right\} = \State{SIR}. \\
    \Out{Boston} &:= \Out{SIR} = \State{Boston} \\
    \In{Boston} &:= \left\{ \begin{bmatrix} a \\ b \\ (e_s, e_i, e_r)  \end{bmatrix}\, \middle|\, a,\,b\in \rr,\, e \in \rr^3 \right\}= \In{SIR} \times \Out{SIR}\\
    \expose{Boston}\left( \begin{bmatrix} s \\ i \\ r \end{bmatrix}\right) &:= \id \\
    \update{Boston}\left( \begin{bmatrix} s\\ i\\ r \end{bmatrix},\, \begin{bmatrix} a\\ b\\  e\end{bmatrix} \right) &:= \begin{bmatrix}s - asi + f_{s, \Sys{NYC}\to\Sys{Bos}}e_s - f_{s, \Sys{Bos}\to\Sys{NYC}} s \\ i + asi - bi + f_{i, \Sys{NYC}\to\Sys{Bos}}e_i - f_{i, \Sys{Bos}\to\Sys{NYC}} i \\ r + bi+ f_{r, \Sys{NYC}\to\Sys{Bos}}e_r - f_{r, \Sys{Bos}\to\Sys{NYC}} r \end{bmatrix} 
  \end{align*}

We can define the $\Sys{NYC}$ system similarly, but with the flows rates
switched around. In \cref{sec.population_flow_graphs}, we will see a general way
to define these systems out of a \emph{population flow graph}, which could have
many systems (and can allow populations to change classes while in transit at
different rates depending on how they travel, like how one is more likely to get
sick from an airborne disease in an airplane than by travelling in a personal
vehicle).

We can box up $\Sys{Boston}$ as:
\begin{equation}\label{eqn.city_SIR_model_box}
\begin{tikzpicture}[oriented WD, every fit/.style={inner xsep=\bbx, inner ysep=\bby}
, bbx = .3cm, bby =.3cm, bb min width=.5cm, bb port length=2pt, bb port sep=1, baseline=(Sys.center)]
	\node[bb={2}{1}, fill=blue!10] (Sys) {$\Sys{Boston}$};

	\draw[label] 
		node [right=2pt of Sys_out1] {$\Out{SIR}$}
		node [left=2pt of Sys_in1] {$\In{SIR}$}
		node [left=2pt of Sys_in2] {$\Out{SIR}$}
		;
    
\end{tikzpicture}
\end{equation}

Now, suppose we want to model the spread of a disease through Boston and New
York City. Each city will be modeled by $\Sys{SIR_{City}}$, and we will connect
them so that the incoming population of one is set by the outflowing population
of the other:
\begin{equation}\label{eqn.multi_city_SIR_model}
\begin{tikzpicture}[oriented WD, every fit/.style={inner xsep=\bbx, inner ysep=\bby}, bbx = .3cm, bby =.3cm, bb min width=.5cm, bb port length=2pt, bb port sep=1, baseline=(Outer.center)]
  \node[bb={2}{1}, fill=blue!10] (Boston) {$\Sys{Boston}$};
  \node[bb={2}{1}, fill=blue!10, below= 1cm of Boston] (NYC)  {$\Sys{NYC}$};

  \node[bb={0}{0}, fit={($(Boston.north east) + (2, 1)$) ($(NYC.south west) - (2, 1)$)}] (Outer) {};
  
  \draw (Boston_out1) to (Outer.east|-Boston_out1);
  \draw (NYC_out1) to (Outer.east|-NYC_out1);
  \draw (Outer.west|-Boston_in1) to (Boston_in1);
  \draw (Outer.west|-NYC_in1) to (NYC_in1);
  
  \draw let \p1=(Boston_out1), \p2=(NYC_in2), \n1=\bbportlen, \n2=\bby in
    (Boston_out1) to[in=0] (\x1 - \n1, \y1 - .7cm) to (\x2 + \n1, \y1 - .7cm) to[out=180] (NYC_in2);
  \draw let \p1=(NYC_out1), \p2=(Boston_in2), \n1=\bbportlen, \n2=\bby in
    (NYC_out1) to[in=0] (\x1 - \n1, \y1 + .7cm) to (\x2 + \n1, \y1 + .7cm) to[out=180] (Boston_in2);
\end{tikzpicture}
\end{equation}
\end{example}

\begin{exercise}
  Calculate the update function of the whole system
  \cref{eqn.multi_city_SIR_model}. Do you see the population flowing between the
  two cities?
\end{exercise}
%---- Subsection ----%
\subsection{Wiring diagrams as lenses in categories of arities}


We have been drawing a bunch of wiring diagrams so far, and we will continue to
do so throughout the rest of the book. Its about time we explicitly described
the rules one uses to draw these diagrams, and give a formal mathematical
definition of them. The motto of this section is:
\slogan{
    A wiring diagram is just a lenses in a free cartesian category --- a category of \emph{arities.}
  }

We'll begin by describing wiring diagrams and their category in informal terms.
Then, we will see how diagrams relate to lenses in a particular category --- which we call the
category of \emph{arities} --- and finally give a formal definition of the
category of wiring diagrams.


\begin{informal}
  A \emph{wiring diagram} is a diagram which consists of a number of \emph{inner
  boxes}, each with some \emph{input ports} and some \emph{output ports}, that are wired
together inside an \emph{outer box}, which also has input and output ports. This gives four types of ports: inner (box) input (port), inner output, outer input, and outer output. 

We can wire in the following ways:
\begin{enumerate}
  \item Every outer output port is wired to exactly one inner output port.
  \item Every inner input port is wired to exactly one inner output port or an
    outer input port.
\end{enumerate}

The category of wiring diagrams has boxes as its objects and wiring diagrams as
its morphisms. Wiring diagrams are composed by filling the inner boxes with other wiring
diagrams, and then erasing the middle layer of boxes.

\begin{equation}\label{eqn.wiring_diagram_example_misc}
\begin{tabular}{c|c|c}
\small Boxes &\small Wiring Diagrams &\small Composition by nesting\\\hline
~&&\\
\parbox{.5in}{
\begin{tikzpicture}[oriented WD, every fit/.style={inner xsep=\bbx, inner ysep=\bby}, bby=1ex]
  \node[bb={3}{2}, fill=blue!10] (X1) {};
  \node[bb={1}{1}, fill=blue!10, below=.4cm of X1] (X2) {};
  \node[bb={0}{2}, fill=blue!10, below=.4cm of X2] (X3) {};   
\end{tikzpicture}
}
&
\;\;\parbox{1.45in}{
\begin{tikzpicture}[oriented WD, every fit/.style={inner xsep=\bbx, inner ysep=\bby},bb min width =.7cm, bb port sep =1, bbx=.6cm,bb port length=3pt] 
  \node[bb port sep=1.6, fill=blue!10, bb={2}{2}] (X1) {};
  \node[bb port sep=.8,bb={1}{1}, fill=blue!10, right=.7 of X1_out1] (X2) {};
  \node[bb={1}{2}, fit={(X1) (X2) ($(X1.north)+(0,1)$)}] (Y) {};
  \draw[ar] (Y_in1') to (X1_in2);
  \draw[ar,pos=.8] (X1_out1) to (X2_in1);
  \draw (X2_out1) to (Y_out1');
  \draw[ar] (X1_out2) to (Y_out2');
  \draw[ar] let \p1=(X2.north east), \p2=(X1.north west), \n1={\y2+\bby}, \n2=\bbportlen in
          (X2_out1) to[in=0] (\x1+.7*\n2,\n1) -- (\x2-.7*\n2,\n1) to[out=180] (X1_in1);

\end{tikzpicture}
}
&
\;\;\parbox{1.2in}{
\begin{tikzpicture}[oriented WD, every fit/.style={inner xsep=\bbx, inner ysep=\bby}, bb small]
  \node[bb={2}{2}, fill=blue!10] (X1) {};
  \node[bb={1}{1}, fit={(X1) ($(X1.north)+(0,1)$)}, densely dotted] (Y1) {};
  \node[bb={2}{1}, fill=blue!10, below right=4 and 0 of X1] (X2) {};
  \node[bb={0}{2}, fill=blue!10, below left=of X2] (X3) {};
  \node[bb={1}{2}, fit=(X2) (X3), densely dotted] (Y2) {};
  \node[bb={1}{2}, fit=(Y1) (Y2)] (Z) {};
  \draw[ar] (Z_in1') to (Y2_in1);
  \draw[ar] (Y2_in1') to (X2_in1);
  \draw[ar] (X3_out1) to (X2_in2);
  \draw[ar] (X3_out2) to (Y2_out2');
  \draw (Y2_out2) to (Z_out2');
  \draw[ar] (Y1_in1') to (X1_in2);
  \draw (X1_out2) to (Y1_out1');
  \draw[ar] (Y1_out1) to (Z_out1');
  \draw (X2_out1) to (Y2_out1');
  \draw[ar] let \p1=(Y2.north east), \p2=(Y1.south west), \n1={\y1+\bby}, \n2=\bbportlen in
          (Y2_out1) to[in=0] (\x1+\n2,\n1) -- (\x2-\n2,\n1) to[out=180] (Y1_in1);
  \draw[ar] let \p1=(X1.north east), \p2=(X1.north west), \n1={\y1+\bby}, \n2=\bbportlen in
          (X1_out1) to[in=0] (\x1+\n2,\n1) -- (\x2-\n2,\n1) to[out=180] (X1_in1);
\end{tikzpicture}
}
\end{tabular}
\end{equation}
\end{informal}

 Wiring diagrams are designed to
express the flow of variables through the system; how they are to be copied from
one port to another, how they are to be shuffled about, and (though we haven't
had need for this yet) how they are to be deleted or forgotton.



In order to capture this idea of copying, deleting, and shuffling around
variables, we will work with the \emph{category of arities} (and variations on it). The
category of arities is extremely important since it captures precisely the
algebra of copying, deleting, and shuffling around variables. 
In this section, we will interpret various sorts of wiring diagrams as lenses in
categories of arities, which are the free cartesian categories.

\begin{definition}
  The category $\arity$ of arities is the free cartesian category generated by
  a single object $\XX$. That is, $\arity$ constains an object $\XX$, called
  the \emph{generic object}, and for any finite set $I$, there is an $I$-fold
  power $\XX^I$ of $\XX$. The only maps are those that can be defined from the
  product structure by pairing and projection.
  
  Explicitly, $\arity$ is has:
\begin{itemize}
  \item Objects $\{\XX^{I}\mid I \text{ a finite set}\}$.
  \item Maps $f^{\ast} : \XX^{I} \to \XX^{J}$ for any function $f : J \to I$.
  \item Composition defined by $g^{\ast} \circ f^{\ast} := (f \circ g)^{\ast}$
    and $\id := \id^{\ast}$.
\end{itemize}
The cartesian product in $\arity$ is given, in terms of index sets, by the following familiar formula:
$$\XX^I \times \XX^J = \XX^{I + J}.$$
\end{definition}

If you like opposite categories, this might clarify things a bit.

\begin{proposition}
$\arity$ is isomorphic to the opposite of the category finite sets
\[\arity\cong\finset\op.\]
\end{proposition}

Now, $\XX$ is just a formal object, so it doesn't have elements. But we can give
a language for writing down the objects and arrows  of $\arity$ that makes it
look like it does.
Think of the elements of $\XX^I$ as finite lists of variables $\XX^I = (x_i \mid i \in
I)$ indexed by the set $I$. Then for any reindexing function $f : J \to I$, we
can see $f^{\ast}$ as telling us how $J$-variables are assigned $I$-variables. For example,
consider the function $f : \ord{3}\to \ord{2}$ given by $1\mapsto 2$, $2 \mapsto 1$, and $3\mapsto 2$
\[
\begin{tikzpicture}
	\boxofbullets{3}{(0,0)}{y}{$y$}{right=0pt};
	\boxofbullets{2}{(-2,0)}{x}{$x$}{left=0pt};
	\draw[blue, dashed, |->] (pt_y_1) -- (pt_x_1);
	\draw[blue, dashed, |->] (pt_y_2) -- (pt_x_1);
	\draw[blue, dashed, |->] (pt_y_3) -- (pt_x_2);
	\node[below=0 of box_y] (lab y) {$\XX^\3$};
	\node at (lab y-|box_x) (lab x) {$\XX^\2$};
	\draw[->] (lab x) -- (lab y);
\end{tikzpicture}
\]
In other words, $f$ says that the first slot of the resulting list will be
filled by the second variable of the first, and the second slot will be filled
by the first variable, and the third slot will be filled by the second variable.\spiz{Jaz, maybe you want to take a look and make sure this still reads the way you want.}


Composition is of course just given by composing functions in the opposite direction.
%given by propagating the reindexing. If we have the function
%\begin{align*}
%  g : \ord{4} &\to \ord{3} \\
%  1,\, 2 &\mapsto 1 \\
%  3 &\mapsto 3 \\
%  4 &\mapsto 2
%\end{align*}
For example, given some $g\colon\4\to\3$, we just compose to get our map $\XX^\2\to\XX^\4$.
\[
\begin{tikzpicture}
	\boxofbullets{2}{(-4,0)}{x}{$x$}{left=0pt};
	\boxofbullets[1.2]{3}{(-2,0)}{y}{$y$}{above=0pt};
	\boxofbullets{4}{(0,0)}{z}{$z$}{right=0pt};
	\draw[blue, dashed, |->] (pt_z_1) -- (pt_y_1);
	\draw[blue, dashed, |->] (pt_z_2) -- (pt_y_1);
	\draw[blue, dashed, |->] (pt_z_3) -- (pt_y_2);
	\draw[blue, dashed, |->] (pt_z_4) -- (pt_y_1);
	\draw[blue, dashed, |->] (pt_y_1) -- (pt_x_1);
	\draw[blue, dashed, |->] (pt_y_2) -- (pt_x_1);
	\draw[blue, dashed, |->] (pt_y_3) -- (pt_x_2);
	\node[below=0 of box_z] (lab z) {$\XX^\4$};
	\node at (lab z-|box_y) (lab y) {$\XX^\3$};
	\node at (lab z-|box_x) (lab x) {$\XX^\2$};
	\draw[->] (lab y) -- (lab z);
	\draw[->] (lab x) -- (lab y);	
\end{tikzpicture}
\]

%Then we can see calculate $g^{\ast} \circ f^{\ast}$ by substituting variables
%using in $g^{\ast}$ for the variables in the right hand side of $f^{\ast}$:
%$$\frac{(x_1,\, x_2 \mapsto x_2, x_1, x_2)\quad (x_1,\, x_2,\, x_3 \mapsto x_1,\,
%  x_1,\, x_3,\, x_2)}{(x_1,\, x_2 \mapsto x_2,\, x_2,\, x_2,\, x_1)}.$$
%We can see that this is equivalently $(f \circ g)^{\ast}$.

\begin{exercise}
  Express the following morphisms in $\arity$ in terms of lists of variables:
  \begin{enumerate}
    \item The terminal morphism $\XX^{\ord{2}} \to \XX^{\ord{0}}$, given by the
      \emph{initial} function $!' : \ord{0} \to \ord{2}$ which includes empty
      set into the set with two elements (hint, there's \emph{nothing} on one side).
    \item The duplication morphism $!^{\ast} : \XX \to \XX^{\ord{2}}$ given by
      $! : \ord{2} \to \ord{1}$. 
    \item The swap morphisms $\fun{swap}^{\ast} : \XX^{\ord{2}} \to
      \XX^{\ord{2}}$ given by $\fun{swap} : \ord{2} \to \ord{2}$ defined by $0
      \mapsto 1$ and $1 \mapsto 0$.
    \item What map corresponds to the map $1 : \ord{1} \to \ord{2}$ picking out
      $1 \in \ord{2} = \{1, 2\}$? What about $2 : \ord{1} \to \ord{2}$. 
    \item Convince yourself that \emph{any} map $\XX^I \to \XX^J$ you can express with the
      universal property of products can be expressed by choosing an appropriate
      $f : J \to I$.
  \end{enumerate}
\end{exercise}

Because $\arity$ expresses the algebra of shuffling, copying, and deleting
variables in the abstract, we can use it to define wiring diagrams. Recall from \cref{def.lens_category} the definition of lens in an arbitrary cartesian category.

\begin{definition}\label{def.category_of_wiring_diagrams}
  The category $\Cat{WD}$ of wiring diagrams is defined to be the category of
  lenses in the category of arities $\arity$.
  $$\Cat{WD} := \Cat{Lens}_{\arity}.$$
\end{definition}

This definition shows us that the wiring diagrams we have been using are
precisely the lenses you can express if you only copy, delete, and shuffle
around your variables. Here's how we interpret a lens
$\lens{w^{\sharp\ast}}{w^{\ast}} : \lens{\XX^{A^{-}}}{\XX^{A^+}}
\leftrightarrows \lens{\XX^{B^{-}}}{\XX^{B^+}}$ in $\Cat{Arity}$ as a
wiring diagram:
\begin{itemize}
  \item First, we interpret the index set $A^-$ as the set of input ports of the
    inner boxes, and the set $A^+$ as the set of output ports of the inner
    boxes. Similarly, we see $B^-$ as the set of input ports of the outer box,
    and $B^+$ as the set of output ports of the outer box.
  \item Then we remember that $w^{\ast} :
    \XX^{A^+} \to \XX^{B^+}$ comes from a reindexing function $w : B^+ \to
    A^+$), which we interpret
    as selecting for each outer output port $p \in B^+$, the unique inner output
    port $w(p)$ it will be wired to.
  \item Finally, we note that $w^{\sharp\ast} : \XX^{A^+} \times \XX^{B^-} \to
    \XX^{A^-}$ comes from a function $w^{\sharp} : A^- \to A^+ + B^-$ (because
    $\XX^{A^+} \times \XX^{B^-} = \XX^{A^+ + B^-}$), and we interpret this as
    selecting for each inner input port $p \in A^-$ either the inner output port
    $w^{\sharp}(p) \in A^+$ or the outer input port
    $w^{\sharp}(p) \in B^-$ which $p$ will be wired to.
\end{itemize}

On the other hand, we can read any wiring diagram as a lens in $\arity$ in the
following way:
\begin{equation}\label{eqn.wiring_diagram_to_map_in_arity}
\begin{tikzpicture}[oriented WD, every fit/.style={inner xsep=\bbx, inner ysep=\bby},bb min width =1cm, bb port sep =1, bbx=.6cm,bb port length=3pt, baseline=(Y.center)] 
  \node[bb port sep=1.6, bb={2}{2}, fill=blue!10] (X1) {};
  \node[bb port sep=.8,bb={1}{1}, fill=blue!10, right=.7 of X1_out1] (X2) {};
  \node[bb={1}{2}, fit={(X1) (X2) ($(X1.north)+(0,1)$)}] (Y) {};
  \draw[ar] (Y_in1') to (X1_in2);
  \draw[ar,pos=.8] (X1_out1) to (X2_in1);
  \draw (X2_out1) to (Y_out1');
  \draw[ar] (X1_out2) to (Y_out2');
  \draw[ar] let \p1=(X2.north east), \p2=(X1.north west), \n1={\y2+\bby}, \n2=\bbportlen in
          (X2_out1) to[in=0] (\x1+.7*\n2,\n1) -- (\x2-.7*\n2,\n1) to[out=180] (X1_in1);
          

   \draw[label] 
     node[left= 3pt of X1_out1] (label1) {$a^+_1$}
     node[left= 3pt of X2_out1] (label1) {$a^+_2$}
     node[left= 3pt of X1_out2] (label1) {$a^+_3$}
     node[right= 4pt of X1_in1] (label1) {$a^-_1$}
     node[right= 4pt of X2_in1] (label1) {$a^-_2$}
     node[right= 4pt of X1_in2] (label1) {$a^-_3$}
     node[left= 2pt of Y_in1] (label1) {$b^-_1$}
     node[right= 2pt of Y_out1] (label1) {$b^+_1$}
     node[right= 2pt of Y_out2] (label1) {$b^+_2$}
   ;
\end{tikzpicture}
\quad\quad
\begin{aligned}
  w^{\ast} &= (a^+_1,\, a^+_2, a^+_3 \mapsto  a_2^+,\, a^+_3)\\
  w^{\sharp\ast} &= (a^+_1,\, a^+_2,\, a^+_3,\,a^+_4,\,b^-_1 \mapsto a^+_2,\, a_1^+,\, b_1^-)
\end{aligned}
\end{equation}
\spiz{redraw with ``boxofbullets''}


\begin{exercise}
Translate the following wiring diagrams into lenses in the category of arities,
and vice versa:
\begin{enumerate}
\item
  \[
\begin{tikzpicture}[oriented WD, every fit/.style={inner xsep=\bbx, inner ysep=\bby}, bbx=1.1em, bb min width=3.3em, bby=2ex, bb port sep = 1]
	\node[bb={2}{1}, fill=blue!10] (X1) {};
	\node[bb={1}{1}, fill=blue!10, below right=-1 and 2 of X1] (X2) {};
	\node[bb={2}{1}, fill=blue!10, below right=-1.5 and 2 of X2] (X3) {};
	\node[bb={3}{2}, fit={($(X1.north west)+(-1,0)$) (X2) ($(X3.south east)+(1,0)$)}] (Y) {};
	%
	\draw (Y_in1') to (X1_in1) ;
	\draw (Y_in2') to (X1_in2);
	\draw (X1_out1) to (Y_out1');
	\draw (X1_out1) to (X2_in1);
	\draw (X2_out1) to (X3_in1);
	\draw (Y_in3') to (X3_in2);
	\draw (X3_out1) to (Y_out2');
\end{tikzpicture}
\]
\item $\lens{w^{\sharp\ast}}{w^{\ast}} : \lens{\XX^{\ord{2}} \times\XX^{\ord{1}}
    \times\XX^{\ord{2}}}{\XX \times\XX \times\XX^{\ord{2}}}
  \leftrightarrows \lens{\XX^{\ord{2}}}{\XX^{\ord{1}}}$
  \[
    \begin{aligned}
      w^{\ast} &= (a_1^+,\, a_2^+,\, a_{31}^+,\, a_{32}^+ \mapsto a_{32}^+)\\
      w^{\sharp\ast} &= (a_1^+,\, a_2^+,\, a_{31}^+,\, a_{32}^+,\, b_1^-,\,
      b_2^- \mapsto a_{31}^+,\, b_1^-,\, b_2^-,\, a_1^+,\, a_2^+)
    \end{aligned}
  \]
  %%
  %% NOTE: This corresponds to example (5) in the collected diagrams
  %%       which can be found right after (cobordism)
  %%
\end{enumerate}
\end{exercise}

That $\arity$ is the free cartesian category generated by a single object
means that it satisfies a very useful universal property.
\begin{proposition}[Universal property of $\arity$]\label{prop.arity_universal_property}
  For any cartesian category $\cat{C}$ and object $C \in \cat{C}$, there is a
  cartesian functor $\Fun{ev}_{C} : \arity \to \cat{C}$ which sends $\XX$ to $C$. This
  functor is the unique such functor up to a unique natural isomorphism.
\end{proposition}
\begin{proof}[Proof Sketch]
  The functor $\Fun{ev}_C$ can be defined by ``just substitute $C$ for $\XX$''.
  Namely, we send 
$$\XX^I \mapsto C^I$$
and for every function $f^{\ast} : \XX^I \to \XX^J$, we send it to $f^{\ast} :
C^I \to C^J$ defined by the universal property of the product in $\cat{C}$. This
is cartesian because $C^{I + J} \cong C^I \times C^J$ in any cartesian category.
It is unique up to a unique natural isomorphism because $\XX^I$ is the $I$-fold
product of $\XX$, and so if $\XX \mapsto C$, then universal comparison maps
between the image of $\XX^I$ and $C^I$ must be isomorphisms.
\end{proof}

We can think of the functor $\Fun{ev}_C : \arity \to \cat{C}$ as the functor
which tells us how to interpret the abstract variables in $\arity$ as variables
of type $C$. For example, the functor $\Fun{ev}_{\rr} : \arity \to \smset$ tells
us how to interpret the abstract variables $(x_i \mid i \in I)$ in $\smset$ as
variable real numbers $\{x_i \in \rr \mid i \in I\}$. Under $\Fun{ev}_{C}$, the
map of arities $(x_1,\, x_2,\, x_3 \mapsto x_2, x_2)$ gets sent to the actual map
$C^{\ord{3}} \to C^{\ord{2}}$ given by sending $(c_1,\, c_2,\, c_3)$ to
$(c_2,\, c_2)$.

By the functoriality of the lens construction, this means that given an object
$C \in \cat{C}$ of a cartesian category of ``values that should be flowing on
our wires'', we can interpret a wiring diagram as a lens in $\cat{C}$! We record
this observation in the following proposition. \spiz{Have we given the monoidal structure on $\Cat{WD}$?}
\begin{proposition}\label{prop.wiring_diagram_as_lens}
  Let $C \in \cat{C}$ be an object of a cartesian category. Then there is a
  strong monoidal functor
  $$\lens{\Fun{ev}_C}{\Fun{ev}_C} : \Cat{WD} \to \cat{C}$$
  which interprets a wiring diagram as a lens in $\cat{C}$ with values in $C$
  flowing along its wires.
\end{proposition}
\begin{proof}
  This is just \cref{prop.lens_functoriality} (and
  \cref{prop.lens_functoriality_moniodal}) applied to $\Fun{ev}_C : \arity \to
  \cat{C}$ from \cref{prop.arity_universal_property}.
\end{proof}

This is how we can interpret a wiring diagram as a lens in whatever cartesian category we
are working in. There is, however, a slight issue; in most of our previous
examples, there have been many different types of signals flowing along the
wires. We can fix this by using \emph{typed arities}. We will keep track of what
type of signal is flowing along each wire, and only allow ourselves to connect
wires that carry the same type of signal.

\begin{definition}
  Let $\cat{T}$ be a set, elements of which we call \emph{types}. The category $\arity_{\cat{T}}$ is the
  free cartesian category generated by objects $\XX_{\tau}$ for each type $\tau
  \in \cat{T}$. Explicitly, $\arity_{\cat{T}}$ has:
  \begin{itemize}
    \item Objects $\prod_{i \in I} \XX_{\tau_i}$ for any finite set $I$ and
      \emph{typing function}
      $\tau_{(-)} : I \to \cat{T}$. We interpret $\tau_i \in \cat{T}$ as the
      type of element $i \in I$.
    \item Maps $f^{\ast} : \prod_{i \in I} \XX_{\tau_i} \to
      \prod_{j \in J} \XX_{\tau_j}$ for any function $f : I \to J$ which
      preserves the typing: $\tau_{fi} = \tau_i$.
    \item Composition is given by $g^{\ast} \circ f^{\ast} = (f \circ g)^{\ast}$
      and the identity is given by $\id := \id^{\ast}$. 
  \end{itemize}
  That is, $\arity_{\cat{T}} \cong (\Cat{Fin} \downarrow \cat{T})\op$ is dual to
  the category
  $\Cat{Fin} \downarrow \cat{T}$ of \emph{$\cat{T}$-typed finite sets}, the slice category (a.k.a. comma category)
  of the inclusion $\Cat{Fin} \hookrightarrow \Cat{Set}$ over the set $\cat{T}$
  of types. 
\end{definition}

\begin{exercise}
  We blew through that isomorphism $\arity_{\cat{T}} \cong (\Cat{Fin} \downarrow
  \cat{T})\op$ quickly, but its not entirely trivial. The category $\Cat{Fin} \downarrow
  \cat{T}$ has objects functions $\tau : I \to \cat{T}$ where $I$ is a finite
  set, and a morphism is a commuting triangle like this:
  \[
    \begin{tikzcd}
      I \ar[rr, dashed] \ar[dr, "\tau"'] & & J \ar[dl, "\tau"] \\
      & \cat{T} &
    \end{tikzcd}
  \]


  Expand the isomorphism out in full and check
  that you understand it.
\end{exercise}

Note that $\arity = \arity_{\ord{1}}$ is the special case where we have a single
type. Just as we wrote the morphisms in $\arity$ as $(x_1,\, x_2 \mapsto x_2,\,
x_1,\, x_2)$, we can write the morphisms in $\arity_{\cat{T}}$ as
$$(x_1 : \tau_1,\, x_2 : \tau_2,\, x_3 : \tau_2 \mapsto x_2 : \tau_2,\, x_1 :
\tau_1,\, x_2 : \tau_1)$$
where $\tau_1,\, \tau_2,\, \tau_3 \in \cat{T}$ are all (fixed, not variable) types.


We check that $\arity_{\cat{T}}$ as we defined it does indeed have the correct
universal property.
\begin{proposition}\label{prop.arity_universal_property_typed}
  For any $\cat{T}$-indexed family of elements $C_{(-)} : \cat{T} \to \cat{C}$
  in a cartesian category $\cat{C}$, there is a cartesian functor $\Fun{ev}_C :
  \arity_{\cat{T}} \to \cat{C}$ sending $\XX_{\tau}$ to $C_{\tau}$. The functor
  $\fun{ev}_C$ is the unique such functor up to a unique natural isomorphism.
\end{proposition}
\begin{proof}[Proof Sketch]
  Just like in \cref{prop.arity_universal_property}, we can take

  $$\Fun{ev}_C\left( \prod_{i \in I} \XX_{\tau_i} \right) :=
  \prod_{i \in I} C_{\tau_i}.$$

\end{proof}
\begin{exercise}
  Complete the proof of \cref{prop.arity_universal_property_typed}.
\end{exercise}

As before, we note that the map
$$(x_1 : \tau_1,\, x_2 : \tau_2,\, x_3 : \tau_2 \mapsto x_2 : \tau_2,\, x_1 :
\tau_1,\, x_2 : \tau_1)$$
\spiz{redraw}
gets sent by $\Fun{ev}_C$ to the function $C_{\tau_1} \times C_{\tau_2} \times C_{\tau_3} \to
C_{\tau_2} \times C_{\tau_1} \times C_{\tau_2}$ which sends $(c_1,\, c_2,\, c_3)$ to
$(c_2,\, c_1,\, c_2)$

\begin{corollary}\label{cor.arity_change_of_types}
  For any function $f : \cat{T} \to \cat{T'}$, there is a change of type functor
  $\Fun{ev}_{\XX_f} : \arity_{\cat{T}} \to \arity_{\cat{T}}$.
\end{corollary}
\begin{proof}
  We apply \cref{prop.arity_universal_property_typed} to the family $\XX_{f(-)}
  : \cat{T} \to \arity_{\cat{T'}}$ of objects of $\arity_{\cat{T'}}$. That is,
  we send
  $$\prod_{i \in I} \XX_{\tau_i} \mapsto \prod_{i \in I} \XX_{\tau({f(i))}}.\qedhere$$
\end{proof}

We can now define the category of typed wiring diagrams to be the category of
lenses in the category of typed arities.
\begin{definition}
  For a set $\cat{T}$ of types, the category $\Cat{WD}_{\cat{T}}$ of \emph{$\cat{T}$-typed wiring diagrams}
  is the category of lenses in the category of $\cat{T}$-typed arities:
  $$\Cat{WD}_{\cat{T}} := \Cat{Lens}_{\cat{T}}.$$
\end{definition}

As with the singly-typed case, we can interpret any typed wiring diagram as a
lens in a cartesian category of our choosing.
\begin{proposition}
  For any family $C_{(-)} : \cat{T} \to \cat{C}$ of objects in a cartesian
  category $\cat{C}$, indexed by a set $\cat{T}$ of types, there is a strong
  monoidal functor
  $$\lens{\Fun{ev}_C}{\Fun{ev}_C} : \Cat{WD}_{\cat{T}} \to \Cat{Lens}_{\cat{C}}$$
  which interprets a typed wiring diagram as a lens in $\cat{C}$ with
  appropriately typed values flowing along its wires.
\end{proposition}

\begin{remark}
  Because the action of $\Fun{ev}_C$ is so simple, we will often just equate the
  typed wiring diagram with the lens it gives when interpreted in our category
  of choice. 
\end{remark}
\begin{example}
  We can describe the wiring diagram
\begin{equation}
\begin{tikzpicture}[oriented WD, every fit/.style={inner xsep=\bbx, inner ysep=\bby}, bbx = .3cm, bby =.3cm, bb min width=2cm, bb port length=0, bb port sep=2, baseline=(Y.center)]
  \node[bb={1}{2}, fill=blue!10]  (Mid) {};

	\node[bb={0}{2}, fit={($(Mid.north west)+(-3,1)$) ($(Mid.north east)+(2,1)$) ($(Mid.south)+(0,-5)$)}] (Y) {};
  \node[above=0pt of Y.south] (Label) {$\lens{w^{\sharp}}{w}$};


  \draw (Mid_out1) to (Y_out1|-Mid_out1);
  \draw (Mid_out2) to (Y_out1|-Mid_out2);
  
  
  \draw let \p1=(Mid.south east), \p2=(Mid.south west), \n1=\bbportlen, \n2=\bby in
    (Mid_out2) to[out=0, in=0] (\x1 + \n1, \y1-\n2) -- (\x2 - \n1, \y1 - \n2) to[out=180, in=180] (Mid_in1);

	\draw[label] 
		node [right=2pt of Y_out1|-Mid_out1] {$\Set{a.m./p.m.}$}
		node [right=2pt of Y_out2|-Mid_out2] {$\Set{Hour}$}
		;
\end{tikzpicture}
\end{equation}
  from \cref{ex.ClockWithDisplay} as a lens in
  a category of typed arities in the following way. We have two types:
  $\Set{a.m./p.m.}$ and $\Set{Hour}$. So, $\cat{T} = \{\Set{a.m./p.m.},
  \Set{Hour}\}$. Then
  \begin{align*}
    w &= (t : \Set{Hour},\, m : \Set{a.m./p.m.} \mapsto t : \Set{Hour},\, m : \Set{a.m./p.m.}) \\
    w^{\sharp} &= (t : \Set{Hour},\, m : \Set{a.m./p.m.} \mapsto t : \Set{Hour})
  \end{align*}
  giving us a wiring diagram in $\Cat{WD}_{\cat{T}}$. We can then interepret
  this as the lens from $\cref{ex.ClockWithDisplay}$ as the image of this wiring
  diagram which interprets the types $\Set{a.m./p.m.}$ and $\Set{Hour}$ as the
  actual sets $\{\const{a.m.},\const{p.m.}\}$ and $\{1, 2,\ldots, 12\}$.
\end{example}

\begin{exercise}

  
  Write the wiring diagram from \cref{ex.multi_city_SIR_discrete} out as a lens
  in $\arity_{\{\Out{SIR},\, \In{SIR} \}}$.
\end{exercise}

%-------- Section --------%
\section[How systems behave: Trajectories, Steady States, etc.]{How systems behave: Trajectories, Steady States, and Periodic Orbits}  \label{sec.behavior_discrete}

So far, we have seen how to wire up dynamical systems. But we haven't seen our
dynamical systems actually \emph{do anything}. In this section, we will begin to
study the behavior of our dynamical systems. We will see particular kinds of
behaviors our systems can have:
trajectories, steady states, and periodic orbits.

\begin{informal}
  A \emph{behavior} of a dynamical system is a particular way its states can
  change according to its dynamics.  

  There are different \emph{kinds of behavior} corresponding to the different
  sorts ways that the states of a system could evolve. Perhaps they eventually
  repeat, or they stay the same despite changing conditions.
\end{informal}

In \cref{sec.behaviors}, we will give a formal definition of behavior of
dynamical system. We will see that the different kinds of behaviors ---
trajectories, steady states, periodic orbits, etc. --- can each
be packaged up into a single system that \emph{represents} that kind of
behavior. This system will behave in exactly that kind of way, and do nothing else. Maps from it to a system of interest will exhibit that sort of behavior in the system of interest.

We will end the section by seeing how the steady states of a complex system formed by wiring together some
component systems can be calculated from the steady states of the components.
The calculation will turn out to be just a bit of matrix arithmetic!

This result will be a preview to our more general results in
\cref{sec.representables} which concern arbitrary behaviors of dynamical
systems. But that's
getting ahead of ourselves.

%---- Subsection ----%
\subsection{Trajectories}\label{sec.trajectory_discrete}

In the introduction, we saw that the $\Sys{Clock}$ system
\cref{eqn.clock_system_box} has behaves in this way if it starts at $3$ o'clock: 
$$3 \xmapsto{\fun{tick}} 4 \xmapsto{\fun{tick}} 5 \xmapsto{\fun{tick}} 6
\xmapsto{\fun{tick}} \cdots$$

This sequence of states of the clock system, each following from the last by the
dynamics of the system, is called a \emph{trajectory}. When our systems have
input parameters, we will need to choose a sequence of input parameters to feed
the system in order for the states to change.

\begin{definition}\label{def.trajectory_discrete}
 Let $$\Sys{S} = \lens{\update{S}}{\expose{S}} : \lens{\State{S}}{\State{S}}
 \leftrightarrows \lens{\In{S}}{\Out{S}}$$
 be a deterministic system. Suppose that $p : \nn \to \In{S}$ is a sequence of
 parameters for $\Sys{S}$. Then a \emph{$p$-trajectory} of $\Sys{S}$ is a sequence $s : \nn \to \State{S}$ of states so that
 $$\update{S}(s_i, p_i) = s_{i + 1}$$
 for all $i \in \nn$.

 If additionally $v : \nn \to \Out{S}$ is a sequence of output values for $\Sys{S}$, then a
 \emph{$\lens{p}{v}$-trajectory} is a sequence of states $s : \nn \to \State{S}$ so
 that
 \begin{align*}
   \update{S}(s_i, p_i) &= s_{i + 1}\\
   \expose{S}(s_i) &= v_i
 \end{align*}
 for all $i \in \nn$. We call the pair $\lens{p}{v}$ the \emph{chart} of the trajectory $s$.
\end{definition}

Its worth noting that a trajectory $s : \nn \to \State{S}$ in a deterministic system is determined
entirely by its start state $s_0$. This is what makes deterministic systems
deterministic; if you know the dynamics and you know what state the system is
in, you know how it will continue to behave. We'll relax this condition later in \cref{??}.

\begin{example}
Consider the SIR model of \cref{ex.SIR_model_discrete}. Suppose that we let our
parameters $(a, b) : \nn \to \In{\Sys{SIR}}$ be constant at $.2$ and $.3$
respectively: that is, $a_t = .2$ and $
b_t = .3$ for all $t$. Then a trajectory for $\Sys{SIR}$ with parameters $(a, b)$
is a sequence of populations $(s, i, r) : \nn \to \State{SIR}$ such that
$$\begin{bmatrix}s_{t+1}\\ i_{t+1}\\ r_{t+1} \end{bmatrix} = \begin{bmatrix}
  s_t - .2s_t i_t\\ i_t + .2s_t i_t - .3i_t \\ r_t + .3i_t\end{bmatrix}$$

Here is an example of such a trajectory with a $1000$ total people and one infected
person to start, that is $(s_0, i_0, r_0) = (999, 1, 0)$.
\[
  \begin{tikzpicture}
    %% horizontal axis
    \draw[->] (0,0) to (11, 0);
    \draw[label]
      node at (11, -.3) {$t$};
    \foreach \x in {1,...,10}
      \draw (\x cm,1pt) -- (\x cm,-1pt) node[anchor=north] {$\x$};

   %% vertical axis
   \draw[->] (0,0) to (0, 11);
   \draw[label] node at (-1, 11) {$(s, i, r)$};
   \foreach \y in {1,...,10}
   {
     \pgfmathtruncatemacro{\display}{100*\y};
     \draw (1pt,\y cm) -- (-1pt,\y cm) node[anchor=east] {$\display$};
   }

%   %% plot
%   \tikzmath{
%     real \s; 
%     \s = 9.99;
%   
%     function srecusion(\x){
%       return {\x-1};
%     }
%     \foreach \t in {0,1,...,10}
%     {
%       \node[draw,circle,red] at (\t, \s) {};
%       \s = srecursion(\s);
%     }
%    }
  \end{tikzpicture}
\]
\jaz{I don't know how to actually plot this...}
\end{example}


\begin{example}\label{ex.transition_diagram_discrete_traj}
If a deterministic system is written as a transition diagram, then the
trajectories in the system are paths through the diagram. Recall from
\cref{ex.transition_diagram_discrete} this system:
\[
\begin{tikzpicture}
	\node[draw] {
  \begin{tikzcd}[column sep=small]
  	\LMO{a}\ar[rr, dgreen, thick, bend left]\ar[loop left, thick, orange]&&
  	\LMO{b}\ar[ll, thick, orange, bend left]\ar[dl, bend left, thick, dgreen]\\&
  	\LMO{b} \ar[ul, thick, orange, bend left] \ar[loop left, thick, dgreen]
  \end{tikzcd}
  };
\end{tikzpicture}
\]

Suppose that $p : \nn \to \{\Green, \Orange\}$
alternates between $\Green$ and $\Orange$. Then
starting at the top right state, a trajectory quickly settles into alternating between the top two
states:
\[
\begin{tikzpicture}
	\node[draw] {
  \begin{tikzcd}[column sep=small]
    \LMO{b} \ar[r, dgreen, thick] & \LMO{b} \ar[r, orange, thick] & \LMO{a} \ar[r, dgreen,thick] & \LMO{b} \ar[r, orange, thick] & \LMO{a} \ar[r, dgreen, thick] & \cdots
  \end{tikzcd}
  };
\end{tikzpicture}
\]

\end{example}

Knowing about trajectories can show us another important role that deterministic
systems play: they are \emph{stream transformers}. From a stream $p : \nn \to
\In{S}$ of inputs and a start state $s_0 \in \State{S}$, we get a trajectory $s
: \nn \to \State{S}$ given recursively by
\begin{align*}
  s_{t+1} &:= \update{S}(s_t,\, p_t).
\end{align*}
We then get a stream $v : \nn \to \Out{S}$ of output values by defining
$$v_t := \expose{S}(s_t).$$
The system $\Sys{S}$ is a way of transforming streams of input parameters into
streams of output values.
\begin{proposition}[Deterministic systems as stream transformers]
 Let $$\Sys{S} = \lens{\update{S}}{\expose{S}} : \lens{\State{S}}{\State{S}}
 \leftrightarrows \lens{\In{S}}{\Out{S}}$$
 be a deterministic system. Then for every $s_0 \in \State{S}$, we get a stream
 transformation function
 $$\fun{transform}_{\Sys{S}} : \In{S}^{\nn} \to \Out{S}^{\nn}$$
 Given by
 \begin{align*}
   \fun{transform}_{\Sys{S}}(p)_0 &= \expose{S}(s_0)\\
   \fun{transform}_{\Sys{S}}(p)_{t + 1} &= \expose{S}(\update{S}(s_t,\, p_t))
 \end{align*}
 where $s_{t+1} = \update{S}({s_t,\, p_t})$ is the trajectory given by $s_0$.
\end{proposition}

\begin{exercise}
 Say how the system of \cref{ex.transition_diagram_discrete_traj} acts as a
 stream transformer on the following streams:
 \begin{enumerate}
   \item $p_{2t} = \Green$ and $p_{2t + 1} = \Orange$.
   \item $p_t = \Green$.
   \item $p_0 = \Green$ and $p_t = \Orange$ for
     all $t > 0$.
     \qedhere
 \end{enumerate}
\end{exercise}

Later, in \cref{sec.representables}, we will see that given trajectories of
component systems, we get a trajectory of a whole wired system. Even better,
every trajectory of the whole wired system can be calculated this way.


%---- Subsection ----%
\subsection{Steady states}

A steady state of a system is a state which does not change. Steady states are
important because they are guarantees of stability: a vase in a steady state is doing great, a heart in a steady state is in need of attention.

\begin{definition}\label{def.steady_state_discrete}
 Let $$\Sys{S} = \lens{\update{S}}{\expose{S}} : \lens{\State{S}}{\State{S}}
 \leftrightarrows \lens{\In{S}}{\Out{S}}$$
 be a deterministic system. For input parameter $i \in \In{S}$ and output value
 $o \in \Out{S}$, an \emph{$\lens{i}{o}$-steady state} is a state $s \in \State{S}$
 such that
 \begin{align*}
   \update{S}(s, i) &= s, \\
   \expose{S}(s) &= o.
 \end{align*}
 We call the pair $\lens{i}{o}$ the \emph{chart} of the steady state.
\end{definition}

\begin{remark}
Its important to note that a steady state is relative to the input parameter
chosen. For example, in \cref{ex.transition_diagram_discrete}, the top left
state is steady for the input parameter $\Orange$ but not for
the input parameter $\Green$.
\end{remark}

Unlike with trajectories, a system need not necessarily have \emph{any} steady
states. For example, the $\Sys{Clock}$ has no steady states; it always keeps
ticking to the next hour. 

In the transition diagram of a finite deterministic system, steady states will be loops
that begin and end at the same node. Since the system is finite, we can arrange
the steady states by their chart into a $\In{S} \times \Out{S}$ matrix. For example, in
\cref{ex.transition_diagram_discrete}, we get the following $\{\Green,\, \Orange\} \times \{a,\, b\}$ matrix:
\begin{equation}\label{eqn.steady_state_matrix_transition_diagram_discrete}
\kbordermatrix{
  & \Green & \Orange\\
  a & \emptyset & \left\{ \begin{tikzcd}\LMO{a} \ar[loop left, orange, thick] \end{tikzcd} \right\}  \\
  b & \left\{ \begin{tikzcd}\LMO{b} \ar[loop left, dgreen, thick]\end{tikzcd} \right\}& \emptyset  \\
}
\end{equation}

This is a ``matrix of sets'', in that the entries are the actual sets of steady
states. If we just counted how many steady states there were for each
input-output pair, we would get this matrix:
\begin{equation}\label{eqn.steady_state_matrix_transition_diagram_discrete2}
\kbordermatrix{
  & \Green & \Orange\\
  a & 0 & 1  \\
  b & 1 & 0  \\
}
\end{equation}

In \cref{sec.steady_states_matrix_arithmetic}, we'll see that each wiring
diagram gives a formula for calculating the matrix of steady states of the
composite system from the matrices of steady states of the inner systems. 

\begin{exercise}\label{ex.find_steady_states1}
  What are the steady state matrices of systems $\Sys{S_1}$ and $\Sys{S_2}$ from
  \cref{ex.wiring_transition_diagrams}? What about the combined system $\Sys{S}$?
\end{exercise}

\paragraph{Steady-looking trajectories.}

The reason we are interested in steady states is that they are highly
predictable; if we know we are in a steady state, then we know we are always
going to get the same results. But it is possible for us to always get the same
outputs for the same input even though the internal state keeps changing. These
are special trajectories, and we call them \emph{steady-looking} trajectories.

\begin{definition}\label{def.steady_looking_trajectory_discrete}
  For $i \in \In{S}$ and $o \in \Out{S}$ of a system $\Sys{S}$, a \emph{$\lens{i}{o}$-steady
    looking trajectory} is a sequence of states $s : \nn \to \State{S}$ such
  that
  \begin{align*}
    \update{S}(s_t, i) &= s_{t + 1} \\
    \expose{S}(s_t) &= o
  \end{align*}
  for all $t \in \nn$. We call the pair $\lens{i}{o}$ the \emph{chart} of the
  steady-looking trajectory $s$.
\end{definition}

While the steady states of a wired together system can be calculated from those
of its components, this is not true for steady-looking trajectories.
Intuitively, this is because the internal systems can be exposing changing
outputs between eachother even while the eventual external output remains
unchanged.

\begin{exercise}\label{ex.steady_looking_trajectories}
Consider the wiring diagram:
\[
\Sys{S} \coloneqq 
\begin{tikzpicture}[oriented WD, bbx = .3cm, bby =.3cm, bb min width=.5cm, bb port length=2pt, bb port sep=1, every fit/.style={inner xsep=\bbx, inner ysep=\bby}
, baseline=(Outer.center)]
  \node[bb={1}{1}, fill=blue!10] (S1) {$\Sys{S_1}$};
  \node[bb={1}{1}, right= of S1, fill=blue!10] (S2) {$\Sys{S_2}$};

  \node[bb={1}{1}, fit={(S1) (S2)}] (Outer) {};

  \draw (Outer_in1) to (S1_in1);
  \draw (S1_out1) to (S2_in1);
  \draw (S2_out1) to (Outer_out1);
\end{tikzpicture}
\]


Find systems $\Sys{S_1}$ and $\Sys{S_2}$ and a steady-looking trajectory of the
wired system $\Sys{S}$ which is not steady-looking on the component systems.
\qedhere
\erase{
%% Here is my start at a solution
Here are two systems, $\Sys{S_1}$ and $\Sys{S_2}$ presented in terms of
transition diagrams. 

 First, let $\Set{Colors}
= \{\Red, \Blue\}$ and let $\Set{Bool} = \{\const{true}, \const{false}\}$. Here is our first system
$\Sys{S_1}$, which has interface $\lens{\Set{Bool}}{\Set{Colors}}$:
\begin{equation}\label{eqn.steady_looking_trajectories1}
\Sys{S_1} \coloneqq \begin{tikzpicture}[baseline=center]
	\node[draw] {
  \begin{tikzcd}[column sep=small]
    \LMO{\Blue} \ar[loop left, "\false"] \ar[rr, bend left, "\true"] \ar[rr, leftarrow, bend right, "\true"'] & & \LMO{\Red} \ar[loop right, "\false"]
  \end{tikzcd}
  };
\end{tikzpicture}
\end{equation}

Here is our second system, $\Sys{S_2}$, a $\lens{\Set{Colors}}{\Set{Bool}}$-system: 
\begin{equation}\label{eqn.steady_looking_trajectories2}
\Sys{S_2} \coloneqq \begin{tikzpicture}[baseline=center]
	\node[draw] {
  \begin{tikzcd}[column sep=small]
    \LMO{\true} \ar[loop left, red] \ar[loop right, blue]
  \end{tikzcd}
  };
\end{tikzpicture}
\end{equation}

When we wire these together, we get this system:
\begin{equation}\label{eqn.steady_looking_trajectories2}
\Sys{S} \coloneqq \begin{tikzpicture}[baseline=center]
	\node[draw] {
  \begin{tikzcd}[column sep=small]
    \LMO{\true} \ar[loop left, "\false"] \ar[rr, bend left, "\true"] \ar[rr, leftarrow, bend right, "\true"'] & & \LMO{\true} \ar[loop right, "\false"]
  \end{tikzcd}
  };
  As we can see, there is a $(\true, \true)$-steady-looking trajectory which hops between these two states, but always outputs $\true$ on an input of $\true$. However, the output of component system $\Sys{S_1}$ is alternating between $\Blue$ and $\Red$ as this trajectory progresses, so it is not steady-looking when restricted to $\Sys{S_1}$.
\end{tikzpicture}
\end{equation}
}%erased
  
\end{exercise}

%---- Subsection ----%
\subsection{Periodic orbits}\label{sec.periodic_orbit_discrete}

Even if the behavior of a system isn't perfectly steady, it may continually
repeat. To a reasonable approximation, the position of the earth around the sun
follows a cycle that repeats every year. Using this as a paradigmatic example,
we call these behaviors that repeat \emph{periodic orbits}.

\begin{definition}[Periodic orbit] \label{def.periodic_orbit_discrete}
  A $\lens{p}{v}$-trajectory $s : \nn \to \State{S}$ is \emph{periodic} if there
  exists a time $t_0 \in \nn_{\geq1}$, called the \emph{period}, such that $s_{t_0} = s_0$. If the sequence of
  parameters $p : \nn \to \In{S}$ is also periodic with the same period (in that $p_{t_0} = p_0$ as well), then we say that $s$ has \emph{periodic parameters}.
  

\end{definition}

\begin{remark}
  Note that when we say that a periodic orbit has periodic parameters, we assume
  that they are periodic with the same period. This has important but subtle
  consequences for our theorems concerning the composition of behaviors in
  \cref{sec.representables}. We explain the difference between a periodic orbit
  and a periodic orbit with periodic parameters in a more precise manner
  in \cref{rmk.periodic_parameters_versus_not}.
\end{remark}

\begin{remark}
  Note that a steady state is a periodic orbit (with periodic parameters) that has
  a period of $1$. 
\end{remark}

\begin{exercise}
Describe a periodic orbit with period $1$ that does not have periodic
parameters; how are they different from steady states? Are there any of these in systems $\Sys{S_1}$ and $\Sys{S_2}$ of \cref{ex.wiring_transition_diagrams}?
\end{exercise}

\begin{example}
  The $\Sys{Clock}$ system is an exemplary periodic system with a period of 12.
  The $\Sys{ClockWithDisplay}$ of \cref{eqn.whole_clock_system_box} has period 24.
\end{example}

\begin{exercise}\label{ex.wiring_transition_diagrams_periodic_orbits}
  What are the periodic orbits in the systems $\Sys{S_1}$ and $\Sys{S_2}$ of
  \cref{ex.wiring_transition_diagrams} with periodic parameters, and what are their periods? What about
  the combined system $\Sys{S}$?
\end{exercise}

\begin{exercise}\label{ex.wiring_transition_diagrams_periodic_parameters}
  Can you think of any periodic orbits in $\Sys{S_1}$ and $\Sys{S_2}$ of
  \cref{ex.wiring_transition_diagrams} which
  don't have periodic parameters? 
\end{exercise}

A trajectory might not get back to where it started, but may still end up being
periodic. We call these trajectories \emph{eventually} periodic orbits, since
they eventually end up in a repeating cycle of states.

\begin{definition}[Eventually periodic orbit] \label{def.eventually_periodic_orbit_discrete}
A $\lens{p}{v}$-trajectory $s : \nn \to
  \State{S}$ is \emph{eventually periodic} if there are times $t_0 < t_1 \in
  \nn$ such that $s_{t_0} = s_{t_1}$ \spiz{is this right?}. If the sequence of
  parameters $p : \nn \to \In{S}$ is also eventually periodic with the same period (in that $p_{t_0} = p_{t_1}$), then we say that $s$ has \emph{eventually periodic parameters}.

The \emph{period} of an eventually periodic trajectory is
  the smallest difference $t_1 - t_0$ between times such that $s_{t_0} = s_{t_1}$.
\end{definition}

%---- Subsection ----%
\subsection{Behaviors of deterministic systems}\label{sec.behaviors}

In the previous
\cref{sec.trajectory_discrete,sec.steady_state_discrete,sec.periodic_orbit_discrete},
we saw a number of different kinds of behaviors of dynamical systems. Not only
were there a lot of definitions in those sections, each of those definitions had
slight variants (like periodic orbits versus periodic orbits with periodic
parameters, or steady states versus steady-looking trajectories). In this
section, we'll define a general notion of behavior and see that we can package each of the above sorts of behavior into a single system in its own right, one that \emph{represents} that sort of
behavior. The representative system of a certain kind of behavior behaves in
exactly that way, and does nothing else.

To get started, we will give a formal definition of behavior of deterministic
system. Before we do this, we need to define another sort of map that can go
between the interfaces of systems.
\begin{definition}[Category of charts]\label{def.chart_discrete}
  A \emph{chart} $\lens{f_{\flat}}{f} : \lens{A^-}{A^+} \rightrightarrows
  \lens{B^-}{B^+}$ in a cartesian category $\cat{C}$ is a pair of maps $f : A^+ \to B^+$ and $f_{\flat} : A^+
  \times A^- \to B^-$. Note that \emph{this is not a lens}.
\end{definition}

\begin{exercise}
\begin{enumerate}
	\item How many lenses are there $\lens{f^\sharp}{f}\colon\lens{\3}{\2}\fromto\lens{\4}{\3}$?
	\item How many charts are there $\lens{f^\flat}{f}\colon\lens{\3}{\2}\tto\lens{\4}{\3}$?
\qedhere
\end{enumerate}
\end{exercise}

\begin{definition}[Behavior of deterministic systems] \label{def.behavior_discrete}
  Let $\Sys{T}$ and $\Sys{S}$ be deterministic systems. Given a chart of interfaces $\lens{f_{\flat}}{f}:
  \lens{\In{T}}{\Out{T}} \rightrightarrows \lens{\In{S}}{\Out{S}}$, a $\lens{f_{\flat}}{f}$-\emph{behavior of
  shape $\Sys{T}$ in $\Sys{S}$}, written
 $\phi :
\Sys{T} \to \Sys{S}$, is a function $\phi : \State{T} \to \State{S}$ sending
states of $\Sys{T}$ to states of $\Sys{S}$ which preserves the dynamics and
exposed variables by satisfying the following equations:
\begin{equation}\label{eqn.behavior_discrete}
\begin{aligned}
  \expose{S}(\phi(t)) &= f(\expose{T}(t)), \\
  \update{S}(\phi(t), f_{\flat}(\expose{T}(t), i)) &= \phi(\update{T}(t, i))
\end{aligned}
\end{equation}
for all $t \in \State{T}$ and $i \in \In{T}$. We say that $\lens{f_{\flat}}{f}$
is the chart of the behavior $\phi$.
\end{definition}

\begin{remark}
  If you prefer commutative diagrams to systems of equations, don't fret. We'll
  reinterpret \cref{eqn.behavior_discrete} in terms of commutative diagrams in \cref{sec.double_cat_arenas}
\end{remark}

\begin{remark}
  Suppose that we have transition diagrams for systems $\Sys{T}$ and $\Sys{S}$.
  Then a behavior of shape $\Sys{T}$ in $\Sys{S}$ will correspond to part of the
  transition diagram of $\Sys{S}$ which is shaped like the transition diagram of
  $\Sys{T}$. See the upcoming examples to see how this looks in practice.
\end{remark}

 Let's make this definition feel real with a few examples.

\begin{example}\label{ex.trajectory_as_behavior_discrete}
  Let $\Sys{Time}$ be the system $\lens{t\mapsto t+1}{\id}\colon\lens{\nn}{\nn}\fromto\lens{\{\fun{tick}\}}{\nn}$, i.e.\ with
\begin{itemize}
\item $\State{Time} \coloneqq \nn$,
\item $\Out{Time} \coloneqq \nn$,
\item $\In{Time} \coloneqq \{\fun{tick}\}$,
\item $\expose{Time} = \id$,
\item $\update{Time}(t, \ast) = t + 1$.
\end{itemize}

As a transition diagram, $\Sys{Time}$ looks like this:

\[
\begin{tikzpicture}
	\node[draw] {
  \begin{tikzcd}[column sep=30pt]
    \LMO{0} \ar[r, thick, "\fun{tick}"] & \LMO{1} \ar[r,  thick, "\fun{tick}"] & \LMO{2} \ar[r, thick, "\fun{tick}"] & \LMO{3} \ar[r, thick, "\fun{tick}"] & \LMO{4} \ar[r, thick, "\fun{tick}"] & \cdots
  \end{tikzcd}
  };
\end{tikzpicture}
\]

Let's see what a behavior of shape $\Sys{Time}$ in $\Sys{S}$ will be \spiz{what's $\Sys{S}$, arbitrary?}. We will expect the
shape of $\Sys{Time}$ to appear in the transition diagram of $\Sys{S}$, like
this:
\[
\begin{tikzpicture}[baseline=(Center)]
  \coordinate (Center) at (0,0);
	\node[draw] (Diag1) {
  \begin{tikzcd}[column sep=small]
    \LMO{0} \ar[r, red, thick] & \LMO{1} \ar[r, red, thick] & \LMO{2} \ar[r, red, thick] & \LMO{3} \ar[r, red, thick] & \LMO{4} \ar[r, red, thick] & \cdots
  \end{tikzcd}
  };
	\node[draw, right = of Diag1] (Diag2){
\begin{tikzcd}
\vdots                      & \vdots                      & \vdots                      & \vdots                      & \vdots \\
\LMO{} \arrow[r] \arrow[u]  & \LMO{} \arrow[r, red] \arrow[u]  & \LMO{} \arrow[r, red] \arrow[u]  & \LMO{} \arrow[u] \arrow[ru, red] &        \\
\LMO{} \arrow[r] \arrow[u]  & \LMO{} \arrow[r] \arrow[u, red]  & \LMO{} \arrow[u] \arrow[ru] &                             &        \\
\LMO{} \arrow[r] \arrow[u]  & \LMO{} \arrow[ru] \arrow[u, red] &                             &                             &        \\
\LMO{} \arrow[u] \arrow[ru, red] &                             &                             &                             &       
\end{tikzcd}
  };
  \draw[->, shorten <=2pt, shorten >=2pt] (Diag1) to (Diag2);
\end{tikzpicture}
\]

First, we need to know what a chart $\lens{f_{\flat}}{f} :
\lens{\In{Time}}{\Out{Time}} \rightrightarrows \lens{\In{S}}{\Out{S}}$ is like. Since
$\Out{Time} = \nn$ and $\In{Time} \cong \ord{1}$, this means $f : \nn \to \Out{S}$
is a sequence of outputs, and $f_{\flat} : \nn \times \ord{1} \to \In{S}$ is a
sequence of input parameters. We might as well instead call $f$ our sequence of
exposed values $v$, and $f_{\flat}$ our sequence of input parameters $p$, so
that we have a chart $\lens{p}{v} : \lens{\ord{1}}{\nn}
\rightrightarrows \lens{\In{S}}{\Out{S}}$.

Now, let's see what a $\lens{p}{v}$-behavior $\gamma : \Sys{Time} \to \Sys{S}$ is. It
is a function $\gamma
: \State{Time} \to 
\State{S}$ satsifying some properties. But $\State{Time} = \nn$, so $\gamma :
\nn \to \State{S}$ is a sequence of states in $\Sys{S}$. Now,
\cref{eqn.behavior_discrete} becomes the equations:
\begin{align*}
  \expose{S}(\gamma(t)) &= v(t) \\
  \update{S}(\gamma(t), p(t)) &= \gamma(t + 1).
\end{align*}
which are exactly the equations defining a $\lens{p}{v}$-trajectory from \cref{def.trajectory_discrete}!

\end{example}

\begin{example}\label{ex.steady_state_as_behavior_discrete}
  Consider the simple system $\Sys{Fix}$ with:
  \begin{itemize}
  \item $\State{Fix} = \{\ast\}$.
  \item $\Out{Fix} = \{\ast\}$.
  \item $\In{Fix} = \{\ast\}.$
  \item $\expose{Fix} = \id$.
  \item $\update{Fix}(\ast, \ast) = \ast$.
  \end{itemize}

  As a transition diagram, this looks like:
\[
\begin{tikzpicture}
	\node[draw] {
  \begin{tikzcd}[column sep=small]
    \LMO{\ast} \ar[loop left, "\ast"]
  \end{tikzcd}
  };
\end{tikzpicture}
\]

  A behavior $s : \Sys{Fix} \to \Sys{S}$ in an arbitrary system $\Sys{S}$ should be a loop of this shape within the transition
  diagram of $\Sys{S}$: a steady state.
\[
\begin{tikzpicture}
	\node[draw] (Diag1) {
  \begin{tikzcd}[column sep=small]
    \LMO{\ast} \ar[loop left, red, "\ast"]
  \end{tikzcd}
  };
  \node[draw, right = of Diag1]  (Diag2) {
  \begin{tikzcd}[column sep=small]
    \LMO{} \ar[out=120, in=90, loop] \ar[in=210, out=250, loop] \ar[rr, bend left = 10] \ar[rr, leftarrow,bend right= 10] \ar[ddr, leftarrow,  bend right= 10] &  & \LMO{} \ar[loop right] \ar[ddl, bend left= 10] \ar[ddl,leftarrow, bend right= 10]  \\
    & & \\
    & \LMO{} \ar[out=300, in=240, loop, red] & 
  \end{tikzcd}
  };
  \draw[->] (Diag1) to (Diag2); 
\end{tikzpicture}
\]

  Let's check that this works. First, we need to know what a chart $\lens{f_{\flat}}{f} :
  \lens{\In{Fix}}{\Out{Fix}} \rightrightarrows \lens{\In{S}}{\Out{S}}$ is. Since
  $\Out{Fix} = \In{Fix} = \{\ast\}$, we have that $f : \{\ast\} \to \Out{S}$ is
  simply an output value of $\Sys{S}$ and $f_{\flat} : \{\ast\}\times\{\ast\} \to
  \In{S}$ is simply an input parameter. Therefore, we might as well write $o$
  for $f$ and $i$ for $f_{\flat}$, to see that a chart $\lens{i}{o} :
  \lens{\{\ast\}}{\{\ast\}} \rightrightarrows \lens{\In{S}}{\Out{S}}$ is a pair
  of elements $i \in \In{S}$ and $o \in \Out{S}$.

  Now, let's see what a $\lens{i}{o}$-behavior $s : \Sys{Fix} \to \Sys{S}$ is. It is
  a function $s : \State{Fix} \to \State{S}$ satisfying a few properties. But
  $\State{S} = \{\ast\}$ so $s : \{\ast\} \to \State{S}$ is a single state of
  $\Sys{S}$. Then, \cref{eqn.behavior_discrete} becomes the equations
  \begin{align*}
    \expose{S}(s) &= o\\
    \update{S}(s, i) &= s
  \end{align*}
  which are precisely the equations defining a $\lens{i}{o}$-steady state from \cref{def.steady_state_discrete}.
  
\end{example}

\begin{example}\label{ex.periodic_orbit_as_behavior}
  Let $0 < n \in \nn$ be a positive natural number, and consider the system $\Sys{Clock_n}$
  having:
  \begin{itemize}
    \item $\State{Clock_n} = \ord{n} = \{1,\ldots, n\}$.
    \item $\Out{Clock_n} = \ord{n}$.
    \item $\In{Clock_n} = \{\ast\}$.
    \item $\expose{Clock_n} = \id$.
    \item $\update{Clock_n}(t, \ast) = \begin{cases} t + 1 &\mbox{if $t < n$}
        \\ 1 &\mbox{if $t = n$}  \end{cases}$.
  \end{itemize}

This is the clock with $n$ hours. Our example system $\Sys{Clock}$ from
\cref{ex.clock_system} is $\Sys{Clock}_{12}$, a clock with 12 hours. Here's what
$\Sys{Clock}_4$ looks like as a transition diagram:
\[
\begin{tikzpicture}
	\node[draw] {
  \begin{tikzcd}[sep=small]
    \LMO{1} \ar[rr, bend left, "\ast"] & & \LMO{2} \ar[dd, bend left, "\ast"] \\
    & & \\
\LMO{3} \ar[rr, leftarrow, bend right, "\ast"'] \ar[uu, bend left, "\ast"] & & \LMO{4}
  \end{tikzcd}
  };
\end{tikzpicture}
\]

A behavior $\gamma : \Sys{Clock_n} \to \Sys{S}$ should be a cycle like this in the
transition diagram of $\Sys{S}$: a periodic orbit. We can see the $\Sys{Clock_\4}$-behavior inside the system shown right:
\[
\begin{tikzpicture}
	\node[draw] (Diag1) {
  \begin{tikzcd}[sep=small]
    \LMO{1} \ar[rr, bend left, red, "\ast"] & & \LMO{2} \ar[dd, bend left, red, "\ast"] \\
    & & \\
\LMO{3} \ar[rr, leftarrow, bend right, red, "\ast"'] \ar[uu, bend left, red, "\ast"] & & \LMO{4}
  \end{tikzcd}
  };
  \node[draw, right= of Diag1] (Diag2) {\begin{tikzcd}[sep=small]
    \LMO{} \ar[loop left] \ar[rr, bend left, red] \ar[dd, leftarrow, bend right, red] &  & \LMO{} \ar[loop right] \ar[dd, bend left, red ]\\
    & & \\
    \LMO{} \ar[loop left] \ar[rr, leftarrow, bend left] \ar[rr, leftarrow, bend right, red] & & \LMO{}
  \end{tikzcd}
};
\draw[->] (Diag1) to (Diag2);
\end{tikzpicture}
\]

Let's check that this works. First, we need to know what a chart
$\lens{f_{\flat}}{f} : \lens{\In{Clock_n}}{\Out{Clock_n}} \rightrightarrows
\lens{\In{S}}{\Out{S}}$ is. Since $\Out{Clock_n} = \ord{n}$ and $\In{Clock_n} =
\{\ast\}$, $f : \ord{n} \to \Out{S}$ is a sequence of $n$ exposed values of
$\Sys{S}$ while $f_{\flat} : \ord{n} \times \{\ast\} \to \In{S}$ is a sequence
of $n$ parameters. Therefore, we might as well write $v$ for $f$ and $p$ for
$f_{\flat}$ to find that a chart $\lens{p}{v} : \lens{\{\ast\}}{\ord{n}}
\rightrightarrows \lens{\In{S}}{\Out{S}}$ consists of an $n$-length sequence of
parameters and an $n$-length sequence of exposed values. 

A $\lens{p}{v}$-behavior $\gamma : \Sys{Clock_n} \to \Sys{S}$, then, is a function
$\gamma : \State{Clock_n} \to \State{S}$ satisfying a few properties. Since
$\State{Clock_n} = \ord{n}$, $\gamma : \ord{n} \to \State{S}$ is a $n$-length
sequence of states of $S$, and \cref{eqn.behavior_discrete} become the equations
\begin{align*}
  \expose{S}(\gamma(t)) &= v(t) \\
  \update{S}(\gamma(t), p(t)) &= \begin{cases} \gamma(t + 1) &\mbox{if $t < n$} \\
\gamma(1) &\mbox{if $t = n$} \end{cases}.
\end{align*}
As we can see, this determines a sequence of length $n$ of states of $\Sys{S}$
which repeats when it gets to the end. In other words, this is a periodic orbit
with periodic parameters as in \cref{def.periodic_orbit_discrete}!
\end{example}

If we have a certain kind of behavior in mind, and we find a system $\Sys{T}$ so
that behaviors of shape $\Sys{T}$ are precisely this kind of behavior, then we
say that $\Sys{T}$ \emph{represents} that behavior. For example, we have just
seen that:
\begin{itemize}
  \item The system $\Sys{Time} = \lens{\_ + 1}{\id} : \lens{\nn}{\nn} \leftrightarrows
    \lens{\{\ast\}}{\nn}$ represents trajectories.
  \item The system $\Sys{Fix} = \lens{\pi_2}{\id} : \lens{\{\ast\}}{\{\ast\}}
    \leftrightarrows \lens{\{\ast\}}{\{\ast\}}$ represents steady states.
  \item The systems $\Sys{Clock_n} = \lens{\_ + 1 \mod n}{\id} :
    \lens{\ord{n}}{\ord{n}} \leftrightarrows \lens{\{\ast\}}{\ord{n}}$
    represents periodic orbits with periodic parameters whose period divides $n$.
\end{itemize}

Note that there is always a particularly simple behavior on a system: the
identity behaviors $\id : \State{T} \to \State{T}$. This says that every system
behaves as itself. In particular, $\Sys{Time}$ has a trajectory behavior given by
$\id : \Sys{Time} \to \Sys{Time}$ (namely, the trajectory $s_t = t$), and $\Sys{Fix}$ has a steady state behavior given by $\id : \Sys{Fix} \to \Sys{Fix}$ (namely,
the steady state $\ast$), etc. We refer to the identity behavior of $\Sys{T}$ as the \emph{generic}
behavior of type $\Sys{T}$.

\begin{exercise}\label{ex.find_representatives_discrete}
  Find a representative system for the following kinds of behavior.
  \begin{enumerate}
    \item An eventually periodic orbit (see \cref{def.eventually_periodic_orbit_discrete}) that takes $n$ steps to get to a period
      of size $m$.
    \item A steady-looking trajectory (see \cref{def.steady_looking_trajectory_discrete}).
    \item A periodic orbit of period at most $n$ whose parameters aren't
      necessarily also periodic (see \cref{def.periodic_orbit_discrete}).
     \qedhere
  \end{enumerate}
\end{exercise}

\begin{remark}\label{rmk.periodic_parameters_versus_not}
As \cref{ex.find_representatives_discrete} shows, the difference between a
periodic orbit and a periodic orbit with periodic parameters can be surmised
precisely by noting that they are represented by systems with different
interfaces. The dynamics of the systems are the same, but the interfaces (and
accordingly, the exposed variable) are different; this explains how the
difference between a periodic orbit and a periodic orbit with periodic
parameters is all in the chart.
\end{remark}

\begin{exercise}\label{ex.represents_what_discrete}
  What kind of behaviors do the following systems represent? First, figure out
  what kind of charts they have, and then see what a behavior with a given chart
  is. Describe in your own words.

  \begin{enumerate}
  \item The system $\Sys{Plus}$ with:
    \begin{itemize}
    \item $\State{Plus} = \nn$.
    \item $\Out{Plus} = \nn$.
    \item $\In{Plus} = \nn$.
    \item $\expose{Plus} = \id$.
    \item $\update{Plus}(t, j) = t + j$.
    \end{itemize}
    %% A chart is a sequence of output values and an a $\nn \times \nn$ grid of
    %% input values. A behavior is a sequence of states with those output values
    %% where state $s_t$ updates on parameter $(t, j)$ to $s_{t + j}$.
  \item The system $\Sys{T_n}$ with:
    \begin{itemize}
    \item $\State{T_n} = \nn$.
    \item $\Out{T_n} = \{0, \ldots, n - 1\}$.
    \item $\In{T_n} = \{\ast\}$.
    \item $\expose{T_n}(t) = t \mod n$.
    \item $\update{T_n}(t, \ast) = t + 1$.
    \end{itemize}
    %% Its a trajectory that looks periodic, and has periodic parameters.
  \item The system $\Sys{XOR}$ with:
    \begin{itemize}
    \item $\State{XOR} = \Set{Bool} = \{\true,\, \false\}$.
    \item $\Out{XOR} = \Set{Bool}$.
    \item $\In{XOR} = \Set{Bool}$.
    \item $\expose{XOR} = \id$.
    \item \[\begin{aligned}
        &\update{XOR}(\true, \true) &= \false, \\
        &\update{XOR}(\false, \true) &= \true, \\
        &\update{XOR}(\true, \false) &= \true, \\
        &\update{XOR}(\false, \false) &= \false.
          \end{aligned}\]
    \end{itemize}
  \item The system $\Sys{List_C}$ for a set of \emph{choices} $C$ with:
    \begin{itemize}
      \item $\State{List_C} = \Set{List}_C$ is the set of lists of elements in
        $C$.
      \item $\Out{List_C} = \Set{List}_C$.
      \item $\In{List_C} = C$.
      \item $\expose{List_C} = \id$.
      \item $\update{List_C}(\ell, c) = c::\ell$, that is, we update a list by
        appending the character $c \in C$ to the start.
    \end{itemize}
  \end{enumerate}
\end{exercise}

While every system $\Sys{T}$ represents some kind of behavior --- just take the kind of
behavior to be exactly described by behaviors $\Sys{T} \to \Sys{S}$ --- we are
most interested in those simple systems $\Sys{T}$ whose behavior we can fully
understand. 


We have written a behavior of shape $\Sys{T}$ in $\Sys{S}$ with an arrow $\phi :
\Sys{T} \to \Sys{S}$. This suggests that there is a category with deterministic
systems as its objects and behaviors as its morphisms; and there is!

\begin{definition}\label{def.category_of_charts}
  The category $\Cat{Chart}_{\cat{C}}$ of charts in $\cat{C}$ has
  \begin{itemize}
    \item Objects the \emph{arenas} $\lens{A^-}{A^+}$, pairs of objects in $\cat{C}$.
    \item Maps the \emph{charts} $\lens{f_{\flat}}{f} : \lens{A^-}{A^+}
      \tto \lens{B^-}{B^+}$.
    \item Composition the composite of a chart $\lens{f_{\flat}}{f} : \lens{A^-}{A^+}
      \tto \lens{B^-}{B^+}$ with a chart $\lens{g_{\flat}}{g} : \lens{B^-}{B^+}
      \tto \lens{C^-}{C^+}$ is 
$$\lens{g_{\flat}}{g} \circ \lens{f_{\flat}}{f} \coloneqq \lens{(a^+, a^-)
  \mapsto g_{\flat}(f(a^+), f_{\flat}(a^+, a^-))}{g \circ f}.$$
     \item The identity chart is $\lens{\pi_2}{\id} : \lens{A^-}{A^+}
       \rightrightarrows \lens{A^-}{A^+}$.
  \end{itemize}
\end{definition}
\begin{exercise}
  Check that $\Cat{Chart}_{\cat{C}}$ is indeed a category. That is,
  \begin{enumerate}
    \item For charts $\lens{f_{\flat}}{f} : \lens{A^-}{A^+}
      \tto \lens{B^-}{B^+}$, $\lens{g_{\flat}}{g} : \lens{B^-}{B^+}
      \tto \lens{C^-}{C^+}$, and $\lens{h_{\flat}}{h} : \lens{C^-}{C^+}
      \tto \lens{D^-}{D^+}$, show that 
$$\lens{h_{\flat}}{h} \circ \left( \lens{g_{\flat}}{g} \circ \lens{f_{\flat}}{f}
\right) = \left( \lens{h_{\flat}}{h} \circ \lens{g_{\flat}}{g} \right) \circ \lens{f_{\flat}}{f}.$$
    \item For a chart $\lens{f_{\flat}}{f} : \lens{A^-}{A^+}
      \tto \lens{B^-}{B^+}$, show that 
$$ \lens{\pi_2}{\id} \circ \lens{f_{\flat}}{f} = \lens{f_{\flat}}{f} =
\lens{f_{\flat}}{f} \circ \lens{\pi_2}{\id}.$$
  \end{enumerate}
\end{exercise}

\begin{exercise}\label{ex.special_charts}
  What are the charts of the following forms in simpler terms? 
  \begin{enumerate}
    \item $\lens{f_{\flat}}{f} : \lens{\ord{1}}{\ord{1}} \tto \lens{A^-}{A^+}$.
    \item $\lens{f_{\flat}}{f} : \lens{A^-}{A^+} \tto \lens{\ord{1}}{\ord{1}}$.
    \item $\lens{f_{\flat}}{f} : \lens{\ord{1}}{A^+} \tto \lens{B^-}{B^+}$.
   \end{enumerate}
\end{exercise}

\begin{proposition}
  There is a category $\Cat{Sys}$ with deterministic systems as its objects and
  where a map $\Sys{T} \to \Sys{S}$ is a pair consisting of a chart
  $\lens{f_{\flat}}{f} : \lens{\In{T}}{\Out{T}} \rightrightarrows
  \lens{\In{S}}{\Out{S}}$ and a $\lens{f_{\flat}}{f}$-behavior $\phi : \Sys{T}
  \to \Sys{S}$. Composition is given by composing both the charts and the
  functions on states, and identities are given by the generic behaviors: the identity chart with the
  identity function $\id : \State{T} \to \State{T}$.
\end{proposition}
\begin{proof}
  We just need to check that the composite $\psi \circ \phi$ of two behaviors $\phi : \Sys{T} \to
  \Sys{S}$ and $\psi : \Sys{S} \to \Sys{U}$ with charts $\lens{f_{\flat}}{f} : \lens{\In{T}}{\Out{T}} \rightrightarrows
  \lens{\In{S}}{\Out{S}}$ and $\lens{g_{\flat}}{g} : \lens{\In{S}}{\Out{S}} \rightrightarrows
  \lens{\In{U}}{\Out{U}}$ is a behavior with chart $\lens{g_{\flat}}{g} \circ
  \lens{f_{\flat}}{f}$. That is, we need to check that
  \cref{eqn.behavior_discrete} is satisfied for $\psi \circ \phi$. We can do
  this using the fact that it is satisfied for both $\psi$ and $\phi$.
  \begin{align*}
    \expose{U}(\psi(\phi(t))) &= \psi(\expose{S}(\phi(t))) \\
                              &= \psi(\phi(\expose{T}(t))).\\
  \end{align*}
  \begin{align*}
    \update{U}(\psi(\phi(t)), &g_{\flat}(f(\expose{T}(t)), f_{\flat}(\expose{T}(t), i)))  \\
                              &=\update{U}(\psi(\phi(t)), g_{\flat}(\expose{S}(\phi(t)), f_{\flat}(\expose{T}(t), i))) \\
                              &= \psi(\update{S}(\phi(t), f_{\flat}(\expose{T}(t), i))) \\
    &= \psi(\phi(\update{T}(t, i))).
    &\qedhere
  \end{align*}
\end{proof}

There are two different ways to understand what composition of behaviors means:
one based on post-composition, and the other based on pre-composition.
\begin{itemize}
  \item We see that any behavior $\Sys{S} \to \Sys{U}$ gives a way of turning
    $\Sys{T}$-shaped behaviors in $\Sys{S}$ to $\Sys{T}$-shaped behaviors in $\Sys{U}$.
  \item We see that any behavior $\Sys{T} \to \Sys{S}$ gives a way of turning
    $\Sys{S}$-shaped behaviors in $\Sys{U}$ into $\Sys{T}$-shaped behaviors in $\Sys{U}$.
\end{itemize}

\begin{example}\label{ex.steady_state_gives_trajectory_discrete}
  Any steady state $s$ can be seen as a particularly simple trajectory: $s_t =
  s$ for all $t$. We have seen in
  \cref{ex.steady_state_as_behavior}
  that steady states are $\Sys{Fix}$-shaped behaviors. We can use composition of behaviors to
  understand how steady states give rise to trajectories.

  The generic steady state $\ast$ of $\Sys{Fix}$ (that is, the identity behavior of
  $\Sys{Fix}$) generates a trajectory $s : \nn \to \State{Fix}$ with input
  parameters $p_t = \ast$ and $s_t = \ast$. This gives us a behavior $s :
  \Sys{Time} \to \Sys{Fix}$.
  
  Now, for every steady state $\gamma : \Sys{Fix} \to \Sys{S}$, we may compose
  to get a trajectory $\gamma \circ s : \Sys{Time} \to \Sys{S}$.
\end{example}

\begin{exercise}\label{ex.behaviors_as_change_of_kind_discrete}
Adapt the argument of \cref{ex.steady_state_gives_trajectory_discrete} to show
that
\begin{enumerate}
  \item Any eventually periodic orbit gives rise to a trajectory.
  \item If $n$ divides $m$, then any orbit of period at most $n$ gives rise to
    an orbit of period of most $m$.
\qedhere
\end{enumerate}
\end{exercise}

While we will often be interested in behaviors of systems that change the
interface in the sense of having non-trivial charts, we will also be interested
in behaviors of systems that do not changed the exposed variables at all.

\begin{definition}\label{def.cat_of_systems_discrete}
  Let $\lens{I}{O}$ be an arena. The category
$$\Cat{Sys}\lens{I}{O}$$
of deterministic $\lens{I}{O}$-systems has as objects the systems
$\lens{\update{S}}{\expose{S}} : \lens{\State{S}}{\State{S}} \fromto
\lens{I}{O}$ with interface $\lens{I}{O}$ and as maps the behaviors $\phi :
\Sys{T} \to \Sys{S}$ whose chart is the identity chart on $\lens{I}{O}$.
\end{definition}

\paragraph{Isomorphisms of Systems}

Now that we have a category of systems and behaviors, category theory supplies
us with a definition of isomorphism for systems. 
\begin{definition}\label{def.isomorphism_of_systems_discrete}
  An \emph{isomorphism} of a system $\Sys{T}$ with a system $\Sys{S}$ is a
  a behavior $\phi : \Sys{T} \to \Sys{S}$ for which there is another behavior
  $\phi\inv : \Sys{S} \to \Sys{T}$ such that $\phi \circ \phi \inv =
  \id_{\Sys{S}}$ and $\phi \inv \circ \phi = \id_{\Sys{T}}$.
\end{definition}

Let's see that this is indeed a good notion of sameness for systems.
\begin{proposition}\label{prop.isomorphism_of_systems_discrete}
  A behavior $\phi : \Sys{T} \to \Sys{S}$ is an isomorphism if and only if the
  following conditions hold:
\begin{enumerate}
  \item The map $\phi : \State{T} \to \State{S}$ is an isomorphism of sets --- a
    bijection.
  \item The chart $\lens{f_{\flat}}{f} : \lens{\In{T}}{\Out{T}}
    \rightrightarrows \lens{\In{S}}{\Out{S}}$ of $\phi$ is an isomorphism in
    $\Cat{Chart}_{\smset}$. That is, $f : \Out{T} \to \Out{S}$ is a bijection
    and there is a bijection $f_{\flat}' : \In{T} \to \Out{T}$ such that
    $f_{\flat} = f_{\flat}' \circ \pi_2$.
\end{enumerate}
\end{proposition}
\begin{proof}
  Since composition in the category of systems and behaviors is given by
  composition of the underlying charts and maps, $\phi$ is an isomorphism of
  systems if and only if its action on states is a bijection and its chart is an
  isomorphism in the category of charts. It just remains to see that our
  description of isomorphism of charts is accurate, which we leave to \cref{ex.isomorphism_in_category_of_charts}.
\end{proof}

\begin{exercise}\label{ex.isomorphism_in_category_of_charts}
  Show that a chart $\lens{f_{\flat}}{f} : \lens{A^-}{A^+}
    \rightrightarrows \lens{B^-}{B^+}$ is an isomorphism if and only if $f$ is
    an isomorphisms and there is an isomorphism $f_{\flat}' : A^- \to B^-$ such
    that $f_{\flat} = f_{\flat}' \circ \pi_2$. 
\end{exercise}

%---- Subsection ----%
\subsection{Steady states compose according to the laws of matrix arithmetic}\label{sec.steady_states_matrix_arithmetic}


We have seen how we can compose systems, and we have seen how systems behave. We
have seen a certain composition of behaviors, a form of transitivity that says
that if we have a $\Sys{T}$-shaped behavior in $\Sys{S}$ and a $\Sys{S}$-shaped
behavior in $\Sys{U}$, then we get a $\Sys{T}$-shaped behavior in $\Sys{U}$. But what's the relationship between composing systems and composing their behaviors?

This question will be a major theme of this book. In this section we will give a
taste by showing how steady states compose. Later, in \cref{sec.representables}, we will see a very abstract
theorem that generalizes what we do here for steady states to something that works for \emph{all behaviors}.
But in order for that abstract theorem to make sense, we should first see the concrete
case of steady states in detail.  

Recall that the chart of a steady state $s \in \State{S}$ is the pair
$\lens{i}{o}$ with $o = \expose{S}(s)$ and $\update{S}(s, i) = s$. The set of all
possible charts for steady states is therefore $\In{S} \times \Out{S}$, and for
every chart $\lens{i}{o}$ we have the set $\Set{Steady}_{Sys{S}}\lens{i}{o}$ of
steady states for this chart. 

We can see this function $\Set{Steady}_{\Sys{S}} : \In{S} \times \Out{S} \to \smset$ as a
\emph{matrix of sets} with $\Set{Steady}_{\Sys{S}}\lens{i}{o}$ in the row $i$
and column $o$. For example, consider system $\Sys{S_1}$ of
\cref{ex.wiring_transition_diagrams}:
\begin{equation}\label{eqn.wiring_transition_diagrams_steady_diag1}
\Sys{S_1} \coloneqq \begin{tikzpicture}[baseline=(Center)]
  \coordinate (Center) at (0,0);
	\node[draw] {
  \begin{tikzcd}[column sep=small]
    \LMOO{\const{s_{11}}}{\Blue} \ar[loop left, "\const{false}"] \ar[rr, bend left, "\const{true}"] \ar[dd, leftarrow, bend right, "\true"'] &  & \LMOO{\const{s_{12}}}{\Red} \ar[loop right, "\true"] \ar[dd, bend left, "\const{false}" ]\\
    & & \\
    \LMOO{\const{s_{13}}}{\Blue} \ar[loop left, "\false"] \ar[rr, leftarrow, bend left, "\false"] \ar[rr, leftarrow, bend right, "\true"'] & & \LMOO{\const{s_{14}}}{\Green}
  \end{tikzcd}
  };
\end{tikzpicture}
\end{equation}
This has output value set $\Set{Colors} = \{\Blue, \Red, \Green\}$ and input
parameter set $\Set{Bool} = \{\true, \false\}$. Here is its $(\Set{Colors}
\times \Set{Bool})$ steady state matrix:
\begin{equation}\label{eqn.steady_state_matrix1}
 \Set{Steady}_{\Sys{S_1}} =
  \kbordermatrix{
    & \Blue & \Red & \Green \\
    \true & \emptyset & \left\{ \begin{tikzcd} \LMOO{\const{s_{12}}}{\Red} \ar[loop right,
        "\true"]\end{tikzcd} \right\}  & \emptyset \\
    \false & \left\{ \begin{tikzcd} \LMOO{\const{s_{11}}}{\Blue} \ar[loop left,
        "\false"] \end{tikzcd}, \begin{tikzcd} \LMOO{\const{s_{13}}}{\Blue} \ar[loop left,
        "\false"] \end{tikzcd} \right\} & \emptyset & \emptyset
}    
\end{equation}
If we just want to know how many $\lens{i}{o}$-steady states there are, and not
precisely which states they are, we can always take the cardinality of the sets
in our matrix of sets to get a bona-fide matrix of numbers. Doing this to the
above matrix gives us the matrix
 \[\kbordermatrix{
    & \Blue & \Red & \Green \\
    \true & 0 & 1 & 0 \\
    \false & 2 & 0 & 0
}    
\]

Now, let's take a look at system $\Sys{S_2}$ from the same exercise:
\[
\Sys{S_2} \coloneqq \begin{tikzpicture}[baseline=(bl)]
	\node[draw] (bl) {
  \begin{tikzcd}[column sep=small]
    \LMOO{\const{s_{21}}}{\true} \ar[out=120, in=90, loop, red] \ar[in=210, out=250, loop, blue] \ar[rr, bend left = 10, dgreen] \ar[rr, leftarrow, red, bend right= 10] \ar[ddr, leftarrow, dgreen, bend right= 10] &  & \LMOO{\const{s_{22}}}{\false} \ar[loop right, dgreen] \ar[ddl, blue, bend left= 10] \ar[ddl,red, leftarrow, bend right= 10]  \\
    & & \\
    & \LMOO{\const{s_{23}}}{\true} \ar[out=300, in=240, loop, blue] & 
  \end{tikzcd}
  };
\end{tikzpicture}
\]

This has steady state matrix:
\begin{equation}\label{eqn.steady_state_matrix2}
  \Set{Steady}_{\Sys{S_2}} = \kbordermatrix{
    & \true & \false \\
    \Blue & \left\{ \begin{tikzcd} \LMOO{\const{s_{21}}}{\true}  \ar[ loop left,
        blue] \end{tikzcd}, \begin{tikzcd}\LMOO{\const{s_{23}}}{\true} \ar[loop left,
        blue] \end{tikzcd}\right\} & \emptyset \\
    \Red & \left\{ \begin{tikzcd} \LMOO{\const{s_{21}}}{\true} \ar[loop left,
        red] \end{tikzcd} \right\}& \emptyset \\
    \Green & \emptyset & \left\{ \begin{tikzcd} \LMOO{\const{s_{22}}}{\false} \ar[loop left,
        dgreen]
      \end{tikzcd} \right\}
}
\end{equation}
Or, again, if we just want to know how many steady states there are for each
chart:
\[
  \Set{Steady}_{\Sys{S_2}} = \kbordermatrix{
    & \true & \false \\
    \Blue & 2 & 0 \\
    \Red & 1 & 0 \\
    \Green & 0 & 1
}
\]

We can wire these systems together to get a system $\Sys{S}$:
\[
\Sys{S} \coloneqq 
\begin{tikzpicture}[oriented WD, bbx = .3cm, bby =.3cm, bb min width=.5cm, bb port length=2pt, bb port sep=1, every fit/.style={inner xsep=\bbx, inner ysep=\bby}
, baseline=(Outer.center)]
  \node[bb={1}{1}, fill=blue!10] (S1) {$\Sys{S_1}$};
  \node[bb={1}{1}, fill=blue!10, right= of S1] (S2) {$\Sys{S_2}$};

  \node[bb={1}{1}, fit={(S1) (S2)}] (Outer) {};

  \draw (Outer_in1) to (S1_in1);
  \draw (S1_out1) to (S2_in1);
  \draw (S2_out1) to (Outer_out1);
\end{tikzpicture}
\]

With just a bit of thought, we can find the steady states of this systems without fully calculating its
dynamics. A state of $\Sys{S}$ is a pair of states $s_1 \in  \State{S_1}$ and
$s_2 \in \State{S_2}$, so for it to be steady both its constituent states must be steady.
So let $\lens{i}{o} : \lens{\ord{1}}{\ord{1}} \tto
\lens{\Set{Bool}}{\Set{Bool}}$ be a chart for $\Sys{S}$ --- a pair of booleans.
We need $s_1$ and $s_2$ to both be steady, so in particular $s_1$ must be steady
at the input $i$, and $s_2$ must expose $o$; but, most importantly, $s_2$ must then be steady at the input
$\expose{S_1}(s_1)$ which $s_1$ exposes.

So, to find the set of
$\lens{\true}{\true}$-steady states of $\Sys{S}$, we must a state of
$\Sys{S_1}$ which is steady for the input $\true$ and then a steady state of
$\Sys{S_2}$ whose input is what that state outputs and whose output is $\true$.
There are three pieces of data here: the state of $\Sys{S_1}$, the state of
$\Sys{S_2}$, and the intermediate value expose by the first state and input into
the second state. We can therefore describe the set of $\lens{\true}{\true}$-steady states of
$\Sys{S}$ like this:
\begin{align*}
  \Set{Steady}_{\Sys{S}}\lens{\true}{\true} &= \left\{ (m, s_1, s_2)
  \middle| \begin{aligned}
    s_1 &\in \Set{Steady}_{\Sys{S_1}}\lens{\true}{m},
    s_2 &\in \Set{Steady}_{\Sys{S_2}}\lens{m}{\true}
  \end{aligned}\right\} \\
  &= \sum_{m \in \Set{Colors}} \Set{Steady}_{\Sys{S_1}} \lens{\true}{m} \times \Set{Steady}_{\Sys{S_2}}\lens{m}{\true}.
\end{align*}

This formula looks very suspiciously like matrix multiplication! Indeed, if we
multiply the matrices of numbers of steady states from $\Sys{S_1}$ and
$\Sys{S_2}$, we get:
\[\kbordermatrix{
    &  &  &  \\
    \true & 0 & 1 & 0 \\
    \false & 2 & 0 & 0
}   
\kbordermatrix{
    & \true & \false \\
     & 2 & 0 \\
     & 1 & 0 \\
     & 0 & 1
}
= \kbordermatrix{
  & \true & \false \\
  \true & 1 & 0 \\
  \false & 4 & 0 
} 
\]
which is the matrix of how many steady states $\Sys{S}$ has! What's even more
suspicious is that our wiring diagram for $\Sys{S}$ looks a lot like the string
diagram we would use to describe the multiplication of matrices:
\[
\begin{tikzpicture}[oriented WD, bbx = .3cm, bby =.3cm, bb min width=.5cm, bb port length=2pt, bb port sep=1, every fit/.style={inner xsep=\bbx, inner ysep=\bby}
, baseline=(Outer.center)]
  \node[bb={1}{1}, fill=blue!10] (S1) {$\Sys{S_1}$};
  \node[bb={1}{1}, fill=blue!10, right= of S1] (S2) {$\Sys{S_2}$};

  \node[bb={1}{1}, fit={(S1) (S2)}] (Outer) {};

  \draw (Outer_in1) to (S1_in1);
  \draw (S1_out1) to (S2_in1);
  \draw (S2_out1) to (Outer_out1);
\end{tikzpicture} \quad\quad\quad\quad
\begin{tikzpicture}[oriented WD, bb small, bb port length=5pt, baseline=(f)]
	\node[bb={1}{1}, rounded corners=5pt, draw=dgreen] (f) {$\Set{Steady}_{\Sys{S_1}}$};
	\node[bb={1}{1}, rounded corners=5pt, draw=dgreen, right=3 of f] (g) {$\Set{Steady}_{\Sys{S_2}}$};
	\node[left=0 of f_in1] {$\Set{Bool}$};
	\node[right=0 of g_out1] {$\Set{Bool}$};
	\draw (f_out1) to node[above, font=\scriptsize] {$\Set{Colors}$} (g_in1);
\end{tikzpicture}
\]
This can't just be a coincidence. Luckily for our sanity, it isn't. In the
remainder of this section, we will show how various things one can do with
matrices --- multiply them, trace them, Kronecker product them --- can be done
for matrices of sets, and how if your wiring diagram looks like its telling you
to that thing, then you can do that thing to the steady states of your internal
systems to get the steady states of the whole wired system

\paragraph{Matrices of sets}\label{sec.matrix_of_sets}

We'll be working with matrices of sets --- now and in the coming section ---
quite a bit, so we should really nail them down. Matrices of sets work a lot
like matrices of numbers, especially when the sets are finite; then they are
very nearly the same thing as matrices of whole numbers. But the matrix
arithmetic of infinite sets works just the same as with finite sets, so we'll do
everything in that generality.\footnote{This will help us later when we deal
  with behaviors that have more complicated charts. For example, even finite
  systems can have infinitely many different trajectories, so we really need the
  infinite sets.}

\begin{definition}\label{def.matrix_of_sets}
  Let $A$ and $B$ be two sets. $B \times A$ \emph{matrix of sets} is a dependent
  set $M : B \times A \to \smset$. For $a \in A$ and $b \in B$, we write
  $M_{ba}$ or $M_{(b, a)}$ for set indexed by $a$ and $b$, and call this the
  $(b,a)$-entry of the matrix $M$.


  We draw of matrix of sets with the following string diagram:
  \[
\begin{tikzpicture}[oriented WD, bb small, bb port length=5pt, baseline=(f)]
	\node[bb={1}{1}, rounded corners=5pt, draw=dgreen] (f) {$M$};
	\node[left=0 of f_in1] {$A$};
  \node[right=0 of f_out1] {$B$};
\end{tikzpicture}
  \]
\end{definition}
\begin{remark}
  We can see a dependent set $X_{-} : A \to \smset$ through the matrix of sets
  point of view as a \emph{vector of sets}. This is because $X_{-}$ is
  equivalently given by $X_{-} : A \times \ord{1} \to \smset$, which we see is a
  $A \times \ord{1}$ matrix of sets. A $n \times 1$ matrix is equivalently a
  column vector.
\end{remark}

Now we'll go through and define the basic operations of matrix arithmetic:
mutliplication, Kronecker product (also known as the tensor product), and
partial trace.

\begin{definition}\label{def.matrix_of_sets_multiplication}
  Given an $B \times A$ matrix of sets $M$ and a  $C \times B$ matrix of sets
  $N$, their \emph{product} $NM$ (or $M \times_B N$ for emphasis) is the $C
  \times A$ matrix of sets with entries
  $$NM_{ca} = \sum_{b \in B}  N_{cb}\times M_{ba}.$$

  We draw the multiplication of matrices of sets with the following string
  diagram:
  \[
\begin{tikzpicture}[oriented WD, bb small, bb port length=5pt, baseline=(f)]
	\node[bb={1}{1}, rounded corners=5pt, draw=dgreen] (f) {$M$};
	\node[bb={1}{1}, rounded corners=5pt, draw=dgreen, right=1.5 of f] (g) {$N$};
	\node[left=0 of f_in1] {$A$};
	\node[right=0 of g_out1] {$C$};
	\draw (f_out1) to node[above, font=\scriptsize] {$B$} (g_in1);
\end{tikzpicture}
  \]
   
  The identity matrix $I_A$ is an $A \times A$ matrix with entries
  $$I_{aa'} = \begin{cases} \ord{1} &\mbox{if $a = a'$} \\ \emptyset &\mbox{if
      $a \neq a'$} \end{cases}.$$

  We draw the identity matrix as a string with no beads on it.
  \[
\begin{tikzpicture}[oriented WD, bb small, bb port length=5pt, baseline=(Left)]
  \node (Left) (Left){};
  \node[right = 3 of Left](Right) {};
  \draw (Left.center) -- (Right.center);
  \node[left=0 of Left] {$A$};
  \node[right=0 of Right] {$A$};
\end{tikzpicture}
  \]
  
\end{definition}

\begin{exercise}\label{ex.matrix_of_sets_mult_laws}
  Multiplication of matrices of sets satisfies the usual properties of
  associativity and unity, but only up to isomorphism. Let $M$ be a $B \times A$
  matrix, $N$ a $C \times B$ matrix, and $L$ a $D \times C$ of sets. Show that
  \begin{enumerate}
   \item  For all $a \in A$ and $d \in D$, $((LN)M)_{da} \cong (L(NM))_{da}$.
   \item For all $a \in A$ and $b \in B$, $(MI_A)_{ba} \cong M_{ba} \cong (I_BM)_{ba}$.
  \end{enumerate}
\end{exercise}

\begin{remark}
  The isomorphisms you defined in \cref{ex.matrix_of_sets_mult_laws} are
  \emph{coherent}, much in the way the associativity and unity isomorphisms of a
  monoidal category are. Together, this means that there is a \emph{bicategory}
  of sets and matrices of sets between them. 
\end{remark}

\begin{definition}\label{def.matrix_of_sets_tensor}
  Let $M$ be a $B \times A$ matrix and $N$ a $C \times D$ matrix of sets. Their
  \emph{Kronecker product} or \emph{tensor product} $M \otimes N$ is a $(B
  \times C) \times (A \times D)$ matrix of sets with entries:
  $$(M \otimes N)_{(b, c)(a, d)} = M_{ba} \times N_{cd}.$$

  We draw the tensor product $M \otimes M$ of matrices as:
  \[
\begin{tikzpicture}[oriented WD, bb small, bb port length=5pt, baseline=(f)]
	\node[bb={1}{1}, rounded corners=5pt, draw=dgreen] (f) {$M$};
	\node[bb={1}{1}, rounded corners=5pt, draw=dgreen, below= of f] (g) {$N$};
	\node[left=0 of f_in1] {$A$};
	\node[right=0 of f_out1] {$B$};
	\node[right=0 of g_out1] {$D$};
	\node[left=0 of g_in1] {$C$};
\end{tikzpicture}
  \]
\end{definition}

Finally, we need to define the partial trace of a matrix of sets.
\begin{definition}
  Suppose that $M$ is a $(A \times C) \times (A \times B)$ matrix of sets. Its
  \emph{partial trace} $\fun{tr}_{A} M$ is a $C \times B$ matrix of sets with
  entries:
  $$(\fun{tr}_{A})M_{cb} = \sum_{a \in A} M_{(a,c)(a,b)}.$$

    We draw the partial trace of a matrix of sets as:
\[
\begin{tikzpicture}[oriented WD, bb small, bb port length=5pt, baseline=(M)]
	\node[bb={2}{2}, rounded corners=5pt, draw=dgreen] (M) {$M$};
	\node[left=1.5 of M_in2] (B) {$B$};
	\node[right=1.5 of M_out2] (C) {$C$};
  \draw let \p1=(M.north east), \p2=(M.north west), \n1={\y2+\bby}, \n2=\bbportlen in
          (M_out1) to[in=0] (\x1+\n2,\n1) -- node[above, font=\scriptsize] {$A$} (\x2-\n2,\n1) to[out=180] (M_in1);
  \draw (B) to (M_in2);
  \draw (M_out2) to (C);
\end{tikzpicture}
\]
\end{definition}

\begin{exercise}\label{ex.matrix_of_sets_sanity_check}
Here's an important sanity check we should do about our string diagrams for
matrices of sets. The following two diagrams should describe the same matrix,
even though they describe it in different ways:
    \[
\begin{tikzpicture}[oriented WD, bb small, bb port length=5pt, baseline=(f)]
	\node[bb={1}{1}, rounded corners=5pt, draw=dgreen] (f) {$M$};
	\node[bb={1}{1}, rounded corners=5pt, draw=dgreen, right=1.5 of f] (g) {$N$};
	\node[left=0 of f_in1] {$A$};
	\node[right=0 of g_out1] {$C$};
	\draw (f_out1) to node[above, font=\scriptsize] {$B$} (g_in1);
\end{tikzpicture}
\quad\quad\quad\quad
\begin{tikzpicture}[oriented WD, bb small, bb port length=5pt, baseline=(Q)]
  \node[bb={1}{1}, rounded corners=5pt, draw=dgreen] (M) {$M$};
  \node[bb={1}{1}, rounded corners=5pt, draw=dgreen, below= of M] (N) {$N$};
  \node[bb={0}{0}, rounded corners=5pt, draw=dgreen, dashed, fit={(N) (M)}] (Q) {};

  \node[left = 3 of Q.center] (Left) {$A$};
  \node[right = 3 of Q.center] (Right) {$C$};

  \draw let \p1=(Q.north east), \p2=(Q.south west), \n1={\y2-\bby}, \n2=\bbportlen in    (M_out1-|Q.east) to[in=0] (\x1 +\n2, \n1) -- node[below, font=\scriptsize] {$B$} (\x2-\n2, \n1) to[out=180] (N_in1-|Q.west);
  \draw (Left) to[out=0, in=180] (M_in1-|Q.west);
  \draw (N_out1-|Q.east) to[out=0, in=180] (Right);
\end{tikzpicture}
    \]
The diagram on the left says ``multiply $M$ and $N$'', while the diagram on the
right says ``tensor $M$ and $N$, and then partially trace them.''. Show that
these two diagrams do describe the same matrix:
$$NM \cong \fun{tr}_{B}(M \otimes N).$$
Compare this to \cref{ex.ClockWithDisplay}, where we say that wiring an input of
a system to an output of another can be seen as first taking their parallel
product, and then forming a loop.
\end{exercise}

\paragraph{Steady states and matrix arithmetic}

For the remainder of this section, we will show that we can calculate the steady
state matrix of a wired together system in terms of its component system in a
very simple way:
\begin{itemize}
  \item First, take the steady state matrices of the component systems.
  \item Then consider the wiring diagram as a string diagram for multiplying,
tensoring, and tracing matrices.
  \item Finally, finish by doing all those operations to the matrix.
\end{itemize}

In \cref{sec.representables}, we will see that this method --- or something a
lot like it --- works calculating the behaviors of a composite system out of the
behaviors of its components, as long as the representative of that behavior
exposes its entire state. That result will be nicely packaged in a beautiful
categorical way: we'll make an \emph{indexed double functor}.

But for now, let's just show that tensoring and partially tracing steady state
matrices correponds to taking the parallel product and wiring an input to an
output, respectively, of systems.

\begin{proposition}\label{prop.steady_state_matrix_parallel_tensor}
  Let $\Sys{S_1}$ and $\Sys{S_2}$ be systems. Then the steady state matrix of the
  parallel product $\Sys{S_1 \otimes S_2}$ is the tensor of their steady state
  matrices:
  $$\Set{Steady}_{\Sys{S_1 \otimes S_2}} \cong \Set{Steady}_{\Sys{S_1}} \otimes \Set{Steady}_{\Sys{S_2}}.$$
\end{proposition}
\begin{proof}
  First, we note that these are both $(\Out{S_1} \times \Out{S_2}) \times
  (\In{S_1} \times \In{S_2})$-matrices of sets. Now, on a chart $\lens{(i_1,
    i_2)}{(o_1, o_2)}$, a steady state in $\Sys{S_1} \otimes \Sys{S_2}$ will be
  a pair $(s_1, s_2) \in \State{S_1} \times \State{S_2}$ such that
  $\update{S_j}(s_j, i_j) = s_j$ and $\expose{S_j}(s_j) = o_j$ for $j = 1,\, 2$.
  In other words, its just a pair of steady states, one in $\Sys{S_1}$ and one
  in $\Sys{S_2}$. This is precisely the $\lens{(i_1, i_2)}{(o_1, o_2) }$-entry
  of the right hand side above. 
\end{proof}


\begin{remark}
  \cref{prop.steady_state_matrix_parallel_tensor} is our motiviation for using
  the symbol ``$\otimes$'' for the parallel product of systems.
\end{remark}

\begin{proposition}\label{prop.steady_state_matrix_trace}
Let $\Sys{S}$ be a system with $\In{S} = A \times B$ and $\Out{S} = A \times C$.
Let $\Sys{S'}$ be the system formed by wiring the $A$ output into the $A$ input
of $\Sys{S}$:
\[\Sys{S'} \coloneqq
\begin{tikzpicture}[oriented WD, every fit/.style={inner xsep=\bbx, inner ysep=\bby}, bbx = .3cm, bby =.3cm, bb min width=.5cm, bb port length=2pt, bb port sep=1, baseline=(S'.center)]
	\node[bb={2}{2}, fill=blue!10] (S) {$\Sys{S}$};

  \node[bb={0}{0}, fit={($(S.north east) + (1,1)$) ($(S.south west) - (1,0)$)}] (S') {};
  
  \draw (S_out2) to (S_out2-|S'.east);
  \draw (S_in2-|S'.west) to (S_in2);

  \draw let \p1=(S.north east), \p2=(S.north west), \n1={\y2+\bby}, \n2=\bbportlen in    (S_out1) to[in=0] (\x1 +\n2, \n1) -- (\x2-\n2, \n1) to[out=180] (S_in1);
\end{tikzpicture}
\]
Then the steady state matrix of $\Sys{S'}$ is given by partially tracing out $A$
in the steady state matrix of $\Sys{S}$:
\[
  \Set{Steady}_{\Sys{S'}} = 
\begin{tikzpicture}[oriented WD, bb small, bb port length=5pt, baseline=(M)]
	\node[bb={2}{2}, rounded corners=5pt, draw=dgreen] (M) {$\Set{Steady}_{\Sys{S}}$};
	\node[left=1.5 of M_in2] (B) {};
	\node[right=1.5 of M_out2] (C) {};
  \draw let \p1=(M.north east), \p2=(M.north west), \n1={\y2+\bby}, \n2=\bbportlen in
          (M_out1) to[in=0] (\x1+\n2,\n1) -- node[above, font=\scriptsize] {$A$} (\x2-\n2,\n1) to[out=180] (M_in1);
  \draw (B) to (M_in2);
  \draw (M_out2) to (C);
\end{tikzpicture} = \fun{tr}_{A}\left(\Set{Steady}_{\Sys{S}}\right)
\]
\end{proposition}
\begin{proof}
  Let's first see what a steady state of $\Sys{S'}$ would be. Since $\Sys{S'}$
  is just a rewiring of $\Sys{S}$, it has the same states; so, a steady state $s$ of
  $\Sys{S'}$ is in particular a state of $\Sys{S}$. Now,
  $$\update{S'}(s,b) = \update{S}(s, (\pi_1\expose{S}(s), b))$$
  by definition, so if $\update{S'}(s, b) = s$, then $\update{S}(s, (\pi_1\expose{S}(s),
  b)) = s$. If also $\expose{S'}(s) = c$ (so that $s$ is a
  $\lens{b}{c}$-steady state of $\Sys{S'}$), then $\pi_2\expose{S}(s) =
  \expose{S'}(s) = c$ as well. In total then, starting with a
  $\lens{b}{c}$-steady state $s$ of $\Sys{S'}$, we get a
  $\lens{(\pi_1\expose{S}(s), b)}{(\pi_1\expose{S}(s), c)}$-steady state of
  $\Sys{S}$.  That is, we have a function
  $$s \mapsto (\pi_1 \expose{S}(s), s) : \Set{Steady}_{\Sys{S'}}\lens{b}{c} \to
  (\fun{tr}_{A} \Set{Steady}_{\Sys{S}}) \lens{b}{c}.$$

  It remains to show that this function is a bijection. So, suppose we have a
  pair $(a, s) \in \fun{tr}_A \Set{Steady}_{\Sys{S}}\lens{b}{c}$ of an $a \in A$ and a
  $\lens{(a, b)}{(a, c)}$ steady state of $\Sys{S}$. Then
  \begin{align*}
    \update{S'}(s, b) &= \update{S}(s, (\pi_1\expose{S}(s), b)) \\
                      &= \update{S}(s, (a, b)) &\mbox{since $\expose{S}(s) = (a, c)$.}\\
                      &= s &\mbox{since $s$ is a $\lens{(a, b)}{(a, c)}$-steady state.}\\
    \expose{S'}(s) &= \pi_2\expose{S}(s) = c.
  \end{align*}
  This shows that $s$ is also a $\lens{b}{c}$ steady state of $\Sys{S'}$, giving
  us a function
  $(a, s) \mapsto s : (\fun{tr}_A \Set{Steady}_{\Sys{S}}) \to \Set{Steady}_{\Sys{S'}}.$
  These two functions are plainly inverse.
\end{proof}

We can summarize \cref{prop.steady_state_matrix_trace} in the following
commutative diagram:
\begin{equation}\label{eqn.steady_state_matrix_compose}
\begin{tikzpicture}
\node[draw] (Topleft) {
\begin{tikzpicture}[oriented WD, every fit/.style={inner xsep=\bbx, inner ysep=\bby}, bbx = .3cm, bby =.3cm, bb min width=.5cm, bb port length=2pt, bb port sep=1, baseline=(S)]
	\node[bb={2}{2}, fill=blue!10] (S) {$\Sys{S}$};
\end{tikzpicture}

};


\node[draw, below = 4 of Topleft] (Botleft) {
\begin{tikzpicture}[oriented WD, every fit/.style={inner xsep=\bbx, inner ysep=\bby}, bbx = .3cm, bby =.3cm, bb min width=.5cm, bb port length=2pt, bb port sep=1, baseline=(S'.center)]
	\node[bb={2}{2}, fill=blue!10] (S) {$\Sys{S}$};

  \node[bb={0}{0}, fit={($(S.north east) + (1,1)$) ($(S.south west) - (1,0)$)}] (S') {};
  
  \draw (S_out2) to (S_out2-|S'.east);
  \draw (S_in2-|S'.west) to (S_in2);

  \draw let \p1=(S.north east), \p2=(S.north west), \n1={\y2+\bby}, \n2=\bbportlen in    (S_out1) to[in=0] (\x1 +\n2, \n1) -- (\x2-\n2, \n1) to[out=180] (S_in1);
\end{tikzpicture}
};



\node[draw, right = 4 of Botleft]  (Botright) {
\begin{tikzpicture}[oriented WD, bb small, bb port length=5pt, baseline=(M)]
	\node[bb={2}{2}, rounded corners=5pt, draw=dgreen] (M) {$\Set{Steady}_{\Sys{S}}$};
	\node[left=1.5 of M_in2] (B) {};
	\node[right=1.5 of M_out2] (C) {};
  \draw let \p1=(M.north east), \p2=(M.north west), \n1={\y2+\bby}, \n2=\bbportlen in
          (M_out1) to[in=0] (\x1+\n2,\n1) -- (\x2-\n2,\n1) to[out=180] (M_in1);
  \draw (B) to (M_in2);
  \draw (M_out2) to (C);
\end{tikzpicture}
};

\node[draw] at (Botright|-Topleft)(Topright) {
\begin{tikzpicture}[oriented WD, bb small, bb port length=5pt, baseline=(f)]
	\node[bb={2}{2}, rounded corners=5pt, draw=dgreen] (f) {$\Set{Steady}_{\Sys{S}}$};
\end{tikzpicture}
};

\draw[->, shorten <= 5pt, shorten >= 5pt] (Topleft) to node[above] {$\Set{Steady}$} (Topright);
\draw[->, shorten <= 5pt, shorten >= 5pt] (Botleft) to node[below] {$\Set{Steady}$} (Botright);
\draw[->, shorten <= 5pt, shorten >= 5pt] (Topleft) to coordinate[left] (Leftlabel) (Botleft);
\draw[->, shorten <= 5pt, shorten >= 5pt] (Topright) to coordinate[right] (Rightlabel) (Botright);

\node[left = 0 of Leftlabel] {
\begin{tikzpicture}[oriented WD, every fit/.style={inner xsep=\bbx, inner ysep=\bby}, bbx = .3cm, bby =.3cm, bb min width=.5cm, bb port length=2pt, bb port sep=1, baseline=(S'.center)]
	\node[bb={2}{2}, fill=blue!10, dashed] (S) {};

  \node[bb={0}{0}, fit={($(S.north east) + (1,1)$) ($(S.south west) - (1,0)$)}] (S') {};
  
  \draw (S_out2) to (S_out2-|S'.east);
  \draw (S_in2-|S'.west) to (S_in2);

  \draw let \p1=(S.north east), \p2=(S.north west), \n1={\y2+\bby}, \n2=\bbportlen in    (S_out1) to[in=0] (\x1 +\n2, \n1) -- (\x2-\n2, \n1) to[out=180] (S_in1);
\end{tikzpicture}
};
\node[right= 0 of Rightlabel] {
\begin{tikzpicture}[oriented WD, bb small, bb port length=5pt, baseline=(M)]
	\node[bb={2}{2}, rounded corners=5pt, draw=dgreen, dashed] (M) {$\phantom{\Set{Steady}_{\Sys{S}}}$};
	\node[left=1.5 of M_in2] (B) {$B$};
	\node[right=1.5 of M_out2] (C) {$C$};
  \draw let \p1=(M.north east), \p2=(M.north west), \n1={\y2+\bby}, \n2=\bbportlen in
          (M_out1) to[in=0] (\x1+\n2,\n1) -- node[above, font=\scriptsize] {$A$} (\x2-\n2,\n1) to[out=180] (M_in1);
  \draw (B) to (M_in2);
  \draw (M_out2) to (C);
\end{tikzpicture}
};
\end{tikzpicture}
\end{equation}

The horizontal maps take the steady states of a system, while the vertical map
on the left wires together the system with that wiring diagram, and the vertical
map on the right applies that transformation of the matrix. In the next section,
we will see how this square can be interepreted as a naturality condition in a
\emph{indexed double functor}.

One thing to notice here is that taking the partial trace (the right vertical
arrow in the diagram) is itself given by multiplying by a certain matrix.
\begin{proposition}\label{prop.trace_multiplying_by_matrix}
  Let $M$ be a $(A \times C) \times (A \times B)$ matrix of sets. Let
  $\Set{Tr}^A$ be the $\big(C \times B \big) \times \big((A \times C) \times (A
  \times B)\big)$ matrix of sets with entries:
  \[
    \Set{Tr}A{A}_{(c, b)((a,c'),(a', b'))} \coloneqq \begin{cases} 
      \ord{1} &\mbox{if $a = a'$, $b = b'$, and $c = c'$.} \\
      \emptyset &\mbox{otherwise.}
    \end{cases} 
\]
  Then, considering $M$ as a $\big((A \times C) \times (A \times B)\big) \times
  \ord{1}$ matrix of sets, taking its trace is given by multiplying by $\Set{Tr}^A$:
$$\fun{tr}_{A}M \cong \Set{Tr}^A M$$
\end{proposition}
\begin{proof}
  Let's calculate that matrix product on the right.
  \begin{align*}
    (\Set{Tr}^A M)_{(c, b)} &= \sum_{((a, c'), (a', b')) \in (A \times C) \times (A \times B)} \Set{Tr}^A_{(c,b)((a, c'),(a', b'))} \times M_{(a,c')(a',b')} \\
  \end{align*}
  Now, since $\Set{Tr}^A_{(c,b)((a,c'),(a',b'))}$ is a one element set (if $a =
  a'$, $c = c'$, and $b = b'$) and is empty otherwise, the inner expression has
  the elements of $M_{(a,c')(a', b')}$ if and only if $a = a'$, $b = b'$, and $c
  = c'$ and is otherwise empty. So, we conclude that
  \[
\sum_{((a, c'), (a', b')) \in (A \times C) \times (A \times B)}
\Set{Tr}^A_{(c,b)((a, c'),(a', b'))} \times M_{(a,c')(a',b')} \cong M_{(a, c)(a,
  b)}.
\]
As desired.
\end{proof}

%-------- Section --------%
\section{Behaviors of the whole from behaviors of the parts}\label{sec.representables}

Let's take stock of where we've been so far this chapter.
\begin{itemize}
  \item In \cref{sec.deterministic_system}, we saw the definition of a
    \emph{deterministic system}.
  \item In \cref{sec.wiring_sytems_discrete}, we learned about \emph{lenses}. We saw how systems can be
    interpreted as special sorts of lenses, and how we can wire together systems
    using lens composition.
  \item In \cref{sec.behavior_discrete}, we learned about behaviors and \emph{charts}. We saw
    how to define behaviors of systems using the notion of chart. Finally, we
    saw how the steady states of wired together systems can be calculated from their
    component systems with matrix arithmetic.
\end{itemize}

The two sorts of composition we have seen so far --- lens composition and chart
composition --- mirror the two sorts of composition at play in systems theory:
\begin{itemize}
  \item We can compose \emph{systems} by wiring them together. This uses lens composition.
  \item We can compose \emph{behaviors} of systems like we compose functions.
    This uses chart composition.
\end{itemize}

In this section, we will see how these two sorts of composition interact.
Because there are two sorts of composition involved, we will use the notion of a
\emph{double category}. A double category is like a category, but there are two
sorts of map between the objects, and there is a notion of interaction between
these two sorts of map. We'll get to see that behaviors $\phi : \Sys{T} \to
\Sys{S}$ of systems
(\cref{def.behavior_discrete}) are squares
\[
  \begin{tikzcd}
    \lens{\State{T}}{\State{T}} \ar[r, shift left, "\lens{\phi}{\phi}"] \ar[r,
    shift right]
    \ar[d, shift right, "\lens{\update{T}}{\expose{T}}"'] \ar[d, shift left, leftarrow] &
    \lens{\State{S}}{\State{S}} \ar[d, shift left, leftarrow,
    "\lens{\update{S}}{\expose{S}}"] \ar[d, shift right]\\
    \lens{\In{T}}{\Out{T}} \ar[r, shift right, "\lens{f^{\sharp}}{f}"'] \ar[r,
    shift left] & \lens{\In{S}}{\Out{S}}
  \end{tikzcd}
\]
in the double category of \emph{arenas}, \cref{prop.behavior_as_square_of_arenas_discrete}.

Along the way, we'll see a new interpretation of the categories of lenses and
charts which explain why they seem so eerily familiar. We will see them as
instances of a very general construction that forms a category out of an
\emph{indexed category}: the \emph{Grothendieck construction}. We take this
detour into the abstract because when we move to different doctrines in later
chapters, the particular notion of lens and chart might change, but they will
always be built out of an indexed category in the way we will see in this section.



%---- Subsection ----%
\subsection{Indexed categories and the Grothendieck construction}\label{sec.indexed_categories}

An \emph{indexed category} is a category which varies functorially over the
objects of another category.
%%
%% :CUSTOM-ID:cite-this.pseudo_functors
%%

\begin{definition}
  An \emph{indexed category} $\cat{A} : \cat{C}\op \to \Cat{Cat}$ is a
  contravariant (pseudo-) functor\footnote{A pseudo-functor is like a functor,
    but it only satisfies the functoriality conditions up to isomorphism, and
    these isomorphisms must satisfy some laws that make them ``cohere''. The
    indexed categories we will see in \cref{chapter.1,chapter.2,chapter.3}
    will all be actual functors, however.}. We call the category $\cat{C}$ the
  \emph{base} of the indexed category $\cat{A}$. Explicitly, an indexed
  category $\cat{A}$ has:
  \begin{itemize}
  \item A base category $\cat{C}$.
  \item For every object $C \in \cat{C}$ of the base, a category $\cat{A}(C)$.
  \item For every map $f : C \to C'$ in the base, a \emph{pullback} functor
    $f^{\ast} : \cat{A}(C') \to \cat{A}(C)$, which we think of as ``reindexing''
    the objects of $\cat{A}(C')$ so that they live over $\cat{A}(C)$.
  \item (Psuedo-functoriality) For any two functions $f : C \to C'$ and $g : C' \to C''$, we need a
    natural isomorphism $f^{\ast} \circ g^{\ast} \xequals{\gamma_{g, f}} (g \circ
    f)^{\ast}$. And for any object $C$, we need a natural isomorphism $:
    \id_{\cat{A}(C)} \xequals{\iota_C } (\id_C)^{\ast}$. These are required to satisfy a
    few \emph{coherence conditions}:
\begin{align}
  \gamma_{h,gf} \circ (\gamma_{g, f}h^{\ast}) &= \gamma_{hg, f} \circ (f^{\ast}\gamma_{h, g}) \\
  \gamma_{\id_C, f} \circ (f^{\ast}\iota_C) &= \gamma_{f, \id_{C'}} \circ (\iota_{C'} f^{\ast}).
\end{align}
   Often, $\gamma$ and $\iota$ will both be identities, and these equalities
   will follow trivially.
  \end{itemize}
\end{definition}

\begin{remark}\label{rmk.coherence_condition_indexed_cat}
  The coherence conditions are almost always trivial --- if not strictly
  identities, then they are probably just a shuffling of parentheses. For this
  reason, and because keeping track of them is a headache, we will omit
  discussion of the coherences. We will write a coherence with a big equals,
  like this $f^{\ast} \circ g^{\ast} \xequals{\phantom{ \gamma_{g, f} }} (g \circ
    f)^{\ast}$ and leave the name implicit. The coherence conditions ensure that
    working with these isomorphisms as though they really were equalities works well.
\end{remark}

Indexed categories are quite common throughout mathematics. We will construct a
particular example for our own purposes in \cref{sec.context_indexed_cat}, and
more throughout the book.

\begin{example}\label{ex.indexed_cat_of_dependent_sets}
  Recall that a \emph{dependent set} is a function $X : A \to \smset$ from a set
  into the category of sets. We have an indexed category of dependent sets
  $$\smset^{(-)} : \smset\op \to \Cat{Cat}$$
  which is defined as follows:
  \begin{itemize}
    \item To each set $A$, we assign the category $\smset^A$ of sets indexed by
      $A$. The objects of $\smset^A$ are the sets $X : A \to \smset$ indexed by
      $A$, and a map $f : X \to Y$ is a family of maps $f_a : X_a \to Y_a$
      indexed by the elements $a \in A$. Composition is given componentwise: $(g
      \circ f)_a = g_a \circ f_a$.
    \item To every function $f : A' \to A$, we get a reindexing functor
$$f^{\ast} : \smset^A \to \smset^{A'}$$
   Given by precomposition: $X \mapsto X \circ f$. The indexed set $X \circ f :
   A' \to \smset$ is the set $X_{f(a')}$ on the index $a' \in A'$. The families
   of functions get reindexed the same way.
     \item Since our reindexing is just given by precomposition, it is clearly
       functorial on the nose (that is, not just up to isomorphism).
  \end{itemize}
  We will return to this example in much greater detail in \cref{chapter.4}.
\end{example} 


If we have an family of sets $A : I \to \Cat{Set}$ indexed by a set $I$, we can
form the disjoint union $\sum_{i \in I} A_i$, together with the projection $\pi
: \sum_{i \in I} A_i \to I$ sending each $a \in A_i$ to $i$. The Grothendieck
construction is a generalization of this construction to indexed categories. Namely, we will take an indexed category $\cat{A}
: \cat{C} \to \Cat{Cat}$ and form a new category

$$\int^{C : \cat{C}} \cat{A}(C)$$
which we think of as a ``union'' off all the categories $\cat{A}(C)$. But this
``union'' will not be
disjoint since there will be morphisms from objects in $\cat{A}(C)$ to objects
in $\cat{A}(C')$. This is why we use the integral notation; we want to suggest
that the Grothendieck construction is a sort of sum.\footnote{The Grothendieck
  construction is an example of a \emph{lax colimit} in 2-category theory,
  another sense in which it is a `sort of sum'.}
\begin{definition}\label{def.grothendieck_construction}
  Let $\cat{A} : \cat{C}\op \to \Cat{Cat}$ be an indexed category. The
  \emph{Grothendieck construction} of $\cat{A}$
  $$\int^{C : \cat{C}} \cat{A}(C)$$
  is the category with:
  \begin{itemize}
    \item Objects pairs $\lens{A}{C}$ of objects $C \in \cat{C}$ and $A \in
      \cat{A}(C)$. We say that $A$ ``sits over'' $C$.
    \item Maps $\lens{f_{\flat}}{f} : \lens{A}{C} \rightrightarrows
      \lens{A'}{C'}$ pairs of $f : C \to C'$ in $\cat{C}$ and $f_{\flat} :
      A \to f^{\ast}A'$ in $\cat{A}(C)$.
    \item Given $\lens{f_{\flat}}{f} : \lens{A}{C} \rightrightarrows
      \lens{A'}{C'}$ and $\lens{g_{\flat}}{g} : \lens{A'}{C'} \rightrightarrows
      \lens{A''}{C''}$, their composite is given by
      $$\lens{g_{\flat}}{g} \circ \lens{f_{\flat}}{f} \coloneqq \lens{f^{\ast}g_{\flat}
      \circ f_{\flat}}{g \circ f}$$
    Written with the signatures, this looks like
    $$\lens{A \xto{f_{\flat}} f^{\ast}A' \xto{f^{\ast}g_{\flat}}
      f^{\ast}g^{\ast}A'' \xequals{\phantom{\gamma}} (g \circ f)^\ast A''}{C \xto{f} C' \xto{g} C''}$$
    \item The identity is given by $\lens{\id_A}{\id_C} : \lens{A}{C}
      \rightrightarrows \lens{A}{C}$
  \end{itemize}
\end{definition}

\begin{exercise}
  Check that \cref{def.grothendieck_construction} does indeed make $\int^{C :
    \cat{C}} \cat{A}(C)$ into a category. That is, check that composition as
  defined above is associative and unital.
\end{exercise}

The confluence of notation with that of charts is no accident. Indeed, we will
see in the next subsection that the category of charts can be described as the
Grothendieck construction of a certain indexed category.

%---- Subsection ----%
\subsection{Sets with context, charts, and lenses}\label{sec.context_indexed_cat}

In this section, we will see how both the category $\Cat{Chart}$ and
$\Cat{Lens}$ of charts and lenses in the category of sets can be described using
the Grothendieck construction. To do this, we need some other categories named
after their maps (rather than their objects): 
\emph{category of sets and functions with context $C$} for some a given set $C$. 

\begin{definition}
  Let $C$ be a set. The \emph{category $\smctx{C}$ of sets and functions with context $C$}
  is the category defined by:
  \begin{itemize}
    \item Objects are sets.
    \item Maps $f : X \ctxto Y$ are functions $f : C \times X \to Y$.
    \item The composite $g \circ f$ of $f : X \ctxto Y$ and $g : Y \ctxto Z$ is
      the function
       $$(c, x) \mapsto g(c, f(c, x)) : C \times X \to Z.$$
       Diagrammatically, this is the composite:
       $$C \times X \xto{\Delta_C \times X} C \times C \times X \xto{C \times f}
       C \times Y \xto{g} Z.$$
    \item The identity $\id : X \ctxto X$ is the second projection $\pi_2 : C
      \times X \to X$.
  \end{itemize}
\end{definition}

\begin{exercise}
  Check that $\smctx{C}$, as defined, really is a category. That is,
  \begin{enumerate}
    \item For $f : X \ctxto Y$, $g : Y \ctxto Z$, and $h : Z \ctxto W$, check
      that $h \circ (g \circ f) = (h \circ g) \circ f$.
    \item For $f : X \ctxto Y$, check that $f \circ \id_X = f = \id_Y \circ f$.
  \end{enumerate}
\end{exercise}

Together, we can arrange the categories of sets and functions with context into
an indexed category.
\begin{definition}
  The \emph{indexed category of sets and functions with context}
  $$\smctx{-} : \smset\op \to \Cat{Cat}$$
  is defined by:
  \begin{itemize}
    \item For a set $C$, we have the category $\smctx{C}$.
    \item For a function $r : C' \to C$, we get a functor
      $$r^{\ast} : \smctx{C} \to \smctx{C'}$$
      given by sending each object to itself, but each morphism $f : C \times X
      \to Y$ in $\smctx{C}$ to the morphism $r^{\ast}f \coloneqq f \circ (r \times X)$:
      $$C' \times X \xto{r \times X} C \times X \xto{f} Y.$$
  \end{itemize}
  We note that this is evidently functorial.
\end{definition}

Now we can see why the category of charts and the category of lenses are so
eerily similar: one will be the Grothendieck construction of $\smctx{-}$, and
the other will be the Grothendieck construction of its dual $\smctx{-}\op$.
\begin{proposition}\label{prop.charts_as_groth_construction}
  The category $\Cat{Chart}$ of charts in the category of sets
  (\cref{def.category_of_charts}) is the Grothendieck construction of $\smctx{-}
  : \smset\op \to \Cat{Cat}$:
  $$\Cat{Chart} = \int^{C \in \smset} \smctx{C}.$$
\end{proposition}
\begin{proof}
  We will expand the definition of the Grothendieck construction, and see that
  it gives us precisely \cref{def.category_of_charts}.

  First, the objects of $\int^{C \in \smset} \smctx{C}$ are pairs of sets
  $\lens{A^-}{A^+}$, since the objects of $\smctx{A^+}$ are just sets
  themselves. So far so good.

  Next, a map $\lens{f_{\flat}}{f} : \lens{A^-}{A^+} \tto \lens{B^-}{B^+}$
  in $\int^{C \in \smset} \smctx{C}$ is a pair of maps $f : A^+ \to B^+$ and
  $f_{\flat} : A^- \ctxto f^{\ast} B^-$ in $\smctx{A^+}$. But $f^{\ast}B^- =
  B^-$ by the definition of the reindexing functor $f^{\ast}$, so by the
  definition of map with context $A^+$, we see that $f_{\flat} : A^+ \times A^-
  \to B^-$. In other words, the maps in $\int^{C \in \smset} \smctx{C}$ are
  precisely the charts. We note that the identity map in the Grothendieck
  construction is the identity chart.

  Finally, we should check that composition of charts is given by composition in
  the Grothendieck construction. Suppose that $\lens{f_{\flat}}{f} :
  \lens{A^-}{A^+} \tto \lens{B^-}{B^+}$ and $\lens{g_{\flat}}{g} :
  \lens{B^-}{B^+} \tto \lens{C^-}{C^+}$ are charts. Then their composite
  in $\int^{C \in \smset} \smctx{C}$ is given by
  $$\lens{f^{\ast}g_{\flat} \circ f_{\flat}}{g \circ f}.$$
  Well, the bottom is all good, what about the top? Expanding the definition, we
  see that $f^{\ast}g_{\flat} = g_{\flat} \circ (f \times B^-)$, so that
  $f^{\ast}g_{\flat} \circ f_{\flat}$ is the map given by
  $$(a^+, a^-) \mapsto g_{\flat}(f(a^+), f_{\flat}(a^+, a^-)),$$
  which is precisely how we defined composition of charts!
\end{proof}

  
The only difference between the category of charts and the category of lenses is
that for lenses, the maps on top go backwards.
\begin{proposition}\label{prop.lenses_as_groth_construction}
  The category $\Cat{Lens}$ of lenses in the category of sets is the
  Grothendieck construction of the indexed category of \emph{opposites} of the
  categories of sets and functions with context:
  $$\Cat{Lens} = \int^{C \in \smset} \smctx{C}\op.$$
\end{proposition}
\begin{proof}
As with \cref{prop.charts_as_groth_construction}, we will simply expand the
definition of the right hand side, and see that it is precisely the category of lenses.

The objects of $\int^{C \in \smset} \smctx{C}\op$ are pairs $\lens{A^-}{A^+}$ of
sets. All good so far.

A map in $\int^{C \in \smset} \smctx{C}\op$ is a pair $\lens{f^{\sharp}}{f}$
with $f : A^+ \to B^+$ and $f^{\sharp} : A^- \ctxto f^{\ast} B^-$ in
$\smctx{A^+}\op$. Now, $f^{\ast}B^- = B^-$ so $f^{\sharp}$ has signature $A^-
\ctxto B^-$ in $\smctx{A^+}\op$, which means $f^{\sharp}$ has signature $B^-
\ctxto A^{-}$ in $\smctx{A^+}$, which means that $f^{\sharp}$ is a really a
function $A^+ \times B^- \to A^{-}$. In other words, a map in $\int^{C \in \smset}
\smctx{C}\op$ is precisely a lens. We note that the identity map is the identity lens.

Finally, we need to check that composition in $\int^{C \in \smset} \smctx{C}\op$
is lens composition. Suppose that $\lens{f^{\sharp}}{f} :
  \lens{A^-}{A^+} \fromto \lens{B^-}{B^+}$ and $\lens{g^{\sharp}}{g} :
  \lens{B^-}{B^+} \from \lens{C^-}{C^+}$ are lenses. In $\int^{C \in \smset}
  \smctx{C}\op$, their composite is
$$\lens{f^{\ast}g^{\sharp} \circ f^{\sharp}}{g \circ f}.$$
  The bottom is all good, we just need to check that the top --- which,
  remember, lives in $\smctx{A^+}\op$ --- is correct. Since the composite up top
  is in
  the opposite, we are really calculating $f^{\sharp} \circ f^{\ast} g^{\sharp}$
  in $\smctx{A^+}$. By definition, this is
$$(a^+, c^-) \mapsto f^{\sharp}(a^+, g^{\sharp}(f(a^+), c^-))$$
which is precisely their composite as lenses!
\end{proof}

\begin{exercise}\label{ex.really_understand_charts_as_groth_construction}
  Make sure you \emph{really} understand \cref{prop.charts_as_groth_construction,prop.lenses_as_groth_construction}.
\end{exercise}

\paragraph{Sections of indexed categories.}

Any function $f : A \to B$ gives rise to a chart $\lens{f \circ \pi_2}{f} :
\lens{A}{A} \tto \lens{B}{B}$ by simply ignoring the context $A$. This gives us
a functor $\smset \to \Cat{Chart}$ which we call a \emph{section} of the
indexed category $\smctx{-}$. 
\begin{definition}\label{def.section_of_indexed_category}
Let $\cat{A} : \cat{C}\op \to \Cat{Cat}$ be an indexed category. A \emph{section} $T$ of
$\cat{A}$ is a functor $\lens{T-}{-} : \cat{C} \to \int^{C \in
  \cat{C}}\cat{A}(C)$ so that the bottom component of the image $\lens{Tf}{f}$
of any map $f$ in $\cat{C}$ is $f$ itself.
\end{definition}

\begin{proposition}\label{prop.section_charts_discrete}
The functor $\lens{-\circ \pi_2}{-} : \smset \to \Cat{Chart}$ is a section of
the indexed category $\smctx{-}$.
\end{proposition}
\begin{proof}
By definition, the bottom component of $\lens{f \circ \pi_2}{f}$ is $f$, so we
are really just checking that this is a functor. We note that it sends
identities to identities.

Let $f :A \to B$ and $g : B \to C$ be functions. We need to show that
$$\lens{g \circ \pi_2}{g} \circ \lens{f \circ \pi_2}{f} = \lens{(g \circ f)
  \circ \pi_2}{g \circ f}.$$ 
The chart composite of $g \circ \pi_2$ with $f \circ \pi_2$ sends $(a, a')$ to
$g(\pi_2(f(a), f(\pi_2(a, a'))))$, which we can quickly see equals $g(f(a'))$ which
is $(g \circ f)(\pi_2(a, a'))$.
\end{proof}

\begin{remark}
  Sections of indexed categories are very useful in the theory of dynamical
  systems because they give us a notion of ``changes possible in a given
  state''. The section of \cref{prop.section_charts_deterministic} is a way of
  telling us that in a deterministic system, a system can transition to any
  state from any state. We'll see more of this point of view when we formally
  define dynamical system doctrines in \cref{chapter.2}.
\end{remark}


\paragraph{Pure and cartesian maps.}\label{sec.pure_and_cartesian_maps}

A map in a Grothendieck construction is a pair $\lens{f_{\flat}}{f} :
\lens{A}{C} \tto \lens{A'}{C'}$ of maps $f : C \to C'$ and $f_{\flat}: A \to
f^{\ast}A'$. It is not too hard to see that a map is an isomorphism in a Grothendieck
construction if and only if both its constituent maps are isomorphisms in their
respective categories.

\begin{proposition}\label{prop.isomorphism_in_groth_construction}
Let $\cat{A} : \cat{C}\op \to \Cat{Cat}$ be an indexed category and let $\lens{f_{\flat}}{f} :
\lens{A}{C} \tto \lens{A'}{C'}$ be a map in its Grothendieck construction. Then
$\lens{f_{\flat}}{f}$ is an isomorphism if and only if $f$ is an isomorphism in
$\cat{C}$ and $f_{\flat}$ is an isomorphism in $\cat{A}(C)$.
\end{proposition}
\begin{proof}
First, let's show that if both $f$ and $f_{\flat}$ are isomorphisms, then
$\lens{f_{\flat}}{f}$ is an isomorphism. We then have $f\inv : C' \to C$ and
$f_{\flat}\inv : f^{\ast}A' \to A$. From $f_{\flat}\inv$, we can form
$(f\inv)^{\ast} (f_{\flat}\inv) : (f\inv)^{\ast}f^{\ast} A' \to
(f\inv)^{\ast}A$, which we can pre-compose with some of the coherences to have
the signature $A' \to (f\inv)^{\ast} A$:
$$A' \xequals{\,} (f \circ f\inv)^{\ast} A' \xequals{\,} 
(f\inv)^{\ast} f^{\ast} A' \xto{(f\inv)^{\ast} (f_{\flat}\inv)} (f\inv)^{\ast} A.$$
Now, consider the map $\lens{(f\inv)^{\ast}f_{\flat}\inv}{f\inv} : \lens{A'}{C'}
\tto \lens{A}{C}$. We'll show that this is an
inverse to $\lens{f_{\flat}}{f}$. Certainly, the bottom components will work
out; we just need to worry about the top. That is, we need to show that
$f^{\ast}((f\inv)^{\ast} f_{\flat}\inv) \circ f_{\flat} = \id$ and
$(f\inv)^{\ast}(f_{\flat}) \circ (f\inv)^{\ast}(f_{\flat}\inv) = \id$. Both of
these follow quickly by functoriality.

On the other hand, suppose that $\lens{f_{\flat}}{f}$ is an isomorphism with
inverse $\lens{g_{\flat}}{g}$. Then $gf = \id$ and $fg = \id$, so $f$ is an
isomorphism. We can focus on $f_{\flat}$. We know that $f^{\ast}g_{\flat} \circ
f_{\flat} = \id$ and $g^{\ast}f_{\flat} \circ g_{\flat} = \id$. Applying
$f^{\ast}$ to the second equation, we find that $f_{\flat} \circ
f^{\ast}g_{\flat} = \id$, so that $f_{\flat}$ is an isomorphism with inverse $f^{\ast}g_{\flat}$.
\end{proof}  

\begin{remark}
  \cref{prop.isomorphism_in_groth_construction} gives a general solution to
  \cref{ex.isomorphism_in_category_of_charts}, since the category of charts is a
  Grothendieck construction.
\end{remark}

This proposition suggests two interesting classes of maps in a Grothendieck
construction: the maps $\lens{f_{\flat}}{f}$ for which $f$ is an isomorphism, and
those for which $f_{\flat}$ is an isomorphism.
\begin{definition}\label{def.pure_and_cartesian}
Let $\cat{A} : \cat{C}\op \to \Cat{Cat}$ be an indexed category and let
$\lens{f_{\flat}}{f}$ be a map in its Grothendieck construction. We say that
$\lens{f_{\flat}}{f}$ is
\begin{itemize}
\item \emph{pure} if $f$ is an isomorphism, and
 \item \emph{cartesian} if $f_{\flat}$ is an isomorphism.
\end{itemize}
\end{definition}

The pure maps correspond essentially to the maps in the categories $\cat{A}(C)$
at a given index $C$, while the cartesian maps correspond essentially to the
maps in $\cat{C}$.

\begin{remark}
  The name ``pure'' is non-standard. The usual name is ``vertical''. But we are
  about to talk about ``vertical'' maps in a technical sense when we come to
  double categories, so we've renamed the concept here to avoid confusion later.
\end{remark}

\begin{example}
  We have often seen systems that expose their entire state, like $\Sys{Time}$ 
  of \cref{ex.trajectory_as_behavior_discrete}. Considered as lenses, these are
  \emph{pure} in the sense that their $\expose{}$ function is an isomorphism.
\end{example}

\begin{exercise}\label{ex.2-of-3_for_pure_cartesian}
  Let $\lens{f_{\flat}}{f}$ and $\lens{g_{\flat}}{g}$ be composable maps in a Grothendieck construction,
  \begin{enumerate}
    \item Suppose that $\lens{g_{\flat}}{g}$ is cartesian.  Show that
      $\lens{f_{\flat}}{f}$ is cartesian if and only if their composite is
      cartesian. Is the same true for pure maps?
    \item Suppose that $\lens{f_{\flat}}{f}$ is pure. Show that
      $\lens{g_{\flat}}{g}$ is pure if and only if their composite is pure. Is
      the same true for cartesian maps?
  \end{enumerate}
\end{exercise}


%---- Subsection ----%
\subsection{Dealing with two kinds of composition: Double categories}

In this section, we will introduce the notion of \emph{double category} to help
us deal with our two kinds of composition: the composition of systems, and the
composition of behaviors.

\begin{definition}
  A \emph{double category} $\cat{D}$ has:
  \begin{itemize}
    \item A class $\const{ob}\cat{D}$ of \emph{objects}.
    \item A \emph{horizontal} category $h \cat{D}$ whose objects are those of
      $\cat{D}$. We call the maps in $h\cat{D}$ the \emph{horizontal} maps of $\cat{D}$.
    \item A \emph{vertical} category $v \cat{D}$ whose objects are those of
      $\cat{D}$. We call the maps in $v\cat{D}$ the \emph{vertical} maps of $\cat{D}$.
    \item For vertical maps $j : A \to B$ and $k : C \to D$ and horizontal maps
      $f : A \to C$ and $g : B \to D$, there is a set of
      \emph{squares}
      \[
        \begin{tikzcd}[sep=small]
          A \ar[dd, "j"'] \ar[rr, "f"] & & B \ar[dd, "k"] \\
           & \alpha & \\
          C \ar[rr, "g"'] & & D
        \end{tikzcd}
      \]
    \item Squares can be composed both horizontally and vertically:
        \[
        \begin{tikzcd}[sep=small]
          A_1 \ar[dd, "j"'] \ar[rr, "f_1"] & & A_2 \ar[dd, "k"]  \ar[rr, "f_2"]&
          & A_3 \ar[dd, "\ell"]\\
           & \alpha &  & \beta &\\
          B_1 \ar[rr, "g_1"'] & & B_2 \ar[rr, "f_2"] & & B_3
        \end{tikzcd} \mapsto
        \begin{tikzcd}[sep=small]
          A_1 \ar[dd, "j"'] \ar[rr, "f_2 f_1"] & & A_3 \ar[dd, "\ell"] \\
           & \alpha \mid \beta & \\
          B_1 \ar[rr, "g_2 g_1"'] & & B_3
        \end{tikzcd}
        \]
        \[
        \begin{tikzcd}[sep=small]
          A_1 \ar[dd, "j_1"'] \ar[rr, "f"] & & A_2 \ar[dd, "k_1"]  \\
           & \alpha &  \\
           B_1 \ar[dd, "j_1"'] \ar[rr, "g"] & & B_2 \ar[dd, "k_2"]\\
           & \beta & \\
           C_1 \ar[rr, "h"']& & C_2
        \end{tikzcd} \mapsto
        \begin{tikzcd}[sep=small]
          A_1 \ar[dd, "j_2 j_1"'] \ar[rr, "f"] & & A_3 \ar[dd, "k_2 k_1"] \\
           & \frac{\alpha}{\beta} & \\
          C_1 \ar[rr, "h"'] & & B_3
        \end{tikzcd}
        \]
      \item For every vertical map $j : A \to B$, there is an identity square
        \[
        \begin{tikzcd}[sep=small]
          A \ar[dd, "j"'] \ar[rr, equals] & & A \ar[dd, "j"] \\
           & j & \\
          B \ar[rr, equals] & & B
        \end{tikzcd}
        \]
        which we will also refer to as $j$, for convenience. Similarly, for
        every horizontal map $f : A \to B$, there is an identity square
        \[
        \begin{tikzcd}[sep=small]
          A \ar[rr, "f"] \ar[dd, equals] & & A \ar[dd, equals] \\
           & f & \\
          B \ar[rr, "f"] & & B
        \end{tikzcd}
        \]
        which we will also refer to as $f$, for convenience.
      \item Vertical and horizontal composition is associative and unital, and
        the \emph{interchange law} holds. That is:
        \begin{itemize}
          \item For horizontally composable squares $\alpha$, $\beta$, and
            $\gamma$, $$(\alpha \mid \beta) \mid \gamma = \alpha \mid (\beta \mid
            \gamma).$$
          \item For vertically composable squares $\alpha$, $\beta$, and
            $\gamma$,%
\footnote{If you're seeing this and feeling worried about fractions, you can put your mind at ease; we promise there will be no fractions. Only squares next to squares.}
\[
\begin{tabular}{c}
\[\left(  \begin{tabular}{c}
\alpha \\ \hline
\beta
\end{tabular}\right)\]\\ \hline
\gamma 
\end{tabular}
= 
\begin{tabular}{c}
  \alpha \\ \hline
\[\left(  \begin{tabular}{c}
\beta \\ \hline
\gamma
\end{tabular}\right)\]
\end{tabular}
\]
\item For a square $\alpha$ with left and right vertical edges $j$ and $k$
  respectively, 
$$j \mid \alpha = \alpha = \alpha | k.$$
\item For a square $\alpha$ with top and bottom horizontal edges $f$ and $g$,
$$\frac{f}{\alpha} = \alpha = \frac{\alpha}{g}.%
\footnote{There aren't any fractions here either.}$$
\item For four appropriately composable squares $\alpha$, $\beta$, $\gamma$, and
  $\delta$, the following interchange law holds:
$$\frac{\alpha \mid \beta}{\gamma \mid \delta} = \left
  \frac{\alpha}{\beta} \middle|  \frac{\gamma}{\delta} \right.$$
        \end{itemize}
  \end{itemize}
\end{definition}

Phew, that was quite the definition! The reason the definition of a double
category is so much more involved than the definition of a category is that
there is more than twice the data: there's the vertical category and the
horizontal category, but also how they interact through the squares. 

We will meet three double categories in our tale of wiring together behaviors,
and more in later chapters \jaz{...}.
Let's meet our main characters now.

\subsubsection{The double category of arenas, charts, and
  lenses}\label{sec.double_cat_arenas}

Finally, we meet the double category of arenas. This is where our dynamical
systems live, and where they behave.

\begin{definition}\label{def.double_category_of_arenas_discrete}
  The \emph{double category of arenas} in the deterministic doctrine is a double
  category which has:
  \begin{itemize}
  \item Its objects are the \emph{arenas}, pairs of sets $\lens{A^-}{A^+}$.
  \item Its horizontal category is the category of charts.
  \item Its vertical category is the category of lenses.
  \item There is a square of the following form
    \begin{equation}\label{eqn.dbl_cat_arena_square}
      \begin{tikzcd}
        \lens{A^-}{A^+} \ar[r, shift left, "\lens{f_{\flat}}{f}"] \ar[r, shift
        right] \ar[d, shift right, "\lens{j^{\sharp}}{j}"'] \ar[d, shift left,
        leftarrow] & \lens{B^-}{B^+} \ar[d, shift left, leftarrow,
        "\lens{k^{\sharp}}{k}"] \ar[d, shift right]\\
        \lens{C^-}{C^+} \ar[r, shift right, "\lens{g^{\sharp}}{g}"'] \ar[r,
        shift left] & \lens{D^-}{D^+}
      \end{tikzcd}
    \end{equation}
    if and only if the following equations hold:
    \begin{align}\label{eqn.dbl_cat_arena_square_commuting}
      g(j(a^+)) &= k(f(a^+)) \\
      k^{\sharp}(f(a^+), g_{\flat}(j(a^+), c^-)) &= f_{\flat}(a^+, g^{\sharp}(a^+, c^-)) 
    \end{align}
    for all $a^+ \in A^+$ and $c^- \in C^-$.
  \end{itemize}
\end{definition}

It's not obvious from this definition that we actually get a double category
with this definition. It's not even clear that we have defined a way to compose
the squares vertically and horizontally.

It turns out we don't need to know anything else to know that we can compose
these squares, at least in principle. This is because there is at most one
square filling any two charts and two lenses that line up as in
\cref{eqn.dbl_cat_arena_square}; to compose these squares just means that if we
have two such squares lining up, the defining equations
\cref{eqn.dbl_cat_arena_square_commuting} hold also for the appropriate
composites. We call double categories with this property \emph{thin}.

\begin{definition}
  A double category is \emph{thin} if there is at most one square of any
  signature.
\end{definition}

So long as composition is well defined in a thin double category, the laws of
associativity and interchange for square composition come for free; there is at
most one square of the appropriate signature, so any two you can write down are
already equal. We do still have to show that composition is well defined in this
way, which we'll do a bit more generally in \cref{sec.groth_double_construction}

Taking for granted that the double category of arenas is indeed a double
category, what does this mean for systems? Well, behaviors are particular
squares in the double category of arenas.

\begin{proposition}\label{prop.behavior_as_square_of_arenas_discrete}
  Let $\Sys{T}$ and $\Sys{S}$ be dynamical systems. A behavior $\phi : \Sys{T}
  \to \Sys{S}$ is equivalently a square of the following form in the double
  category of arenas:
  \begin{equation}\label{eqn.behavior_as_square_of_arenas_discrete}
    \begin{tikzcd}
      \lens{\State{T}}{\State{T}} \ar[r, shift left, "\lens{\phi \circ
        \pi_2}{\phi}"] \ar[r, shift right] \ar[d, shift right,
      "\lens{\update{T}}{\expose{T}}"'] \ar[d, shift left, leftarrow] &
      \lens{\State{S}}{\State{S}} \ar[d, shift left, leftarrow,
      "\lens{\update{S}}{\expose{S}}"] \ar[d, shift right]\\
      \lens{\In{T}}{\Out{T}} \ar[r, shift right, "\lens{f^{\sharp}}{f}"'] \ar[r,
      shift left] & \lens{\In{S}}{\Out{S}}
    \end{tikzcd}
  \end{equation}
\end{proposition}
\begin{proof}
  This is a simple matter of checking the definitions against eachother. The
  defining equations of \cref{def.double_category_of_arenas_discrete} specialize
  to the defining equations of \cref{def.behavior_discrete}.
\end{proof}

\begin{remark}
  Note that in order to make the square in
  \cref{eqn.behavior_as_square_of_arenas_discrete}, we had to use the section
  $\lens{- \circ \pi_2}{-}$ we constructed in
  \cref{prop.section_charts_discrete} to turn the function $\phi : \State{T} \to
  \State{S}$ into a chart $\lens{\phi \circ \pi_2}{\phi} :
  \lens{\State{T}}{\State{T}} \tto \lens{\State{S}}{\State{S}}$.
\end{remark}

\begin{remark}\label{rmk.double_category_direction}
While the definition of double category we gave treated both horizontal and
vertical directions the same, we will often want to see a square
\[
  \begin{tikzcd}[sep=small]
    A \ar[dd, "j"'] \ar[rr, "f"] & & B \ar[dd, "k"] \\
    & \alpha & \\
    C \ar[rr, "g"'] & & D
  \end{tikzcd}
\]
as a sort of map $\alpha : j \to k$ from its left to its right side, or a map
$\alpha : f \to g$ from its top to its bottom side. For example, the systems
themselves are certain lenses (vertical maps), and the behaviors are squares
between them. On the other hand, we can also see a square as a way of wiring
together charts. 
\end{remark}

\begin{example}\label{ex.understanding_squares_in_double_cat_of_arenas}
  A square
  \begin{equation}\label{eqn.understanding_squares_in_double_cat_of_arenas}
    \begin{tikzcd}
      \lens{A^-}{A^+} \ar[r, shift left, "\lens{f_{\flat}}{f}"] \ar[r, shift
      right] \ar[d, shift right, "\lens{j^{\sharp}}{j}"'] \ar[d, shift left,
      leftarrow] & \lens{B^-}{B^+} \ar[d, shift left, leftarrow,
      "\lens{k^{\sharp}}{k}"] \ar[d, shift right]\\
      \lens{C^-}{C^+} \ar[r, shift right, "\lens{g^{\sharp}}{g}"'] \ar[r, shift
      left] & \lens{D^-}{D^+}
    \end{tikzcd}
    \end{equation}
  can be seen as a map between charts telling us how to wire them together. For example, consider a square of the
  following form where $\lens{w^{\sharp}}{w}$ is a wiring diagram:
  \[
    \begin{tikzcd}
      \lens{\ord{1}}{\ord{1}} \ar[r, shift left, "\lens{b^-}{b^+}"] \ar[r, shift
      right] \ar[d, shift right, equals] \ar[d, shift left, equals] &
      \lens{B^-}{B^+} \ar[d, shift left, leftarrow,
      "\lens{w^{\sharp}}{w}"] \ar[d, shift right]\\
      \lens{\ord{1}}{\ord{1}} \ar[r, shift right, "\lens{d^-}{d^+}"'] \ar[r,
      shift left] & \lens{D^-}{D^+}
    \end{tikzcd}
  \]
  By \cref{ex.special_charts}, we know that the charts in this diagram are pairs
  of elements $\lens{b^-}{b^+}$ and $\lens{d^-}{d^+}$ in the arenas
  $\lens{B^-}{B^+}$ and $\lens{D^-}{D^+}$ respectively. The square then says
  that $\lens{d^-}{d^+}$ are the values you would get if you passed
  $\lens{b^-}{b^+}$ along the wires in the wiring diagram
  $\lens{w^{\sharp}}{w}$.
\end{example}

Let's take a minute to see what composition of squares in the double category of
arenas means for systems. Horizontal composition is familiar because it's what
lets us compose behaviors:
\[
  \begin{tikzcd}
    \lens{\State{T}}{\State{T}} \ar[r, shift left, "\lens{\phi \circ
      \pi_2}{\phi}"] \ar[r, shift right] \ar[d, shift right,
    "\lens{\update{T}}{\expose{T}}"'] \ar[d, shift left, leftarrow] &
    \lens{\State{S}}{\State{S}} \ar[r, shift left, "\lens{\psi \circ
      \pi_2}{\psi}"] \ar[r, shift right] \ar[d, shift left, leftarrow,
    "\lens{\update{S}}{\expose{S}}"] \ar[d, shift right] &
    \lens{\State{U}}{\State{U}}\ar[d, shift left, leftarrow,
    "\lens{\update{U}}{\expose{U}}"] \ar[d, shift right] \\
    \lens{\In{T}}{\Out{T}} \ar[r, shift right, "\lens{f^{\sharp}}{f}"'] \ar[r,
    shift left] & \lens{\In{S}}{\Out{S}} \ar[r, shift right,
    "\lens{g^{\sharp}}{g}"'] \ar[r, shift left]& \lens{\In{U}}{\Out{U}}
  \end{tikzcd}\mapsto
  \begin{tikzcd}
    \lens{\State{T}}{\State{T}} \ar[r, shift left, "\lens{\psi\phi \circ
      \pi_2}{\psi\phi}"] \ar[r, shift right] \ar[d, shift right,
    "\lens{\update{T}}{\expose{T}}"'] \ar[d, shift left, leftarrow] &
    \lens{\State{U}}{\State{U}} \ar[d, shift left, leftarrow,
    "\lens{\update{U}}{\expose{U}}"] \ar[d, shift right]\\
    \lens{\In{T}}{\Out{T}} \ar[r, shift right, "\lens{(gf)^{\sharp}}{gf}"']
    \ar[r, shift left] & \lens{\In{U}}{\Out{U}}
  \end{tikzcd}
\]

On the other hand, vertical composition tells us something else interesting: if
you get a chart $\lens{g_{\flat}}{g}$ by wiring together a chart
$\lens{f_{\flat}}{f}$, then you can turn a behavior $\phi$ with
chart $\lens{f_{\flat}}{f}$ induces a behavior with chart $\lens{g_{\flat}}{g}$
on the wired together systems.
\[
  \begin{tikzcd}
    \lens{\State{T}}{\State{T}} \ar[r, shift left, "\lens{\phi \circ
      \pi_2}{\psi\phi}"] \ar[r, shift right] \ar[d, shift right,
    "\lens{\update{T}}{\expose{T}}"'] \ar[d, shift left, leftarrow] &
    \lens{\State{S}}{\State{S}} \ar[d, shift left, leftarrow,
    "\lens{\update{S}}{\expose{S}}"] \ar[d, shift right]\\
    \lens{\In{T}}{\Out{T}} \ar[d, shift right, "\lens{j^{\sharp}}{j}"'] \ar[d, shift left,
        leftarrow] \ar[r, shift right, "\lens{f^{\sharp}}{f}"']
    \ar[r, shift left] & \lens{\In{S}}{\Out{S}} \ar[d, shift left, leftarrow,
        "\lens{k^{\sharp}}{k}"] \ar[d, shift right]\\
    \lens{I}{O} \ar[r, shift right, "\lens{g^{\sharp}}{g}"']
    \ar[r, shift left] & \lens{I'}{O'} \\
  \end{tikzcd} \mapsto
  \begin{tikzcd}
    \lens{\State{T}}{\State{T}} \ar[r, shift left, "\lens{\psi\phi \circ
      \pi_2}{\psi\phi}"] \ar[r, shift right] \ar[d, shift right,
    "\lens{f^{\sharp}}{f} \circ \lens{\update{T}}{\expose{T}}"'] \ar[d, shift left, leftarrow] &
    \lens{\State{S}}{\State{S}} \ar[d, shift left, leftarrow,
    "\lens{k^{\sharp}}{k} \circ \lens{\update{S}}{\expose{S}}"] \ar[d, shift right]\\
    \lens{I}{O} \ar[r, shift right, "\lens{g^{\sharp}}{g}"']
    \ar[r, shift left] & \lens{I}{O'}
  \end{tikzcd}
\]

\begin{example}\label{ex.understanding_squares_in_double_cat_of_arenas2}
  Continuing from \cref{ex.understanding_squares_in_double_cat_of_arenas},
  suppose that we have a $\lens{b^-}{b^+}$-steady state $s$ in a system $\Sys{S}$:
  \begin{equation}\label{eqn.understanding_squares_in_double_cat_of_arenas2}
    \begin{tikzcd}
      \lens{\ord{1}}{\ord{1}} \ar[r, shift left, "\lens{s}{s}"] \ar[r, shift
      right] \ar[d, shift right, equals] \ar[d, shift left, equals] &
      \lens{\State{S}}{\State{S}} \ar[d, shift left, leftarrow,
      "\lens{\update{S}}{\expose{S}}"] \ar[d, shift right]\\
      \lens{\ord{1}}{\ord{1}} \ar[r, shift right, "\lens{b^+}{b^-}"'] \ar[r,
      shift left] & \lens{B^-}{B^+}
    \end{tikzcd}
  \end{equation}
  We can see that $s$ is a $\lens{d^-}{d^+}$-steady state of the wired system by
  vertically composing the square in
  \cref{eqn.understanding_squares_in_double_cat_of_arenas2} with the square in
  \cref{eqn.understanding_squares_in_double_cat_of_arenas}. This basic fact
  underlies our arguments in \cref{sec.steady_states_matrix_arithmetic}.
\end{example}

\paragraph{The Grothendieck double
  construction.} \label{sec.groth_double_construction}

It does remain for us to show that \cref{def.double_category_of_arenas_discrete}
does indeed give a double category. We could do this by hand, but we'll do it
diagrammatically by generalizing the double category of arenas to any indexed
category.

\begin{definition}\label{def.groth_double_construction}
  Let $\cat{A} : \cat{C}\op \to \Cat{Cat}$ be an indexed category. The
  \emph{Grothendieck double construction}
$$\sqiint^{C \in \cat{C}} \cat{A}(C)$$
is the double category defined by:
\begin{itemize}
\item Its objects are the pairs $\lens{A}{C}$ of an object $C \in \cat{C}$ and
  an object $A \in \cat{A}(C)$.
\item Its horizontal category is the Grothendieck construction $\int^{C \in
    \cat{C}}\cat{A}(C)$ of $\cat{A}$.
\item Its vertical category is the Grothendieck construction $\int^{C \in
    \cat{C}} \cat{A}(C)\op$ of the opposite of $\cat{A}$.
\item There is a square of the following form:
  \begin{equation}\label{eqn.groth_double_construction}
    \begin{tikzcd}
      \lens{A_1}{C_1} \arrow[r, shift left, "\lens{g_{1\flat}}{g_1}"]\arrow[r, shift right] \arrow[d, leftarrow,  shift left] \arrow[d, shift right, "\lens{f_1^{\sharp}}{f_1}"'] & \lens{A_2}{C_2} \arrow[d, leftarrow, shift left, "\lens{f_2^{\sharp}}{f_2}"] \arrow[d, shift right] \\
      \lens{A_3}{C_3} \arrow[r, shift left]\arrow[r, shift right,
      "\lens{g_{2\flat}}{g_2}"'] & \lens{A_4}{C_4}
    \end{tikzcd}
  \end{equation}
  if and only if the following diagrams commute:
  \begin{equation} \label{eqn.groth_double_diagram}
    \begin{tikzcd}
      C_1 \arrow[r, "g_1"] \arrow[d, "f_1"'] & C_2 \arrow[d, "f_2"] &  & f_1^{\ast}A_3 \arrow[rr, "f_1^{\sharp}"] \arrow[d, "f_1^{\ast}g_{2\flat}"'] &                                                              & A_1 \arrow[d, "g_{1\flat}"] \\
      C_3 \arrow[r, "g_2"'] & C_4 & & f_1^{\ast}g_2^{\ast}A_4 \arrow[r, equals]
      & g_1^{\ast}f_2^{\ast}A_4 \arrow[r, "g_1^{\ast}f_2^{\sharp}"'] &
      g_1^{\ast}A_2
    \end{tikzcd}
  \end{equation}
  We will call the squares in the Grothendieck double construction
  \emph{commuting squares}, since they represent the proposition that the
  ``lower'' and ``upper'' squares appearing in their boundary commute.
\item Composition is given as in the appropriate Grothendieck constructions.
\end{itemize}

\end{definition}

It just remains to show that commuting squares compose.
\begin{itemize}
\item For vertical composition we appeal to the following diagram:
  \[
    \begin{tikzcd}
      f_1^{\ast}f_3^{\ast} A_5 \arrow[rr, "f_1^{\ast}f_3^{\sharp}"] \arrow[d, "f_1^{\ast}f_3^{\ast}g_{3\flat}"'] &                                                                                 & f_1^{\ast}A_3 \arrow[r, "f_1^{\sharp}"] \arrow[d, "f_1^{\ast}g_{2\flat}"] & A_1 \arrow[dd, "g_{1\flat}"] \\
      f_1^{\ast}f_3^{\ast}g_3^{\ast}A_6 \arrow[d,equals] \arrow[r,equals]                                                       & f_1^{\ast}g_2^{\ast}f_4^{\ast}A_6 \arrow[r, "f_1^{\ast}g_2^{\ast}f_4^{\sharp}"] & f_1^{\ast}g_2^{\ast}A_4 \arrow[d,equals]                                           &                               \\
      g_1^{\ast}f_2^{\ast}f_4^{\ast}A_6 \arrow[rr,
      "g_1^{\ast}f_2^{\ast}f_4^{\sharp}"'] & & g_1^{\ast}f_2^{\ast}A_4 \arrow[r,
      "g_1^{\ast}f_2^{\sharp}"'] & g_1^{\ast}A_2
    \end{tikzcd}
  \]
  The outer diagram is the ``upper'' square of the composite, while the
  ``upper'' squares of each factor appear in the top left and right
  respectively.
\item For horizontal composition we appeal to the following diagram:
  \[
    \begin{tikzcd}
      f_1^{\ast}A_3 \arrow[rr, "f_1^{\sharp}"] \arrow[d, "f_1^{\ast}g_{2\flat}"']     &                                                                                                           & A_1 \arrow[d, "g_{1\flat}"]                      \\
      f_1^{\ast}g_2^{\ast}A_4 \arrow[r,equals] \arrow[dd, "f_1^{\ast}g_2^{\ast}g_{4\flat}"'] & g_1^{\ast}f_2^{\ast}A_4 \arrow[r, "g_1^{\ast}f_2^{\sharp}"] \arrow[d, "g_1^{\ast}f_2^{\ast}g_{4\flat}"'] & g_1^{\ast}A_2 \arrow[dd, "g_1^{\ast}g_{3\flat}"] \\
      & g_1^{\ast}f_2^{\ast}g_4^{\ast}A_6 \arrow[d,equals]                                                               &                                                   \\
      f_1^{\ast}g_2^{\ast}g_4^{\ast}A_6 \arrow[r,equals] &
      g_1^{\ast}g_3^{\ast}f_3^{\ast} \arrow[r,
      "g_1^{\ast}g_3^{\ast}f_3^{\sharp}"'] & g_1^{\ast}g_3^{\ast}
    \end{tikzcd}
  \]
\end{itemize}

We can now check that this does indeed abstract the double category of arenas.

\begin{proposition}\label{prop.arenas_double_groth_contextual_maps}
  The double category of arenas in the deterministic doctrine is the
  Grothendieck double construction of the indexed category of sets and functions
  in context $\smctx{-} : \smset\op \to \Cat{Cat}$:
$$\Cat{Arena} = \sqiint^{C \in \smset} \smctx{C}.$$ 
\end{proposition}
\begin{proof}
  By \cref{prop.charts_as_groth_construction,prop.lenses_as_groth_construction},
  the horizontal and vertical categories are the same. It remains to show that
  the diagrams of \cref{eqn.groth_double_construction} mean the same things as
  \cref{eqn.dbl_cat_arena_square_commuting}.

  Consider a square of the form
  \[
    \begin{tikzcd}
      \lens{A^-}{A^+} \ar[r, shift left, "\lens{f_{\flat}}{f}"] \ar[r, shift
      right] \ar[d, shift right, "\lens{j^{\sharp}}{j}"'] \ar[d, shift left,
      leftarrow] & \lens{B^-}{B^+} \ar[d, shift left, leftarrow,
      "\lens{k^{\sharp}}{k}"] \ar[d, shift right]\\
      \lens{C^-}{C^+} \ar[r, shift right, "\lens{g^{\sharp}}{g}"'] \ar[r, shift
      left] & \lens{D^-}{D^+}
    \end{tikzcd}
  \]
  The first diagram and first equation say:
  \[
    \begin{tikzcd}
      A^+ \ar[d, "j"'] \ar[r, "f"] & B^+ \ar[d, "k"] \\
      C^+ \ar[r, "g"'] & D^+
    \end{tikzcd}
    \quad\quad g(j(a^+)) &= k(f(a^+)) \quad\mbox{for all $a^+ \in A^+$,}
  \]
  which mean the same thing. The second diagram, which takes place in
  $\smctx{A^+}$, is more interesting. Here's that diagram with the names we're
  currently using:
  \[
    \begin{tikzcd}
      j^{\ast} C^- \arrow[rr, "j^{\sharp}"] \arrow[d, "j^{\ast}g_{\flat}"'] &
      &  A^- \arrow[d, "f_{\flat}"] \\
      j^{\ast}g^{\ast}D^- \arrow[r, equals] & f^{\ast}k^{\ast}D^- \arrow[r,
      "f^{\ast}k^{\sharp}"'] & f^{\ast}B^-
    \end{tikzcd}
  \]
  Let's compute the two paths from the top left to the bottom right. First is
  $f_{\flat} \circ j^{\sharp} : j^{\ast} C^- \to f^{\ast}B^-$, which sends
  $(a^+, c^-)$ to $f_{\flat}(a^+, j^{\sharp}(a^+, c^-))$. This is the right hand
  side of the second equation, so we're on the right track. The other path is
  $f^{\ast}k^{\sharp} \circ j^{\ast}g_{\flat}$. Recall that $j^{\ast}g_{\flat}$
  sends $(a^+, c^-)$ to $g_{\flat}(j(a^+), c^-)$, and similarly
  $f^{\ast}k^{\sharp}$ sends $(a^+, d^-)$ to $k^{\sharp}(f(a^+), d^-)$. Putting
  them together, we send $(a^+, c^-)$ to $k^{\sharp}(f(a^+), g_{\flat}(j(a^+),
  c^-))$. Therefore the commutation of this diagram means the same thing as the
  second equation in the definition of a square of arenas.
\end{proof}


\subsubsection{The double category of sets, functions, and matrices}\label{sec.double_cat_of_matrices}

Now we turn to our second double category of interest, the double category of
sets, functions, and matrices of sets.
\begin{definition}\label{def.double_cat_of_matrices}
  The double category $\Cat{Matrix}$ of sets, functions, and matrices of sets
  is defined by:
  \begin{itemize}
    \item Its objects are sets.
    \item Its horizontal category is the category of sets and functions.
    \item Its vertical category is the category of sets and matrices of sets,
      where composition is given by matrix multiplication. We write $M : A
      \to B$ to say that $M$ is a $B \times A$ matrix.\footnote{Observant
        readers will notice that multiplication of matrices of sets isn't
        strictly associative or unital. It only satisfies those properties up to
      isomorphism. Technically, we are defining a form of \emph{weak} double
      category, where one form of composition is only defined up to isomorphism.
    But since these isomorphisms are a trivial form of book-keeping, we'll sweep
    this inconvenient fact under the rug.}
     \item For functions $f : A \to B$ and $g : C \to D$ and matrices $M : A
       \to C$ and $N : B \to D$, a square
       \[
        \begin{tikzcd}[sep=small]
          A \ar[dd, "M"'] \ar[rr, "f"] & & B \ar[dd, "N"] \\
           & \alpha & \\
          C \ar[rr, "g"'] & & D
        \end{tikzcd}
       \]
       is a family of functions $\alpha_{ba} : M_{ba} \to N_{g(b)f(a)}$ for all
       $a \in A$ and $b \in B$.
     \item Horizontal composition of squares is given by composition of the
       families:
       $$(\alpha | \beta)_{ba} = \beta_{g(b)f(a)} \circ \alpha_{ba}.$$
     \item Vertical composition of squares is given by
       \begin{align*}
         \left( \frac{\alpha}{\beta} \right)_{ac} : \sum_{b_1 \in B_1} M^2_{cb} \times M^1_{ba} &\to \sum_{b_2 \in B_2} N^2_{h(c)b_2} \times N^1{b_2f(a)} \\
         (b_1, m_2, m_1) &\mapsto (g(b_1), \beta(m_2), \alpha(m_1)).
       \end{align*}
  \end{itemize}
\end{definition}

\begin{exercise}\label{ex.double_cat_of_matrices}
  We can see that horizontal composition of squares is associative and unital
  since it is basically just function composition. Show that $\Cat{Matrix}$ is a
  double category by checking that
  \begin{enumerate}
    \item Vertical composition of squares is associative and unital (up to isomorphism).
      \item The interchange law holds.
  \end{enumerate}
\end{exercise}




\subsubsection{The double category of categories, profunctors, and functors}

Now we come to the primordial double category: the double category of
categories, \emph{profunctors}, and functors. This is an important double
category because it is in some sense the setting in which all category theory
takes place. Before we describe this double category, let's define the notion of
profunctor and their category.
\begin{definition}
  A \emph{profunctor} $P : \cat{A} \tickar \cat{B}$ is a functor $P : \cat{A}\op
  \times \cat{B} \to \smset$. Given objects $A \in \cat{A}$ and $B \in \cat{B}$,
  we write an element $p \in P(A, B)$ as $p : A \tickar B$. 

In terms of this,
  the functoriality of $P$ can be seen as letting us compose $p : A \tickar B$
  on the left and right by $f : A' \to A$ and $g : B \to B'$ to get $fpg : A'
  \tickar B'$. In other words, we can interpret a diagram of this form
$$A' \xto{f} A \xtickar{p} B \xto{g} B'$$
as an element of $P(A', B')$.
\end{definition}

If we call maps $f : A' \to A$ in a category $\cat{A}$ \emph{homomorphisms}
because they go between objects of the same form, we could call elements $p : A
\xtickar B$ --- that is, $p \in P(A, B)$ --- as \emph{heteromorphisms}, maps
going between objects of different forms.

We can't necessarily compose these heteromorphisms, which we can see right away
from their signature: for $p : A \tickar B$, there is always an object of $\cat{A}$ on
the left and an object of $\cat{B}$ on the right, so we'll never be able to line
two of them up. However, if we have another
profunctor $Q : \cat{B} \tickar \cat{C}$ --- another notion of heteromorphism
--- then we can ``compose'' heteromorphisms $A \xtickar{p} B$ in $P$ with $B
\xtickar{q} C$ in $Q$ to get a heteromorphism $A \xtickar{p} B \xtickar{q} C$ in a new
profunctor $P \odot Q : \cat{A} \tickar \cat{C}$.

\begin{definition}
  The composite $P \odot Q$ of a profunctor $P : \cat{A} \tickar \cat{B}$ with a
  profunctor $Q : \cat{B} \tickar \cat{C}$ is defined to be the following quotient:
\begin{equation}\label{eqn.profunctor_composition}
  (P \odot Q)(A, C) := \frac{\sum_{B \in \cat{B}}P(A, B) \times Q(B, C)}{(pf, q) \sim (p, fq)}
\end{equation}
We write an element $[(p, q)] \in (P \odot Q)(A, C)$ as $A \xtickar{p} B
\xtickar{q} C$, so that the relation we quotient by says that 
$$A \xtickar{p} B \xto{f} B' \xtickar{q} C$$
has a unique interpretation as an element of $P \odot Q$. 

The identity profunctor $\cat{A} : \cat{A} \tickar \cat{A}$ is the hom-functor
sending $A$ and $A'$ to the set $\cat{A}(A, A')$ of maps $A \to A'$. 
\end{definition}

We can see that composition of profunctors is associative (up to isomorphism)
because the objects of $P \odot (Q \odot R)$ and $(P \odot Q) \odot R$ can both
be written as 
$$A \xtickar{p} B \xtickar{q} C \xtickar{r} D.$$
The reason the hom profunctor $\cat{A} : \cat{A} \tickar \cat{A}$ is the
identity profunctor is because the elements of $\cat{A} \odot P$ would be
written as
$$A' \xto{f} A \xtickar{p} B$$
but by the functoriality of $P$, this is already an element of $P(A', B)$, 
which is to say more precisely that every equivalence class $[(f, p)] \in
(\cat{A} \odot P)(A', B)$ is equally presented as $[(\id_{A'}, fp)]$. 

\begin{remark}
  We omit full proofs of associativity and unitality for profunctor composition
  because they are best done with the \emph{coend calculus}, and this would take
  us quite far afield. 
  %%
  %% :CUSTOM-ID:cite-me.profunctors
  %%
\end{remark}

\begin{example}
  A profunctor $\ord{1} \tickar \cat{A}$ is the same thing as a functor $\cat{A}
  \to \smset$, and a profunctor $\cat{A} \tickar \ord{1}$ is the same thing as a
  functor $\cat{A}\op \to \smset$. Profunctors are therefore intimately related
  with presheaves.
\end{example}

\begin{example}
  The notion of profunctor gives us a nice way to understand the relationship
  between a behavior $\phi : \Sys{T} \to \Sys{S}$ and its chart
  $\lens{f_{\flat}}{f} : \lens{I}{O} \tto \lens{I'}{O'}$.

  As we saw in \cref{def.cat_of_systems_discrete}, for every arena $\lens{I}{O}$
  we have a category $\Cat{Sys}\lens{I}{O}$ of systems with interface
  $\lens{I}{O}$. Given a chart $\lens{f_{\flat}}{f} : \lens{I}{O} \tto
  \lens{I'}{O'}$, we get a profunctor
  $$\Cat{Sys}\lens{f_{\flat}}{f} : \Cat{Sys}\lens{I}{O} \tickar
  \Cat{Sys}\lens{I'}{O'}$$

  Which is defined like this:
\begin{align*}
  \Cat{Sys}\lens{f_{\flat}}{f}(\Sys{T}, \Sys{S}) &= \left\{ \phi : \State{T} \to
                                                   \State{S}\, \middle| \mbox{ $\phi$ is a behavior with chart $\lens{f_{\flat}}{f}$} \right\}\\
  &= \left\{  
    {
    \begin{tikzcd}[ampersand replacement = \&]
      \lens{\State{T}}{\State{T}} \ar[r, dashed, shift left, "\lens{\phi \circ
        \pi_2}{\phi}"] \ar[r, dashed, shift right] \ar[d, shift right,
      "\lens{\update{T}}{\expose{T}}"'] \ar[d, shift left, leftarrow] \&
      \lens{\State{S}}{\State{S}} \ar[d, shift left, leftarrow,
      "\lens{\update{S}}{\expose{S}}"] \ar[d, shift right]\\
      \lens{\In{T}}{\Out{T}} \ar[r, shift right, "\lens{f^{\sharp}}{f}"'] \ar[r,
      shift left] \& \lens{\In{S}}{\Out{S}}
    \end{tikzcd}
                    }
                    \right\}
\end{align*}

The action of the profunctor $\Cat{Sys}\lens{f_{\flat}}{f}$ on behaviors in the
categories $\Cat{Sys}\lens{I}{O}$ and $\Cat{Sys}\lens{I'}{O'}$ is given by
composition on the left and right.
\end{example}

Now, we are ready to put functors and profunctors together into a double category.
\begin{definition}\label{def.double_cat_of_cats}
The double category $\Cat{Cat}$ of categories, profunctors, and functors
has
\begin{itemize}
\item Objects the categories.
\item Horizontal category the category of categories and profunctors.
\item Vertical category the category of categories and functors between them.
\item A square 
\[
\begin{tikzcd}[sep = small]
  \cat{A} \ar[rr, tick, "P"] \ar[dd, "F"'] & & \cat{B} \ar[dd, "G"] \\
  & \alpha & \\
\cat{C} \ar[rr, tick, "Q"'] & & \cat{D}
\end{tikzcd}
\]
Is a natural transformation $\alpha : P \rightarrow Q(F, G)$, where $Q(F, G)$ is
the profunctor $\cat{A}\op \times \cat{B} \xto{F\op \times G} \cat{C} \times
\cat{D} \xto{Q} \smset$. For $p : A \tickar B$, we have $\alpha(p) : FA \tickar
GB$, and naturality says that $\alpha(fpg) = (Ff)\alpha(p)(Gg)$.
\item Vertical composition of squares is given by composing the natural transformations.
\item Given squares $\alpha : P_1 \rightarrow Q_1(F_1, F_2)$ and $\beta : P_2
  \to Q_2(F_2, F_3)$, we define their horizontal composite $\alpha \mid \beta :
  P_1 \cdot P_2 \to (Q_1 \cdot Q_2)(F_1, F_3)$ by 
$$(\alpha \mid \beta)(A_1 \xtickar{p_1} A_2 \xtickar{p_2} A_3) = F_1 A_1
\xtickar{\alpha(p_1)} F_2 A_2 \xtickar{\beta(p_2)} F_3 A_3$$
and checking that this descends correctly to the quotient.
\end{itemize}
\end{definition}




%---- Subsection ----%
\subsection{The indexed double category of deterministic systems}
\label{sec.indexed_double_category_of_systems}



%% write-here

%---- Subsection ----%
\subsection{The big theorem: representable indexed double functors}




\end{document}