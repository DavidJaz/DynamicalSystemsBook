\documentclass[DynamicalBook]{subfiles}
\begin{document}
%


\setcounter{chapter}{2}%Just finished 1.


%------------ Chapter ------------%
\chapter{How systems behave}\label{chapter.2}

\section{Introduction}

So far, we have seen how to wire up dynamical systems. But we haven't seen our
dynamical systems actually \emph{do anything}. In this section, we will begin to
study the behavior of our dynamical systems. We will see particular kinds of
behaviors our systems can have:
trajectories, steady states, and periodic orbits.

\begin{informal}
  A \emph{behavior} of a dynamical system is a particular way its states can
  change according to its dynamics.  

  There are different \emph{kinds of behavior} corresponding to the different
  sorts of ways that the states of a system could evolve. Perhaps they eventually
  repeat, or they stay the same despite changing conditions.
\end{informal}

In \cref{sec.behaviors}, we will give a formal definition of behavior of
dynamical system. We will see that the different kinds of behaviors ---
trajectories, steady states, periodic orbits, etc. --- can each
be packaged up into a single system\footnote{Or family of systems.} that \emph{represents} that kind of
behavior. This system will behave in exactly that kind of way, and do nothing else. Maps from it to a system of interest will exhibit that sort of behavior in the system of interest.

We will then investigate the definition of behaviors in terms of a \emph{double
  category} which merges together the category of lenses with a category of
\emph{charts} (which are important for defining behaviors). We will see that
behaviors are certain squares in this double category, and see what using this
doublec category can tell us about how behaviors of component systems relate to
the behaviors of composed systems. 


%---- Section ----%
\section{Kinds of behavior}\label{sec.kinds_of_behavior}
\subsection{Trajectories}

A trajectory is the simplest and freest sort of behavior a system can have. A
trajectory is just ``what a state does''. In this section, we will see what
trajectories look like in the deterministic and differential doctrines.

\subsubsection{Trajectories in the deterministic doctrine}\label{sec.trajectory_discrete}
In the introduction, we saw that the $\Sys{Clock}$ system
\cref{eqn.clock_system_box} has behaves in this way if it starts at $3$ o'clock: 
$$3 \xmapsto{\fun{tick}} 4 \xmapsto{\fun{tick}} 5 \xmapsto{\fun{tick}} 6
\xmapsto{\fun{tick}} \cdots$$

This sequence of states of the clock system, each following from the last by the
dynamics of the system, is called a \emph{trajectory}. When our systems have
input parameters, we will need to choose a sequence of input parameters to feed
the system in order for the states to change.

\begin{definition}\label{def.trajectory_discrete}
 Let $$\Sys{S} = \lens{\update{S}}{\expose{S}} : \lens{\State{S}}{\State{S}}
 \leftrightarrows \lens{\In{S}}{\Out{S}}$$
 be a deterministic system. Suppose that $p : \nn \to \In{S}$ is a sequence of
 parameters for $\Sys{S}$. Then a \emph{$p$-trajectory} of $\Sys{S}$ is a sequence $s : \nn \to \State{S}$ of states so that
 $$\update{S}(s_i, p_i) = s_{i + 1}$$
 for all $i \in \nn$.

 If additionally $v : \nn \to \Out{S}$ is a sequence of output values for $\Sys{S}$, then a
 \emph{$\lens{p}{v}$-trajectory} is a sequence of states $s : \nn \to \State{S}$ so
 that
 \begin{align*}
   \update{S}(s_i, p_i) &= s_{i + 1}\\
   \expose{S}(s_i) &= v_i
 \end{align*}
 for all $i \in \nn$. We call the pair $\lens{p}{v}$ the \emph{chart} of the trajectory $s$.
\end{definition}

Its worth noting that a trajectory $s : \nn \to \State{S}$ in a deterministic system is determined
entirely by its start state $s_0$. This is what makes deterministic systems
deterministic; if you know the dynamics and you know what state the system is
in, you know how it will continue to behave. We'll relax this condition later in \cref{??}.

\begin{example}
Consider the SIR model of \cref{ex.SIR_model_discrete}. Suppose that we let our
parameters $(a, b) : \nn \to \In{\Sys{SIR}}$ be constant at $.2$ and $.3$
respectively: that is, $a_t = .2$ and $
b_t = .3$ for all $t$. Then a trajectory for $\Sys{SIR}$ with parameters $(a, b)$
is a sequence of populations $(s, i, r) : \nn \to \State{SIR}$ such that
$$\begin{bmatrix}s_{t+1}\\ i_{t+1}\\ r_{t+1} \end{bmatrix} = \begin{bmatrix}
  s_t - .2s_t i_t\\ i_t + .2s_t i_t - .3i_t \\ r_t + .3i_t\end{bmatrix}$$

Here is an example of such a trajectory with a $1000$ total people and one infected
person to start, that is $(s_0, i_0, r_0) = (999, 1, 0)$.
\[
  \begin{tikzpicture}
    horizontal axis
    \draw[->] (0,0) to (11, 0);
    \draw[label]
      node at (11, -.3) {$t$};
    \foreach \x in {1,...,10}
      \draw (\x cm,1pt) -- (\x cm,-1pt) node[anchor=north] {$\x$};

   %% vertical axis
   \draw[->] (0,0) to (0, 11);
   \draw[label] node at (-1, 11) {$(s, i, r)$};
   \foreach \y in {1,...,10}
   {
     \pgfmathtruncatemacro{\display}{100*\y};
     \draw (1pt,\y cm) -- (-1pt,\y cm) node[anchor=east] {$\display$};
   }

%   %% plot
%   \tikzmath{
%     real \s; 
%     \s = 9.99;
%   
%     function srecusion(\x){
%       return {\x-1};
%     }
%     \foreach \t in {0,1,...,10}
%     {
%       \node[draw,circle,red] at (\t, \s) {};
%       \s = srecursion(\s);
%     }
%    }
  \end{tikzpicture}
\]
\jaz{I don't know how to actually plot this...}
\end{example}


\begin{example}\label{ex.transition_diagram_discrete_traj}
If a deterministic system is written as a transition diagram, then the
trajectories in the system are paths through the diagram. Recall from
\cref{ex.transition_diagram_discrete} this system:
\[
\begin{tikzpicture}
	\node[draw] {
  \begin{tikzcd}[column sep=small]
  	\LMO{a}\ar[rr, dgreen, thick, bend left]\ar[loop left, thick, orange]&&
  	\LMO{b}\ar[ll, thick, orange, bend left]\ar[dl, bend left, thick, dgreen]\\&
  	\LMO{b} \ar[ul, thick, orange, bend left] \ar[loop left, thick, dgreen]
  \end{tikzcd}
  };
\end{tikzpicture}
\]

Suppose that $p : \nn \to \{\Green, \Orange\}$
alternates between $\Green$ and $\Orange$. Then
starting at the top right state, a trajectory quickly settles into alternating between the top two
states:
\[
\begin{tikzpicture}
	\node[draw] {
  \begin{tikzcd}[column sep=small]
    \LMO{b} \ar[r, dgreen, thick] & \LMO{b} \ar[r, orange, thick] & \LMO{a} \ar[r, dgreen,thick] & \LMO{b} \ar[r, orange, thick] & \LMO{a} \ar[r, dgreen, thick] & \cdots
  \end{tikzcd}
  };
\end{tikzpicture}
\]

\end{example}

Knowing about trajectories can show us another important role that deterministic
systems play: they are \emph{stream transformers}. From a stream $p : \nn \to
\In{S}$ of inputs and a start state $s_0 \in \State{S}$, we get a trajectory $s
: \nn \to \State{S}$ given recursively by
\begin{align*}
  s_{t+1} &:= \update{S}(s_t,\, p_t).
\end{align*}
We then get a stream $v : \nn \to \Out{S}$ of output values by defining
$$v_t := \expose{S}(s_t).$$
The system $\Sys{S}$ is a way of transforming streams of input parameters into
streams of output values.
\begin{proposition}[Deterministic systems as stream transformers]
 Let $$\Sys{S} = \lens{\update{S}}{\expose{S}} : \lens{\State{S}}{\State{S}}
 \leftrightarrows \lens{\In{S}}{\Out{S}}$$
 be a deterministic system. Then for every $s_0 \in \State{S}$, we get a stream
 transformation function
 $$\fun{transform}_{\Sys{S}} : \In{S}^{\nn} \to \Out{S}^{\nn}$$
 Given by
 \begin{align*}
   \fun{transform}_{\Sys{S}}(p)_0 &= \expose{S}(s_0)\\
   \fun{transform}_{\Sys{S}}(p)_{t + 1} &= \expose{S}(\update{S}(s_t,\, p_t))
 \end{align*}
 where $s_{t+1} = \update{S}({s_t,\, p_t})$ is the trajectory given by $s_0$.
\end{proposition}

\begin{exercise}
 Say how the system of \cref{ex.transition_diagram_discrete_traj} acts as a
 stream transformer on the following streams:
 \begin{enumerate}
   \item $p_{2t} = \Green$ and $p_{2t + 1} = \Orange$.
   \item $p_t = \Green$.
   \item $p_0 = \Green$ and $p_t = \Orange$ for
     all $t > 0$.
     \qedhere
 \end{enumerate}
\end{exercise}

Later, in \cref{sec.representables}, we will see that given trajectories of
component systems, we get a trajectory of a whole wired system. Even better,
every trajectory of the whole wired system can be calculated this way.

\subsubsection{Trajectories in the differential doctrine}\label{sec.trajectory_differential}

In a differential system, there is no ``next'' state after a given state. All we
know is how each state is tending to change. So to define a trajectory in the
differential doctrine, we can't just pick a state and see how it updates;
instead, we are going to pick a state $s_t$ for every time $t \in \mathbb{R}$
which are changing in the way described by the system. 

\begin{definition}\label{def.trajectory_diff}
  Let
  $$\Sys{S} = \lens{\update{S}}{\expose{S}} : \lens{\State{S}}{\State{S}}
 \leftrightarrows \lens{\In{S}}{\Out{S}}$$
 be a differential system. Suppose that $p : \rr \to \In{S}$ is a differentiable
 choice of parameter for all times $t \in \rr$. Then a $p$-\emph{trajectory} is a
 differentiable
 function $s : \rr \to \State{S}$ so that
 $$\update{S}(s_t, p_t) = \frac{ds}{dt}(t).$$
 for all $t \in \rr$. Here, $\frac{ds}{dt}$ is the vector of derivatives
 $\frac{ds_i}{dt}$ for $i \in \{1, \ldots, n\}$ where $n$ is the number of state variables.

 If, additionally, $v : \rr \to \Out{S}$ is a differentiable choice of outputs,
 then a $\lens{p}{v}$-\emph{trajectory} is a differentiable function $s : \rr
 \to \State{S}$ so that
 \begin{align*}
   \update{S}(s_t, p_t) &= \frac{ds}{dt}(t) \\
   \expose{S}(s_t) &= v_t
 \end{align*}
 for all $t \in \rr$. We call the pair $\lens{p}{v}$ the \emph{chart} of the
 trajectory $s$.
\end{definition}
\begin{remark}
  A $p$-trajectory of a differential system is also referred to as a \emph{solution}
  of the differential equation it represents which choice of parameters $p$.
\end{remark}

The definition of trajectory is what makes our differential systems actually
describe differential equations. Consider the Lotka-Volterra preditor prey model
from \cref{sec.differential_system}:
\begin{equation}\label{eqn.lotka_volterra_model}
  \begin{cases}
    
\frac{dr}{dt} =  \const{b}_{\Sys{Rabbits}}
\cdot r - c_1 f r \\
\frac{df}{dt} = c_2 r f - \const{d}_{\Sys{Foxes}}
\cdot f
  \end{cases}
\end{equation}

  Strictly speaking, this is not how we represent the system of differential
  equations as a differential system. Instead, we would describe its update
  function $\update{LK} : \rr^3 \times \rr^2 \to \rr^3$ as
  \[
\update{LK}\left( \begin{bmatrix}S \\ I \\ R \end{bmatrix}, \begin{bmatrix}
    \const{b}_{\Sys{Rabbits}} \\ \const{d}_{\Sys{Foxes}} \end{bmatrix}\right)
  \coloneqq \begin{bmatrix}\const{b}_{\Sys{Rabbits}}
\cdot r - c_1 f r \\ c_2 r f - \const{d}_{\Sys{Foxes}}
\cdot f
 \end{bmatrix}
  \]
The differential equations \cref{eqn.lotka_volterra_model} are the defining equations
which make the function
$$t \mapsto \begin{bmatrix}S(t) \\ I(t) \\ R(t) \end{bmatrix} : \rr \to \rr^3$$
a $\begin{bmatrix}\const{b}_{\Sys{Rabbits}} \\
  \const{d}_{\Sys{Foxes}} \end{bmatrix}$-trajectory. That is, we interpret a
differential system $\lens{\update{S}}{\expose{S}}$ as a system of differential
equations by considering the equations which define what it means for a $s : \rr
\to \State{S}$ to be a trajectory.
  
Unlike deterministic systems, it is not necessarily the case that a state
uniquely determines a trajectory through it for differentiable systems. This is
the case, however, if the differential equations are linear.

%
% :CUSTOM-ID:cite-me.SIR
%
\begin{example}
  Consider the following variant of an SIR model proposed by Normal Bailey in
  \cite{BaileySIR}:
  \[
    \begin{cases}
      \frac{dS}{dt} &= \frac{-b SI}{S + I} \\
      \frac{dI}{dt} &= \frac{b SI}{S + I} - b I \\
      \frac{dR}{dt} &= bI
    \end{cases}
  \]

  That is,
  \[
\update{SIR}\left( \begin{bmatrix} S \\ I \\ R\end{bmatrix} \right)
= \begin{bmatrix}\frac{-b SI}{S + I} \\ \frac{b SI}{S + I} - b I\\ bI\end{bmatrix}.
  \]
  We note that the total population $N = S + I + R$ will always be constant.
  Suppose, for simplicity, that $b$ is a constant. Suppose that $S_0$
  and $I_0$ are initial values for susceptible and infected populations
  respectively, and let $\kappa \coloneqq \frac{I_0}{S_0}$. Then the function
\[
\begin{bmatrix} S(t) \\ I(i) \\ R(t) \end{bmatrix}
\coloneqq \begin{bmatrix} S_0 e^{-\frac{b\kappa t}{1 + \kappa}} \\
  I_0e^{-\frac{b\kappa t}{1 + \kappa}} \\ N - (S_0 + I_0)e^{-\frac{b\kappa t}{1 + \kappa}} \end{bmatrix}
\]
will be a $b$-trajectory for $\Sys{SIR}$. This can be solved in greater
generality, for variable parameter $b$ and for two separate parameters governing the
transition from susceptible to infected and infected to removed; see \cite{SIR_solved}.
\end{example}

\begin{example}\label{ex.RL_circuit_trajectory}
  In this example, we will consider a simple $\Sys{RL}$-circuit:
  \[
    \begin{circuitikz}
      \draw (0,0)
      to[V,v=$V$] (0,2) % The voltage source
      to (2,2)
      to[R=$R$] (2,0) % The resistor
      to[L=$L$] (0,0);
    \end{circuitikz}
  \]
  The voltage across the resistor is $V_R = IR$ while the 
  voltage across the inductor is $V_L = L \frac{dI}{dt}$. By Kirchoff's voltage
  law, the total voltage differences, summed in an oriented manner, must be $0$.
  Therefore, $-V + V_R + V_L = 0$, or, in terms of $\frac{dI}{dt}$:
  \[
\frac{dI}{dt} = \frac{V - RI}{L}.
  \]
  We can express this $\Sys{RL}$-circuit as a differential system
  \[
\lens{\update{RL}}{\id} : \lens{\rr}{\rr} \fromto \lens{\rr^2 \times \rr^{\ast}}{\rr}
  \]
  where
  \[
\update{RL}\left( I, \begin{bmatrix} V \\ R \\ L \end{bmatrix} \right) \coloneqq
\frac{V - RI}{L}.
  \]
  We can then see that $I : \rr \to \rr$ defined by
  $$I(t) = \frac{V}{R}(1 - e^{-\frac{R}{L}t})$$
  gives a $\begin{bmatrix}V \\ R \\ L \end{bmatrix}$-trajectory for the
  $\Sys{RL}$ system. 
\end{example}

%---- Section ----%
\subsection{Steady states}

A steady state of a system is a state which does not change. Steady states are
important because they are guarantees of stability: a vase in a steady state is doing great, a heart in a steady state is in need of attention.

\subsubsection{Steady states in the deterministic doctrine}

A steady state in the deterministic doctrine is a state which transitions to itself.

\begin{definition}\label{def.steady_state_discrete}
 Let $$\Sys{S} = \lens{\update{S}}{\expose{S}} : \lens{\State{S}}{\State{S}}
 \leftrightarrows \lens{\In{S}}{\Out{S}}$$
 be a deterministic system. For input parameter $i \in \In{S}$ and output value
 $o \in \Out{S}$, an \emph{$\lens{i}{o}$-steady state} is a state $s \in \State{S}$
 such that
 \begin{align*}
   \update{S}(s, i) &= s, \\
   \expose{S}(s) &= o.
 \end{align*}
 We call the pair $\lens{i}{o}$ the \emph{chart} of the steady state.
\end{definition}

\begin{remark}
Its important to note that a steady state is relative to the input parameter
chosen. For example, in \cref{ex.transition_diagram_discrete}, the top left
state is steady for the input parameter $\Orange$ but not for
the input parameter $\Green$.
\end{remark}

Unlike with trajectories, a system need not necessarily have \emph{any} steady
states. For example, the $\Sys{Clock}$ has no steady states; it always keeps
ticking to the next hour. 

In the transition diagram of a finite deterministic system, steady states will be loops
that begin and end at the same node. Since the system is finite, we can arrange
the steady states by their chart into a $\In{S} \times \Out{S}$ matrix. For example, in
\cref{ex.transition_diagram_discrete}, we get the following $\{\Green,\, \Orange\} \times \{a,\, b\}$ matrix:
\begin{equation}\label{eqn.steady_state_matrix_transition_diagram_discrete}
\kbordermatrix{
  & \Green & \Orange\\
  a & \emptyset & \left\{ \begin{tikzcd}\LMO{a} \ar[loop left, orange, thick] \end{tikzcd} \right\}  \\
  b & \left\{ \begin{tikzcd}\LMO{b} \ar[loop left, dgreen, thick]\end{tikzcd} \right\}& \emptyset  \\
}
\end{equation}

This is a ``matrix of sets'', in that the entries are the actual sets of steady
states. If we just counted how many steady states there were for each
input-output pair, we would get this matrix:
\begin{equation}\label{eqn.steady_state_matrix_transition_diagram_discrete2}
\kbordermatrix{
  & \Green & \Orange\\
  a & 0 & 1  \\
  b & 1 & 0  \\
}
\end{equation}

In \cref{sec.steady_states_matrix_arithmetic}, we'll see that each wiring
diagram gives a formula for calculating the matrix of steady states of the
composite system from the matrices of steady states of the inner systems. 

\begin{exercise}\label{ex.find_steady_states1}
  What are the steady state matrices of systems $\Sys{S_1}$ and $\Sys{S_2}$ from
  \cref{ex.wiring_transition_diagrams}? What about the combined system $\Sys{S}$?
\end{exercise}

\paragraph{Steady-looking trajectories.}

The reason we are interested in steady states is that they are highly
predictable; if we know we are in a steady state, then we know we are always
going to get the same results. But it is possible for us to always get the same
outputs for the same input even though the internal state keeps changing. These
are special trajectories, and we call them \emph{steady-looking} trajectories.

\begin{definition}\label{def.steady_looking_trajectory_discrete}
  For $i \in \In{S}$ and $o \in \Out{S}$ of a system $\Sys{S}$, a \emph{$\lens{i}{o}$-steady
    looking trajectory} is a sequence of states $s : \nn \to \State{S}$ such
  that
  \begin{align*}
    \update{S}(s_t, i) &= s_{t + 1} \\
    \expose{S}(s_t) &= o
  \end{align*}
  for all $t \in \nn$. We call the pair $\lens{i}{o}$ the \emph{chart} of the
  steady-looking trajectory $s$.
\end{definition}

While the steady states of a wired together system can be calculated from those
of its components, this is not true for steady-looking trajectories.
Intuitively, this is because the internal systems can be exposing changing
outputs between eachother even while the eventual external output remains
unchanged.

\begin{exercise}\label{ex.steady_looking_trajectories}
Consider the wiring diagram:
\[
\Sys{S} \coloneqq 
\begin{tikzpicture}[oriented WD, bbx = .3cm, bby =.3cm, bb min width=.5cm, bb port length=2pt, bb port sep=1, every fit/.style={inner xsep=\bbx, inner ysep=\bby}
, baseline=(Outer.center)]
  \node[bb={1}{1}, fill=blue!10] (S1) {$\Sys{S_1}$};
  \node[bb={1}{1}, right= of S1, fill=blue!10] (S2) {$\Sys{S_2}$};

  \node[bb={1}{1}, fit={(S1) (S2)}] (Outer) {};

  \draw (Outer_in1) to (S1_in1);
  \draw (S1_out1) to (S2_in1);
  \draw (S2_out1) to (Outer_out1);
\end{tikzpicture}
\]


Find systems $\Sys{S_1}$ and $\Sys{S_2}$ and a steady-looking trajectory of the
wired system $\Sys{S}$ which is not steady-looking on the component systems.
\qedhere
\erase{
%% Here is my start at a solution
Here are two systems, $\Sys{S_1}$ and $\Sys{S_2}$ presented in terms of
transition diagrams. 

 First, let $\Set{Colors}
= \{\Red, \Blue\}$ and let $\Set{Bool} = \{\const{true}, \const{false}\}$. Here is our first system
$\Sys{S_1}$, which has interface $\lens{\Set{Bool}}{\Set{Colors}}$:
\begin{equation}\label{eqn.steady_looking_trajectories1}
\Sys{S_1} \coloneqq \begin{tikzpicture}[baseline=center]
	\node[draw] {
  \begin{tikzcd}[column sep=small]
    \LMO{\Blue} \ar[loop left, "\false"] \ar[rr, bend left, "\true"] \ar[rr, leftarrow, bend right, "\true"'] & & \LMO{\Red} \ar[loop right, "\false"]
  \end{tikzcd}
  };
\end{tikzpicture}
\end{equation}

Here is our second system, $\Sys{S_2}$, a $\lens{\Set{Colors}}{\Set{Bool}}$-system: 
\begin{equation}\label{eqn.steady_looking_trajectories2}
\Sys{S_2} \coloneqq \begin{tikzpicture}[baseline=center]
	\node[draw] {
  \begin{tikzcd}[column sep=small]
    \LMO{\true} \ar[loop left, red] \ar[loop right, blue]
  \end{tikzcd}
  };
\end{tikzpicture}
\end{equation}

When we wire these together, we get this system:
\begin{equation}\label{eqn.steady_looking_trajectories2}
\Sys{S} \coloneqq \begin{tikzpicture}[baseline=center]
	\node[draw] {
  \begin{tikzcd}[column sep=small]
    \LMO{\true} \ar[loop left, "\false"] \ar[rr, bend left, "\true"] \ar[rr, leftarrow, bend right, "\true"'] & & \LMO{\true} \ar[loop right, "\false"]
  \end{tikzcd}
  };
  As we can see, there is a $(\true, \true)$-steady-looking trajectory which hops between these two states, but always outputs $\true$ on an input of $\true$. However, the output of component system $\Sys{S_1}$ is alternating between $\Blue$ and $\Red$ as this trajectory progresses, so it is not steady-looking when restricted to $\Sys{S_1}$.
\end{tikzpicture}
\end{equation}
}%erased
  
\end{exercise}

\subsubsection{Steady states in the differential doctrine}

A steady state in the differential doctrine is a state which has no tendency to change.

\begin{definition}\label{def.steady_state_differential}
 Let $$\Sys{S} = \lens{\update{S}}{\expose{S}} : \lens{\State{S}}{\State{S}}
 \leftrightarrows \lens{\In{S}}{\Out{S}}$$
 be a differential system. For input parameter $i \in \In{S}$ and output value
 $o \in \Out{S}$, an \emph{$\lens{i}{o}$-steady state} is a state $s \in \State{S}$
 such that
 \begin{align*}
   \update{S}(s, i) &= 0, \\
   \expose{S}(s) &= o.
 \end{align*}
 We call the pair $\lens{i}{o}$ the \emph{chart} of the steady state.
\end{definition}

\begin{example}\label{ex.lotka_volterra_steady}
Let's see if there are any steady states of the Lotka-Volterra predator prey
model:
  \[
\update{LK}\left( \begin{bmatrix} r \\ f\end{bmatrix}, \begin{bmatrix}
    \const{b}_{\Sys{Rabbits}} \\ \const{d}_{\Sys{Foxes}} \end{bmatrix}\right)
  \coloneqq \begin{bmatrix}\const{b}_{\Sys{Rabbits}}
\cdot r - c_1 f r \\ c_2 r f - \const{d}_{\Sys{Foxes}}
\cdot f
 \end{bmatrix}
  \]
We are looking for a state $\begin{bmatrix} r \\ f \end{bmatrix}$ whose
update is $0$. That is, we want to solve the system of equations
\[
  \begin{cases}
    0 &= \const{b}_{\Sys{Rabbits}} \cdot r - c_1 r f \\
    0 &= c_2 r f - \const{d}_{\Sys{Foxes}} \cdot f
  \end{cases}
\]
If the parameters $\begin{bmatrix} \const{b}_{\Sys{Rabbits}} \\
  \const{d}_{\Sys{Foxes}} \end{bmatrix}$ are both zero, then any state is a
steady state. Clearly, $\begin{bmatrix} 0 \\ 0 \end{bmatrix}$ is a steady state
for any choice of parameters; this steady state could be called ``extinction''.
But if the populations and parameters are non-zero, then
\[
  \begin{bmatrix}
r \\ f 
\end{bmatrix}
= \begin{bmatrix}
  \frac{\const{d}_{\Sys{Foxes}}}{c_2} \\
  \frac{\const{b}_{\Sys{Rabbits}}}{c_1}
\end{bmatrix}
\]
is a steady state.
\end{example}

\begin{example}\label{ex.RL_circuit_steady}
  Recall the $\Sys{RL}$ circuit from \cref{ex.RL_circuit_trajectory}:
  \[
\update{RL}\left( I, \begin{bmatrix} V \\ R \\ L \end{bmatrix} \right) \coloneqq
\frac{V - RI}{L}.
  \]
  We can see that $I \coloneqq \frac{V}{R}$ is a steady state for this system
  given the parameters $V$ and $R$.
\end{example}


%---- Section ----%
\subsection{Periodic orbits}\label{sec.periodic_orbit_discrete}

Even if the behavior of a system isn't perfectly steady, it may continually
repeat. To a reasonable approximation, the position of the earth around the sun
follows a cycle that repeats every year. Using this as a paradigmatic example,
we call these behaviors that repeat \emph{periodic orbits}.

\subsubsection{Periodic orbits in the deterministic doctrine}

\begin{definition}[Periodic orbit] \label{def.periodic_orbit_discrete}
  A $\lens{p}{v}$-trajectory $s : \nn \to \State{S}$ is \emph{periodic} if there
  exists a time $t_0 \in \nn_{\geq1}$, called the \emph{period}, such that $s_{t_0} = s_0$. If the sequence of
  parameters $p : \nn \to \In{S}$ is also periodic with the same period (in that $p_{t_0} = p_0$ as well), then we say that $s$ has \emph{periodic parameters}.
  

\end{definition}

\begin{remark}
  Note that when we say that a periodic orbit has periodic parameters, we assume
  that they are periodic with the same period. This has important but subtle
  consequences for our theorems concerning the composition of behaviors in
  \cref{sec.representables}. We explain the difference between a periodic orbit
  and a periodic orbit with periodic parameters in a more precise manner
  in \cref{rmk.periodic_parameters_versus_not}.
\end{remark}

\begin{remark}
  Note that a steady state is a periodic orbit (with periodic parameters) that has
  a period of $1$. 
\end{remark}

\begin{exercise}
Describe a periodic orbit with period $1$ that does not have periodic
parameters; how are they different from steady states? Are there any of these in systems $\Sys{S_1}$ and $\Sys{S_2}$ of \cref{ex.wiring_transition_diagrams}?
\end{exercise}

\begin{example}
  The $\Sys{Clock}$ system is an exemplary periodic system with a period of 12.
  The $\Sys{ClockWithDisplay}$ of \cref{eqn.whole_clock_system_box} has period 24.
\end{example}

\begin{exercise}\label{ex.wiring_transition_diagrams_periodic_orbits}
  What are the periodic orbits in the systems $\Sys{S_1}$ and $\Sys{S_2}$ of
  \cref{ex.wiring_transition_diagrams} with periodic parameters, and what are their periods? What about
  the combined system $\Sys{S}$?
\end{exercise}

\begin{exercise}\label{ex.wiring_transition_diagrams_periodic_parameters}
  Can you think of any periodic orbits in $\Sys{S_1}$ and $\Sys{S_2}$ of
  \cref{ex.wiring_transition_diagrams} which
  don't have periodic parameters? 
\end{exercise}

\subsubsection{Periodic orbits in the differential doctrine}

\begin{definition}
A $p$-trajectory $s : \rr \to \State{S}$ for a differential system $\Sys{S}$ is
a \emph{periodic orbit} if there is a number $k$ such that
\[
s(t) = s(t + k)
\]
for all $t \in \rr$. We refer to $k$ as the \emph{period} of the orbit $s$. If
$p$ is periodic of period $k$ as well (that is, $p(t) = p(t + k)$ for all $t$),
then we say that $s$ has \emph{periodic parameters}.
\end{definition}

\begin{example}\label{ex.lotka_volterra_orbit}
  Recall the Lotka-Volterra predator prey model of \cref{sec.differential_system}:
  \[
  \begin{cases}
    
\frac{dr}{dt} =  a
\cdot r - b f r \\
\frac{df}{dt} = c r f - d f
  \end{cases}
  \]

  We may take the Jacobian of this system to get the ``community matrix''
  \[
J(r, f) = \begin{pmatrix} a - bf & -br \\ cf \\ cr - d  \end{pmatrix}.
  \]
We may investigate the stability of the steady states (from \cref{ex.lotka_volterra_steady}) by looking at the Jacobian.
In particular, we find that
\[
J\left( \frac{d}{c}, \frac{a}{b} \right) = \begin{pmatrix}0 & -\frac{bd}{c}\\ \frac{ac}{b}& 0 \end{pmatrix}
\]
whose eigenvalues are $\pm i \sqrt{ad}$. Since the eigenvalues are purely
imaginary and conjugate, this steady state is elliptic. Therefore the
trajectories around this steady state are ellipses, which is to say, periodic. 
 
\end{example}

\paragraph{Eventually Periodic Orbits}
A trajectory might not get back to where it started, but may still end up being
periodic. We call these trajectories \emph{eventually} periodic orbits, since
they eventually end up in a repeating cycle of states.

\begin{definition}[Eventually periodic orbit] \label{def.eventually_periodic_orbit_discrete}
A $\lens{p}{v}$-trajectory $s : \nn \to
  \State{S}$ is \emph{eventually periodic} if there are times $t_0 < t_1 \in
  \nn$ such that $s_{t_0 + t} = s_{t_1 + t}$ for all $t \in \nn$. If the sequence of
  parameters $p : \nn \to \In{S}$ is also eventually periodic with the same
  period (in that $p_{t_0 + t} = p_{t_1 + t}$ for all $t$), then we say that $s$ has \emph{eventually periodic parameters}.

The \emph{period} of an eventually periodic trajectory is
  the smallest difference $t_1 - t_0$ between times such that $s_{t_0} = s_{t_1}$.
\end{definition}

\begin{exercise}
  Formulate an analogous definition of eventually periodic orbit in the
  differential doctrine.
\end{exercise}








%---- Section ----%
\section{Behaviors of systems in the deterministic doctrine}\label{sec.behaviors}

In the previous
\cref{sec.trajectory_discrete,sec.steady_state_discrete,sec.periodic_orbit_discrete},
we saw a number of different kinds of behaviors of dynamical systems. Not only
were there a lot of definitions in those sections, each of those definitions had
slight variants (like periodic orbits versus periodic orbits with periodic
parameters, or steady states versus steady-looking trajectories). In this
section, we'll define a general notion of behavior and see that we can package
each of the above sorts of behavior into a single system\footnote{Or a family of
systems.} in its own right, one that \emph{represents} that sort of
behavior. The representative system of a certain kind of behavior behaves in
exactly that way, and does nothing else.

We will begin, for concreteness, with the deterministic doctrine. We will then
return in the next section to see how we may formulate a general definition
which also encompasses the differential doctrine.

We begin with a general definition of \emph{chart}. A behavior is defined
relative to its chart, which is the choice of parameters and the values of the
variables it will expose. For example, the chart of a steady state was a
parameter and an output value so that the state is steady with that parameter
and it exposes that output value.

\begin{definition}\label{def.chart_discrete}
  A \emph{chart} $\lens{f_{\flat}}{f} : \lens{A^-}{A^+} \rightrightarrows
  \lens{B^-}{B^+}$ in a cartesian category $\cat{C}$ is a pair of maps $f : A^+ \to B^+$ and $f_{\flat} : A^+
  \times A^- \to B^-$. Note that \emph{this is not a lens}.
\end{definition}

\begin{exercise}
\begin{enumerate}
	\item How many lenses are there $\lens{f^\sharp}{f}\colon\lens{\3}{\2}\fromto\lens{\4}{\3}$?
	\item How many charts are there $\lens{f^\flat}{f}\colon\lens{\3}{\2}\tto\lens{\4}{\3}$?
\qedhere
\end{enumerate}
\end{exercise}

\begin{exercise}
  \begin{enumerate}
    \item Show that a chart $\lens{\ord{1}}{\ord{1}} \rightrightarrows
      \lens{A^-}{A^+}$ is given by the data of a pair of elements $a^{-} \in A^{-}$
      and $a^+ \in A^+$. Compare this to the notion of chart used in the
      definition of steady state (\cref{def.steady_state_discrete}).
     \item Show that a chart $\lens{\ord{1}}{\nn} \rightrightarrows
       \lens{A^-}{A^+}$ is given by the data of a sequence $a^- : \nn \to A^+$
       and a sequence $a^+ : \nn \to A^+$. Compare this to the notion of chart
       used in the definition of trajectory (\cref{def.trajectory_discrete})
  \end{enumerate}
\end{exercise}




\begin{definition}[Behavior of deterministic systems] \label{def.behavior_discrete}
  Let $\Sys{T}$ and $\Sys{S}$ be deterministic systems. Given a chart of interfaces $\lens{f_{\flat}}{f}:
  \lens{\In{T}}{\Out{T}} \rightrightarrows \lens{\In{S}}{\Out{S}}$, a $\lens{f_{\flat}}{f}$-\emph{behavior of
  shape $\Sys{T}$ in $\Sys{S}$}, written
 $\phi :
\Sys{T} \to \Sys{S}$, is a function $\phi : \State{T} \to \State{S}$ sending
states of $\Sys{T}$ to states of $\Sys{S}$ which preserves the dynamics and
exposed variables by satisfying the following equations:
\begin{equation}\label{eqn.behavior_discrete}
\begin{aligned}
  \expose{S}(\phi(t)) &= f(\expose{T}(t)), \\
  \update{S}(\phi(t), f_{\flat}(\expose{T}(t), i)) &= \phi(\update{T}(t, i))
\end{aligned}
\end{equation}
for all $t \in \State{T}$ and $i \in \In{T}$. We say that $\lens{f_{\flat}}{f}$
is the chart of the behavior $\phi$.
\end{definition}

\begin{remark}
  If you prefer commutative diagrams to systems of equations, don't fret. We'll
  reinterpret \cref{eqn.behavior_discrete} in terms of commutative diagrams in \cref{sec.double_cat_arenas}
\end{remark}

\begin{remark}
  Suppose that we have transition diagrams for systems $\Sys{T}$ and $\Sys{S}$.
  Then a behavior of shape $\Sys{T}$ in $\Sys{S}$ will correspond to part of the
  transition diagram of $\Sys{S}$ which is shaped like the transition diagram of
  $\Sys{T}$. See the upcoming examples to see how this looks in practice.
\end{remark}

 Let's make this definition feel real with a few examples.

\begin{example}\label{ex.trajectory_as_behavior_discrete}
  Let $\Sys{Time}$ be the system $\lens{t\mapsto t+1}{\id}\colon\lens{\nn}{\nn}\fromto\lens{\{\fun{tick}\}}{\nn}$, i.e.\ with
\begin{itemize}
\item $\State{Time} \coloneqq \nn$,
\item $\Out{Time} \coloneqq \nn$,
\item $\In{Time} \coloneqq \{\fun{tick}\}$,
\item $\expose{Time} = \id$,
\item $\update{Time}(t, \ast) = t + 1$.
\end{itemize}

As a transition diagram, $\Sys{Time}$ looks like this:

\[
\begin{tikzpicture}
	\node[draw] {
  \begin{tikzcd}[column sep=30pt]
    \LMO{0} \ar[r, thick, "\fun{tick}"] & \LMO{1} \ar[r,  thick, "\fun{tick}"] & \LMO{2} \ar[r, thick, "\fun{tick}"] & \LMO{3} \ar[r, thick, "\fun{tick}"] & \LMO{4} \ar[r, thick, "\fun{tick}"] & \cdots
  \end{tikzcd}
  };
\end{tikzpicture}
\]

Let's see what a behavior of shape $\Sys{Time}$ in an arbitrary system $\Sys{S}$ will be. We will expect the
shape of $\Sys{Time}$ to appear in the transition diagram of $\Sys{S}$, like
this:
\[
\begin{tikzpicture}[baseline=(Center)]
  \coordinate (Center) at (0,0);
	\node[draw] (Diag1) {
  \begin{tikzcd}[column sep=small]
    \LMO{0} \ar[r, red, thick] & \LMO{1} \ar[r, red, thick] & \LMO{2} \ar[r, red, thick] & \LMO{3} \ar[r, red, thick] & \LMO{4} \ar[r, red, thick] & \cdots
  \end{tikzcd}
  };
	\node[draw, right = of Diag1] (Diag2){
\begin{tikzcd}
\vdots                      & \vdots                      & \vdots                      & \vdots                      & \vdots \\
\LMO{} \arrow[r] \arrow[u]  & \LMO{} \arrow[r, red] \arrow[u]  & \LMO{} \arrow[r, red] \arrow[u]  & \LMO{} \arrow[u] \arrow[ru, red] &        \\
\LMO{} \arrow[r] \arrow[u]  & \LMO{} \arrow[r] \arrow[u, red]  & \LMO{} \arrow[u] \arrow[ru] &                             &        \\
\LMO{} \arrow[r] \arrow[u]  & \LMO{} \arrow[ru] \arrow[u, red] &                             &                             &        \\
\LMO{} \arrow[u] \arrow[ru, red] &                             &                             &                             &       
\end{tikzcd}
  };
  \draw[->, shorten <=2pt, shorten >=2pt] (Diag1) to (Diag2);
\end{tikzpicture}
\]

First, we need to know what a chart $\lens{f_{\flat}}{f} :
\lens{\In{Time}}{\Out{Time}} \rightrightarrows \lens{\In{S}}{\Out{S}}$ is like. Since
$\Out{Time} = \nn$ and $\In{Time} \cong \ord{1}$, this means $f : \nn \to \Out{S}$
is a sequence of outputs, and $f_{\flat} : \nn \times \ord{1} \to \In{S}$ is a
sequence of input parameters. We might as well instead call $f$ our sequence of
exposed values $v$, and $f_{\flat}$ our sequence of input parameters $p$, so
that we have a chart $\lens{p}{v} : \lens{\ord{1}}{\nn}
\rightrightarrows \lens{\In{S}}{\Out{S}}$.

Now, let's see what a $\lens{p}{v}$-behavior $\gamma : \Sys{Time} \to \Sys{S}$ is. It
is a function $\gamma
: \State{Time} \to 
\State{S}$ satsifying some properties. But $\State{Time} = \nn$, so $\gamma :
\nn \to \State{S}$ is a sequence of states in $\Sys{S}$. Now,
\cref{eqn.behavior_discrete} becomes the equations:
\begin{align*}
  \expose{S}(\gamma(t)) &= v(t) \\
  \update{S}(\gamma(t), p(t)) &= \gamma(t + 1).
\end{align*}
which are exactly the equations defining a $\lens{p}{v}$-trajectory from \cref{def.trajectory_discrete}!

\end{example}

\begin{example}\label{ex.steady_state_as_behavior_discrete}
  Consider the simple system $\Sys{Fix}$ with:
  \begin{itemize}
  \item $\State{Fix} = \{\ast\}$.
  \item $\Out{Fix} = \{\ast\}$.
  \item $\In{Fix} = \{\ast\}.$
  \item $\expose{Fix} = \id$.
  \item $\update{Fix}(\ast, \ast) = \ast$.
  \end{itemize}

  As a transition diagram, this looks like:
\[
\begin{tikzpicture}
	\node[draw] {
  \begin{tikzcd}[column sep=small]
    \LMO{\ast} \ar[loop left, "\ast"]
  \end{tikzcd}
  };
\end{tikzpicture}
\]

  A behavior $s : \Sys{Fix} \to \Sys{S}$ in an arbitrary system $\Sys{S}$ should be a loop of this shape within the transition
  diagram of $\Sys{S}$: a steady state.
\[
\begin{tikzpicture}
	\node[draw] (Diag1) {
  \begin{tikzcd}[column sep=small]
    \LMO{\ast} \ar[loop left, red, "\ast"]
  \end{tikzcd}
  };
  \node[draw, right = of Diag1]  (Diag2) {
  \begin{tikzcd}[column sep=small]
    \LMO{} \ar[out=120, in=90, loop] \ar[in=210, out=250, loop] \ar[rr, bend left = 10] \ar[rr, leftarrow,bend right= 10] \ar[ddr, leftarrow,  bend right= 10] &  & \LMO{} \ar[loop right] \ar[ddl, bend left= 10] \ar[ddl,leftarrow, bend right= 10]  \\
    & & \\
    & \LMO{} \ar[out=300, in=240, loop, red] & 
  \end{tikzcd}
  };
  \draw[->] (Diag1) to (Diag2); 
\end{tikzpicture}
\]

  Let's check that this works. First, we need to know what a chart $\lens{f_{\flat}}{f} :
  \lens{\In{Fix}}{\Out{Fix}} \rightrightarrows \lens{\In{S}}{\Out{S}}$ is. Since
  $\Out{Fix} = \In{Fix} = \{\ast\}$, we have that $f : \{\ast\} \to \Out{S}$ is
  simply an output value of $\Sys{S}$ and $f_{\flat} : \{\ast\}\times\{\ast\} \to
  \In{S}$ is simply an input parameter. Therefore, we might as well write $o$
  for $f$ and $i$ for $f_{\flat}$, to see that a chart $\lens{i}{o} :
  \lens{\{\ast\}}{\{\ast\}} \rightrightarrows \lens{\In{S}}{\Out{S}}$ is a pair
  of elements $i \in \In{S}$ and $o \in \Out{S}$.

  Now, let's see what a $\lens{i}{o}$-behavior $s : \Sys{Fix} \to \Sys{S}$ is. It is
  a function $s : \State{Fix} \to \State{S}$ satisfying a few properties. But
  $\State{S} = \{\ast\}$ so $s : \{\ast\} \to \State{S}$ is a single state of
  $\Sys{S}$. Then, \cref{eqn.behavior_discrete} becomes the equations
  \begin{align*}
    \expose{S}(s) &= o\\
    \update{S}(s, i) &= s
  \end{align*}
  which are precisely the equations defining a $\lens{i}{o}$-steady state from \cref{def.steady_state_discrete}.
  
\end{example}

\begin{example}\label{ex.periodic_orbit_as_behavior}
  Let $0 < n \in \nn$ be a positive natural number, and consider the system $\Sys{Clock_n}$
  having:
  \begin{itemize}
    \item $\State{Clock_n} = \ord{n} = \{1,\ldots, n\}$.
    \item $\Out{Clock_n} = \ord{n}$.
    \item $\In{Clock_n} = \{\ast\}$.
    \item $\expose{Clock_n} = \id$.
    \item $\update{Clock_n}(t, \ast) = \begin{cases} t + 1 &\mbox{if $t < n$}
        \\ 1 &\mbox{if $t = n$}  \end{cases}$.
  \end{itemize}

This is the clock with $n$ hours. Our example system $\Sys{Clock}$ from
\cref{ex.clock_system} is $\Sys{Clock}_{12}$, a clock with 12 hours. Here's what
$\Sys{Clock}_4$ looks like as a transition diagram:
\[
\begin{tikzpicture}
	\node[draw] {
  \begin{tikzcd}[sep=small]
    \LMO{1} \ar[rr, bend left, "\ast"] & & \LMO{2} \ar[dd, bend left, "\ast"] \\
    & & \\
\LMO{3} \ar[rr, leftarrow, bend right, "\ast"'] \ar[uu, bend left, "\ast"] & & \LMO{4}
  \end{tikzcd}
  };
\end{tikzpicture}
\]

A behavior $\gamma : \Sys{Clock_n} \to \Sys{S}$ should be a cycle like this in the
transition diagram of $\Sys{S}$: a periodic orbit. We can see the $\Sys{Clock_\4}$-behavior inside the system shown right:
\[
\begin{tikzpicture}
	\node[draw] (Diag1) {
  \begin{tikzcd}[sep=small]
    \LMO{1} \ar[rr, bend left, red, "\ast"] & & \LMO{2} \ar[dd, bend left, red, "\ast"] \\
    & & \\
\LMO{3} \ar[rr, leftarrow, bend right, red, "\ast"'] \ar[uu, bend left, red, "\ast"] & & \LMO{4}
  \end{tikzcd}
  };
  \node[draw, right= of Diag1] (Diag2) {\begin{tikzcd}[sep=small]
    \LMO{} \ar[loop left] \ar[rr, bend left, red] \ar[dd, leftarrow, bend right, red] &  & \LMO{} \ar[loop right] \ar[dd, bend left, red ]\\
    & & \\
    \LMO{} \ar[loop left] \ar[rr, leftarrow, bend left] \ar[rr, leftarrow, bend right, red] & & \LMO{}
  \end{tikzcd}
};
\draw[->] (Diag1) to (Diag2);
\end{tikzpicture}
\]

Let's check that this works. First, we need to know what a chart
$\lens{f_{\flat}}{f} : \lens{\In{Clock_n}}{\Out{Clock_n}} \rightrightarrows
\lens{\In{S}}{\Out{S}}$ is. Since $\Out{Clock_n} = \ord{n}$ and $\In{Clock_n} =
\{\ast\}$, $f : \ord{n} \to \Out{S}$ is a sequence of $n$ exposed values of
$\Sys{S}$ while $f_{\flat} : \ord{n} \times \{\ast\} \to \In{S}$ is a sequence
of $n$ parameters. Therefore, we might as well write $v$ for $f$ and $p$ for
$f_{\flat}$ to find that a chart $\lens{p}{v} : \lens{\{\ast\}}{\ord{n}}
\rightrightarrows \lens{\In{S}}{\Out{S}}$ consists of an $n$-length sequence of
parameters and an $n$-length sequence of exposed values. 

A $\lens{p}{v}$-behavior $\gamma : \Sys{Clock_n} \to \Sys{S}$, then, is a function
$\gamma : \State{Clock_n} \to \State{S}$ satisfying a few properties. Since
$\State{Clock_n} = \ord{n}$, $\gamma : \ord{n} \to \State{S}$ is a $n$-length
sequence of states of $S$, and \cref{eqn.behavior_discrete} become the equations
\begin{align*}
  \expose{S}(\gamma(t)) &= v(t) \\
  \update{S}(\gamma(t), p(t)) &= \begin{cases} \gamma(t + 1) &\mbox{if $t < n$} \\
\gamma(1) &\mbox{if $t = n$} \end{cases}.
\end{align*}
As we can see, this determines a sequence of length $n$ of states of $\Sys{S}$
which repeats when it gets to the end. In other words, this is a periodic orbit
with periodic parameters as in \cref{def.periodic_orbit_discrete}!
\end{example}

If we have a certain kind of behavior in mind, and we find a system $\Sys{T}$ so
that behaviors of shape $\Sys{T}$ are precisely this kind of behavior, then we
say that $\Sys{T}$ \emph{represents} that behavior. For example, we have just
seen that:
\begin{itemize}
  \item The system $\Sys{Time} = \lens{\_ + 1}{\id} : \lens{\nn}{\nn} \leftrightarrows
    \lens{\{\ast\}}{\nn}$ represents trajectories.
  \item The system $\Sys{Fix} = \lens{\pi_2}{\id} : \lens{\{\ast\}}{\{\ast\}}
    \leftrightarrows \lens{\{\ast\}}{\{\ast\}}$ represents steady states.
  \item The systems $\Sys{Clock_n} = \lens{\_ + { 1 \mbox{ mod } n }}{\id} :
    \lens{\ord{n}}{\ord{n}} \leftrightarrows \lens{\{\ast\}}{\ord{n}}$
    represents periodic orbits with periodic parameters whose period divides $n$.
\end{itemize}

Note that there is always a particularly simple behavior on a system: the
identity behaviors $\id : \State{T} \to \State{T}$. This says that every system
behaves as itself. In particular, $\Sys{Time}$ has a trajectory behavior given by
$\id : \Sys{Time} \to \Sys{Time}$ (namely, the trajectory $s_t = t$), and $\Sys{Fix}$ has a steady state behavior given by $\id : \Sys{Fix} \to \Sys{Fix}$ (namely,
the steady state $\ast$), etc. We refer to the identity behavior of $\Sys{T}$ as the \emph{generic}
behavior of type $\Sys{T}$.

\begin{exercise}\label{ex.find_representatives_discrete}
  Find a representative system for the following kinds of behavior.
  \begin{enumerate}
    \item An eventually periodic orbit (see \cref{def.eventually_periodic_orbit_discrete}) that takes $n$ steps to get to a period
      of size $m$.
    \item A steady-looking trajectory (see \cref{def.steady_looking_trajectory_discrete}).
    \item A periodic orbit of period at most $n$ whose parameters aren't
      necessarily also periodic (see \cref{def.periodic_orbit_discrete}).
    \item An trajectory which yields the same output value at every
      $10^{\text{th}}$ step.
     \qedhere
  \end{enumerate}
\end{exercise}

\begin{remark}\label{rmk.periodic_parameters_versus_not}
As \cref{ex.find_representatives_discrete} shows, the difference between a
periodic orbit and a periodic orbit with periodic parameters can be surmised
precisely by noting that they are represented by systems with different
interfaces. The dynamics of the systems are the same, but the interfaces (and
accordingly, the exposed variable) are different; this explains how the
difference between a periodic orbit and a periodic orbit with periodic
parameters is all in the chart.
\end{remark}

\begin{exercise}\label{ex.represents_what_discrete}
  What kind of behaviors do the following systems represent? First, figure out
  what kind of charts they have, and then see what a behavior with a given chart
  is. Describe in your own words.

  \begin{enumerate}
  \item The system $\Sys{Plus}$ with:
    \begin{itemize}
    \item $\State{Plus} = \nn$.
    \item $\Out{Plus} = \nn$.
    \item $\In{Plus} = \nn$.
    \item $\expose{Plus} = \id$.
    \item $\update{Plus}(t, j) = t + j$.
    \end{itemize}
    %% A chart is a sequence of output values and an a $\nn \times \nn$ grid of
    %% input values. A behavior is a sequence of states with those output values
    %% where state $s_t$ updates on parameter $(t, j)$ to $s_{t + j}$.
  \item The system $\Sys{T_n}$ with:
    \begin{itemize}
    \item $\State{T_n} = \nn$.
    \item $\Out{T_n} = \{0, \ldots, n - 1\}$.
    \item $\In{T_n} = \{\ast\}$.
    \item $\expose{T_n}(t) = t \mod n$.
    \item $\update{T_n}(t, \ast) = t + 1$.
    \end{itemize}
    %% Its a trajectory that looks periodic, and has periodic parameters.
  \item The system $\Sys{XOR}$ with:
    \begin{itemize}
    \item $\State{XOR} = \Set{Bool} = \{\true,\, \false\}$.
    \item $\Out{XOR} = \Set{Bool}$.
    \item $\In{XOR} = \Set{Bool}$.
    \item $\expose{XOR} = \id$.
    \item \[\begin{aligned}
        &\update{XOR}(\true, \true) &= \false, \\
        &\update{XOR}(\false, \true) &= \true, \\
        &\update{XOR}(\true, \false) &= \true, \\
        &\update{XOR}(\false, \false) &= \false.
          \end{aligned}\]
    \end{itemize}
  \item The system $\Sys{List_C}$ for a set of \emph{choices} $C$ with:
    \begin{itemize}
      \item $\State{List_C} = \Set{List}_C$ is the set of lists of elements in
        $C$.
      \item $\Out{List_C} = \Set{List}_C$.
      \item $\In{List_C} = C$.
      \item $\expose{List_C} = \id$.
      \item $\update{List_C}(\ell, c) = c::\ell$, that is, we update a list by
        appending the character $c \in C$ to the start.
    \end{itemize}
  \end{enumerate}
\end{exercise}

While every system $\Sys{T}$ represents some kind of behavior --- just take the kind of
behavior to be exactly described by behaviors $\Sys{T} \to \Sys{S}$ --- we are
most interested in those simple systems $\Sys{T}$ whose behavior we can fully
understand. 


We have written a behavior of shape $\Sys{T}$ in $\Sys{S}$ with an arrow $\phi :
\Sys{T} \to \Sys{S}$. This suggests that there is a category with deterministic
systems as its objects and behaviors as its morphisms; and there is!

\begin{definition}\label{def.category_of_charts}
  The category $\Cat{Chart}_{\cat{C}}$ of charts in $\cat{C}$ has
  \begin{itemize}
    \item Objects the \emph{arenas} $\lens{A^-}{A^+}$, pairs of objects in $\cat{C}$.
    \item Maps the \emph{charts} $\lens{f_{\flat}}{f} : \lens{A^-}{A^+}
      \tto \lens{B^-}{B^+}$.
    \item Composition the composite of a chart $\lens{f_{\flat}}{f} : \lens{A^-}{A^+}
      \tto \lens{B^-}{B^+}$ with a chart $\lens{g_{\flat}}{g} : \lens{B^-}{B^+}
      \tto \lens{C^-}{C^+}$ is 
$$\lens{g_{\flat}}{g} \circ \lens{f_{\flat}}{f} \coloneqq \lens{(a^+, a^-)
  \mapsto g_{\flat}(f(a^+), f_{\flat}(a^+, a^-))}{g \circ f}.$$
     \item The identity chart is $\lens{\pi_2}{\id} : \lens{A^-}{A^+}
       \rightrightarrows \lens{A^-}{A^+}$.
  \end{itemize}
\end{definition}
\begin{exercise}
  Check that $\Cat{Chart}_{\cat{C}}$ is indeed a category. That is,
  \begin{enumerate}
    \item For charts $\lens{f_{\flat}}{f} : \lens{A^-}{A^+}
      \tto \lens{B^-}{B^+}$, $\lens{g_{\flat}}{g} : \lens{B^-}{B^+}
      \tto \lens{C^-}{C^+}$, and $\lens{h_{\flat}}{h} : \lens{C^-}{C^+}
      \tto \lens{D^-}{D^+}$, show that 
$$\lens{h_{\flat}}{h} \circ \left( \lens{g_{\flat}}{g} \circ \lens{f_{\flat}}{f}
\right) = \left( \lens{h_{\flat}}{h} \circ \lens{g_{\flat}}{g} \right) \circ \lens{f_{\flat}}{f}.$$
    \item For a chart $\lens{f_{\flat}}{f} : \lens{A^-}{A^+}
      \tto \lens{B^-}{B^+}$, show that 
$$ \lens{\pi_2}{\id} \circ \lens{f_{\flat}}{f} = \lens{f_{\flat}}{f} =
\lens{f_{\flat}}{f} \circ \lens{\pi_2}{\id}.$$
  \end{enumerate}
\end{exercise}

\begin{exercise}\label{ex.special_charts}
  What are the charts of the following forms in simpler terms? 
  \begin{enumerate}
    \item $\lens{f_{\flat}}{f} : \lens{\ord{1}}{\ord{1}} \tto \lens{A^-}{A^+}$.
    \item $\lens{f_{\flat}}{f} : \lens{A^-}{A^+} \tto \lens{\ord{1}}{\ord{1}}$.
    \item $\lens{f_{\flat}}{f} : \lens{\ord{1}}{A^+} \tto \lens{B^-}{B^+}$.
   \end{enumerate}
\end{exercise}

\begin{proposition}\label{prop.category_of_systems_discrete}
  There is a category $\Cat{Sys}$ with deterministic systems as its objects and
  where a map $\Sys{T} \to \Sys{S}$ is a pair consisting of a chart
  $\lens{f_{\flat}}{f} : \lens{\In{T}}{\Out{T}} \rightrightarrows
  \lens{\In{S}}{\Out{S}}$ and a $\lens{f_{\flat}}{f}$-behavior $\phi : \Sys{T}
  \to \Sys{S}$. Composition is given by composing both the charts and the
  functions on states, and identities are given by the generic behaviors: the identity chart with the
  identity function $\id : \State{T} \to \State{T}$.
\end{proposition}
\begin{proof}
  We just need to check that the composite $\psi \circ \phi$ of two behaviors $\phi : \Sys{T} \to
  \Sys{S}$ and $\psi : \Sys{S} \to \Sys{U}$ with charts $\lens{f_{\flat}}{f} : \lens{\In{T}}{\Out{T}} \rightrightarrows
  \lens{\In{S}}{\Out{S}}$ and $\lens{g_{\flat}}{g} : \lens{\In{S}}{\Out{S}} \rightrightarrows
  \lens{\In{U}}{\Out{U}}$ is a behavior with chart $\lens{g_{\flat}}{g} \circ
  \lens{f_{\flat}}{f}$. That is, we need to check that
  \cref{eqn.behavior_discrete} is satisfied for $\psi \circ \phi$. We can do
  this using the fact that it is satisfied for both $\psi$ and $\phi$.
  \begin{align*}
    \expose{U}(\psi(\phi(t))) &= \psi(\expose{S}(\phi(t))) \\
                              &= \psi(\phi(\expose{T}(t))).\\
  \end{align*}
  \begin{align*}
    \update{U}(\psi(\phi(t)), &g_{\flat}(f(\expose{T}(t)), f_{\flat}(\expose{T}(t), i)))  \\
                              &=\update{U}(\psi(\phi(t)), g_{\flat}(\expose{S}(\phi(t)), f_{\flat}(\expose{T}(t), i))) \\
                              &= \psi(\update{S}(\phi(t), f_{\flat}(\expose{T}(t), i))) \\
    &= \psi(\phi(\update{T}(t, i))).
    &\qedhere
  \end{align*}
\end{proof}

There are two different ways to understand what composition of behaviors means:
one based on post-composition, and the other based on pre-composition.
\begin{itemize}
  \item We see that any behavior $\Sys{S} \to \Sys{U}$ gives a way of turning
    $\Sys{T}$-shaped behaviors in $\Sys{S}$ to $\Sys{T}$-shaped behaviors in $\Sys{U}$.
  \item We see that any behavior $\Sys{T} \to \Sys{S}$ gives a way of turning
    $\Sys{S}$-shaped behaviors in $\Sys{U}$ into $\Sys{T}$-shaped behaviors in $\Sys{U}$.
\end{itemize}

\begin{example}\label{ex.steady_state_gives_trajectory_discrete}
  Any steady state $s$ can be seen as a particularly simple trajectory: $s_t =
  s$ for all $t$. We have seen in
  \cref{ex.steady_state_as_behavior}
  that steady states are $\Sys{Fix}$-shaped behaviors. We can use composition of behaviors to
  understand how steady states give rise to trajectories.

  The generic steady state $\ast$ of $\Sys{Fix}$ (that is, the identity behavior of
  $\Sys{Fix}$) generates a trajectory $s : \nn \to \State{Fix}$ with input
  parameters $p_t = \ast$ and $s_t = \ast$. This gives us a behavior $s :
  \Sys{Time} \to \Sys{Fix}$.
  
  Now, for every steady state $\gamma : \Sys{Fix} \to \Sys{S}$, we may compose
  to get a trajectory $\gamma \circ s : \Sys{Time} \to \Sys{S}$.
\end{example}

\begin{exercise}\label{ex.behaviors_as_change_of_kind_discrete}
Adapt the argument of \cref{ex.steady_state_gives_trajectory_discrete} to show
that
\begin{enumerate}
  \item Any eventually periodic orbit gives rise to a trajectory.
  \item If $n$ divides $m$, then any orbit of period at most $n$ gives rise to
    an orbit of period of most $m$.
\qedhere
\end{enumerate}
\end{exercise}


\paragraph{Isomorphisms of Systems}

Now that we have a category of systems and behaviors, category theory supplies
us with a definition of isomorphism for systems. 
\begin{definition}\label{def.isomorphism_of_systems_discrete}
  An \emph{isomorphism} of a system $\Sys{T}$ with a system $\Sys{S}$ is a
  a behavior $\phi : \Sys{T} \to \Sys{S}$ for which there is another behavior
  $\phi\inv : \Sys{S} \to \Sys{T}$ such that $\phi \circ \phi \inv =
  \id_{\Sys{S}}$ and $\phi \inv \circ \phi = \id_{\Sys{T}}$.
\end{definition}

Let's see that this is indeed a good notion of sameness for systems.
\begin{proposition}\label{prop.isomorphism_of_systems_discrete}
  A behavior $\phi : \Sys{T} \to \Sys{S}$ is an isomorphism if and only if the
  following conditions hold:
\begin{enumerate}
  \item The map $\phi : \State{T} \to \State{S}$ is an isomorphism of sets --- a
    bijection.
  \item The chart $\lens{f_{\flat}}{f} : \lens{\In{T}}{\Out{T}}
    \rightrightarrows \lens{\In{S}}{\Out{S}}$ of $\phi$ is an isomorphism in
    $\Cat{Chart}_{\smset}$. That is, $f : \Out{T} \to \Out{S}$ is a bijection
    and there is a bijection $f_{\flat}' : \In{T} \to \Out{T}$ such that
    $f_{\flat} = f_{\flat}' \circ \pi_2$.
\end{enumerate}
\end{proposition}
\begin{proof}
  Since composition in the category of systems and behaviors is given by
  composition of the underlying charts and maps, $\phi$ is an isomorphism of
  systems if and only if its action on states is a bijection and its chart is an
  isomorphism in the category of charts. It just remains to see that our
  description of isomorphism of charts is accurate, which we leave to \cref{ex.isomorphism_in_category_of_charts}.
\end{proof}

\begin{exercise}\label{ex.isomorphism_in_category_of_charts}
  Show that a chart $\lens{f_{\flat}}{f} : \lens{A^-}{A^+}
    \rightrightarrows \lens{B^-}{B^+}$ is an isomorphism if and only if $f$ is
    an isomorphisms and there is an isomorphism $f_{\flat}' : A^- \to B^-$ such
    that $f_{\flat} = f_{\flat}' \circ \pi_2$. 
\end{exercise}



\subsection{Simulations}

While we will often be interested in behaviors of systems that change the
interface in the sense of having non-trivial charts, we will also be interested
in behaviors of systems that do not changed the exposed variables at all. These
behaviors play a very different role in the theory of dynamical systems than
behaviors like trajectories and steady states. Because they don't change
observable behavior (since they have identity chart), they say more about how we
\emph{model} the observable behavior than what that behavior is itself. For that
reason, we will call behaviors with identity chart \emph{simulations}.

\begin{definition}\label{def.cat_of_systems_discrete}
  Let $\lens{I}{O}$ be an arena. The category
$$\Cat{Sys}\lens{I}{O}$$
of deterministic $\lens{I}{O}$-systems has as objects the systems
$\lens{\update{S}}{\expose{S}} : \lens{\State{S}}{\State{S}} \fromto
\lens{I}{O}$ with interface $\lens{I}{O}$ and as maps the \emph{simulations} $\phi :
\Sys{T} \to \Sys{S}$, those behaviors whose chart is the identity chart on $\lens{I}{O}$.
\end{definition}

\begin{example}\label{ex.simulation_discrete}
  Recall the $\lens{\{\Green, \Orange\}}{\{a, b\}}$-system $\Sys{S}$ from
  \cref{ex.transition_diagram_discrete}:
 \[
\begin{tikzpicture}
	\node[draw] {
  \begin{tikzcd}[column sep=small]
  	\LMOO{1}{a}\ar[rr, dgreen, thick, bend left]\ar[loop left, thick, orange]&&
  	\LMOO{2}{b}\ar[ll, thick, orange, bend left]\ar[dl, bend left, thick, dgreen]\\&
  	\LMOO{3}{b} \ar[ul, thick, orange, bend left] \ar[loop left, thick, dgreen]
  \end{tikzcd}
  };
\end{tikzpicture}
\]
 If we had built this system as a model of some relationships between input
 colors and output letters we were seeing in the wild, then we have made this
 system a bit redundant. If the output is $a$, and we feed it $\Green$, the
 output will be $b$; if we feed it $\Orange$, the output will be $a$. Similarly,
 if the output is $b$ --- no matter which of states $2$ or $3$ the system
 is actually in --- and we feed it $\Green$, the output will again be $b$, and
 if we feed it $\Orange$, the output will be $a$. And there really isn't much
 else going on in the system.

 We can package this observation into a behavior in $\Cat{Sys}\lens{\{\Green,
   \Orange\}}{\{a, b\}}$. Let $\Sys{U}$ be the system
 \[
\begin{tikzpicture}
	\node[draw] {
  \begin{tikzcd}[column sep=small]
  	\LMOO{1}{a}\ar[rr, dgreen, thick, bend left]\ar[loop left, thick, orange]&&
  	\LMOO{2}{b}\ar[ll, thick, orange, bend left]\ar[loop left, thick, dgreen]
  \end{tikzcd}
  };
\end{tikzpicture}
\]
We can give a behavior $q : \Sys{S} \to \Sys{U}$ with identity chart as follows
defined by
\begin{align*}
  q(1) &= 1 \\
  q(2) &= 2 \\
  q(3) &= 2
\end{align*}
We can check, by cases, that this is indeed a behavior. That it is a behavior in
$\Cat{Sys}\lens{\{\Green, \Orange\}}{\{a, b\}}$ means that it doesn't change the
observable behavior.
\end{example}

\cref{ex.simulation_discrete} also gives us an example of an important relation between
systems: \emph{bisimulation}. We saw what it means for two systems to be
isomorphic: it means they have isomorphic states and the same dynamics and
output relative to those isomorphisms. But this is sometimes too strong a notion
of sameness for systems; we want to know when two systems \emph{look the same}
on the outside. 

Let's see what this notion looks like for deterministic systems; then we will
describe it in a doctrinal way.
\begin{definition}\label{def.bisimulation_discrete}
  In the deterministic doctrine, a \emph{bisimulation} $\sim$ between $\lens{I}{O}$-systems $\Sys{S}$ and
  $\Sys{U}$ is a relation $\sim : \State{S} \times \State{U} \to \{\true,
  \false\}$ between states of these systems such that $s \sim u$ only when $s$
  and $u$ have related dynamics:
  \begin{align*}
    s \sim u &\mbox{ implies } \expose{S}(s) = \expose{U}(u) \\
    s \sim u &\mbox{ implies } \update{S}(s, i) \sim \update{U}(u, i) \mbox{ for all $i \in I$}.
  \end{align*}
  If $\sim$ is a bisimulation, we say that $s$ and $u$ are \emph{bisimilar} when
  $s \sim u$.
  
  A bisimulation $\sim$ is said to be \emph{total} if every $s \in \State{S}$ is
  bisimilar to some $u \in \State{U}$ and vice-versa.
\end{definition}

Bisimilarity is a strong relation between states of systems. For deterministic
systems, this implies that they act the same on any input.
\begin{proposition}\label{prop.bisimulation_discrete}
  Let $\Sys{S}$ and $\Sys{U}$ be deterministic $\lens{I}{O}$-systems, and let $\sim$ be a
  bisimulation between them. If $s_0 \sim u_0$ are bisimilar, then they
  induce the same transformation on streams of inputs into streams of outputs: 
$$\fun{transform}^{s_0}_{\Sys{S}} = \fun{transform}^{u_0}_{\Sys{U}}.$$
\end{proposition}
\begin{proof}
Let $i : \nn \to I$ be a stream of inputs. Let $s : \nn \to \State{S}$ be the
stream of states generated by $s_0$ and similarly, let $u : \nn \to \State{U}$
be the stream of states generated by $u_0$. 

We first show that $s_n \sim u_n$ for all $n$. Our base case holds by
hypothesis; now suppose that $s_n \sim u_n$ seeking $s_{n+1} \sim u_{n + 1}$.
Well,
\[s_{n +1} = \update{S}(s_n, i_n) \sim \update{U}(u_n, i_n) = u_{n+1}\]
because $\sim$ is a bisimulation.

Finally, 
\[
\fun{transform}_{\Sys{S}}(i)_n = \expose{S}(s_n) = \expose{U}(u_n) = \fun{transform}_{\Sys{U}}(i)_n
\]
because $s_n \sim u_n$. 
\end{proof}

We can talk about bisimilar states without reference to the particular
bisimulation between the systems they are a part of because, as it turns out,
being bisimilar is independent of the particular bisimulation. To see this, we
need to introduce an interesting system: the system of trees.

\begin{definition}
  Let $\lens{I}{O}$ be an arena in the deterministic doctrine. An
  $\lens{I}{O}$-tree $\tau$ (or a $O$-labeled, $I$-branching tree) consists of:
  \begin{itemize}
    \item A \emph{root} $\fun{root}(\tau) \in O$.
    \item For each parameter $i \in I$, a \emph{child} tree $\fun{child}(\tau,
      i)$.
  \end{itemize}
\end{definition}


\begin{definition}
Let $\lens{I}{O}$ be an arena in the deterministic doctrine. The $\lens{I}{O}$-system
$\Sys{Tree}_{\littlelens{I}{O}}$ of $\lens{I}{O}$-trees has
\begin{itemize}
\item $\State{Tree}$ is the set of $\lens{I}{O}$-trees.
\item Each tree exposes its root: $\expose{Tree}(\tau) = \fun{root}(\tau)$.
\item The system updates by following a tree down the $i^{\text{th}}$ branch: $\update{Tree}(\tau, i) = \fun{child}(\tau, i)$
\end{itemize}
\end{definition}

We can think of an $\lens{I}{O}$-tree as a stream of possible outputs of an
$\lens{I}{O}$-system. In the current state, we see the root of the tree. When we
transition to the next state with parameter $i$, we will see the rest of the
output. This observation suggests a universal characterization of the system of $\lens{I}{O}$-trees.

\begin{proposition}
  The $\lens{I}{O}$-system $\Sys{Tree}_{\lens{I}{O}}$ of $\lens{I}{O}$-trees is
  terminal in the category of $\lens{I}{O}$-systems.
\end{proposition}
\begin{proof}
 We will show that there is a unique simulation $!_{\Sys{S}} : \Sys{S} \to
 \Sys{Tree}_{\lens{I}{O}}$ for any $\lens{I}{O}$-system $\Sys{S}$.
 For any $s \in \State{S}$, we will define a tree $!_{\Sys{S}}(s)$ of outputs
 visible from the state $s$. We define this as
 follows:
 \begin{itemize}
   \item The root of $!_{\Sys{S}}(s)$ is the variable exposed by $\Sys{S}$: \[\fun{root}(!_{\Sys{S}}(s)) = \expose{S}(s).\]
   \item The $i^{\text{th}}$ child of $!_{\Sys{S}}(s)$ is the tree of outputs
     visible from the next state $\update{S}(s, i)$: \[\fun{child}(!_{\Sys{S}}(s), i) = !_{\Sys{S}}(\update{S}(s, i)).\]
 \end{itemize}
 Now, we can show that this is a simulation \emph{and} that it is the unique
 such similation by noticing that this definition is precisely what is required
 to satisfy the defining laws of a simulation.
\end{proof}

Now we can express the idea that bisimilarity of states is independent of any
particular bisimulation between their systems with the following theorem.

\begin{theorem}
  Let $\Sys{S}$ and $\Sys{U}$ be $\lens{I}{O}$-systems. A state $s \in
  \State{S}$ is bisimilar to a state $u \in \State{U}$ for some bisimulation
  $\sim$ between $\Sys{S}$ and $\Sys{U}$ if and only if $!_{\Sys{S}}(s) = !_{\Sys{U}}(u)$.
\end{theorem}
\begin{proof}
 First, let's show that if $s$ is bisimilar to $u$ via some bisimulation $\sim$,
 then $!_{\Sys{S}}(s) = !_{\Sys{U}}(u)$. Now, to show that two trees are equal,
 we need to show that they have equal roots and equal children.
 \begin{itemize}
   \item The root of $!_{\Sys{S}}(s)$ is $\expose{S}(s)$, and the root of
     $!_{\Sys{U}}(u)$ is $\expose{U}(u)$. But since $s \sim u$ by hypothesis,
     these are equal.
   \item Similarly, the $i^{\text{th}}$ child of $!_{\Sys{S}}(s)$ is
    $!_{\Sys{S}}(\update{S}(s, i))$, while the $i^{\text{th}}$ child of
    $!_{\Sys{U}}(u)$ is $!_{\Sys{U}}( \update{U}(u, i) )$. But since $\sim$ is a
    bisimulation, we have that $\update{S}(s, i) \sim \update{U}(u, i)$, and so
    by the same argument we are giving, we will find that
    $!_{\Sys{S}}(\update{S}(s, i)) = !_{\Sys{U}}(\update{U}(u,
    i))$.\footnote{This style of proof is called \emph{proof by co-induction}.
      Where induction assumes a base case and then breaks apart the next step
      into a smaller step, co-induction shows that the proof can always be
      continued in a manner which covers all possible options.}
 \end{itemize}

On the other hand, suppose that $!_{\Sys{S}}(s) = !_{\Sys{U}}(u)$. We now need
to defined a bisimulation $\sim$ between $\Sys{S}$ and $\Sys{U}$ for which $s
\sim u$. For any sequence of inputs $i : \ord{n} \to I$, we can evolve a system
in state $s$ by the entire sequence $i$ to yield a state $\update{S}^{\ast}(s,
i)$ in the following way:
\begin{itemize}
  \item If $n = 0$, then $\update{S}^{\ast}(s, i) = s$.
  \item For $n + 1$, then $\update{S}^{\ast}(s, i) =
    \update{S}(\update{S}^{\ast}(s, i|_{\ord{n}}), i_{n + 1})$.
\end{itemize}
We may then define $\sim$ in the following way.

\begin{center}
$x \sim y$ if and only if there is an $n \in \nn$ and $i : \ord{n} \to I$ with
$x = \update{S}^{\ast}(s, i)$ and $y = \update{U}^{\ast}(u, i)$.
\end{center}
It remains to show that this is a bisimulation.

For any
$\lens{I}{O}$-tree $\tau$ and any $n$-length sequence $i : \ord{n} \to I$ of
parameter (for any $n \in \nn$), we can follow the path $i$ through the tree
$\tau$ to get a new tree $\fun{subtree}(\tau, i)$:
\begin{itemize}
  \item If $n = 0$, then $\fun{subtree}(\tau, i) = \tau$.
  \item For $n + 1$, $\fun{subtree}(\tau, i) = \fun{child}(\fun{subtree}(\tau,
    i|_{\ord{n}}))$ is the $i^{\text{th}}$ child of the tree found by following
    $i$ for the first $n$ steps.
\end{itemize}
Note that $\!_{\Sys{S}}(\update{S}^{\ast}(s, i)) = \fun{subtree}(!_{\Sys{S}}(s),
i)$ by a quick inductive argument. Now we can show that $\sim$ is a bisimulation.

\begin{itemize}
  \item Suppose that $x \sim y$, seeking to show that $\expose{S}(x) =
    \expose{U}(y)$. By hypothesis, $x = \update{S}^{\ast}(s, i)$ and $y =
    \update{U}^{\ast}(u, i)$. But then
    \begin{align*}
      \expose{S}(x) &= \fun{root}(!_{\Sys{S}}(x)) \\
                    &= \fun{root}(\fun{subtree}(!_{\Sys{S}}(s), i)) \\
      &= \fun{root}(\fun{subtree}(!_{\Sys{U}}(u), i)) \\
                    &= \fun{root}(!_{\Sys{U}}(y)) \\
      &= \expose{U}(y).
    \end{align*}
  \item Suppose that $x \sim y$, seeking to show that
    $\update{S}(x, j) \sim \update{U}(y, j)$. By hypothesis, $x =
    \update{S}^{\ast}(s, i)$ and same for $y = \update{U}^{\ast}(u,i)$. Then
    letting $i' : \ord{n+1} \to \nn$ be defined by $i'_{n+1} = j$ and $i'_k =
    i_k$ otherwise, we see that $\update{S}(x, j) = \update{S}^{\ast}(s, i')$
    and $\update{U}(y, j) = \update{U}^{\ast}(y, i')$, so that by definition
    they are related by $\sim$.
\end{itemize}
\end{proof}

%---- Section ----%
\section{Dealing with two kinds of composition: Double categories}

In this section, we will introduce the notion of \emph{double category} to help
us deal with our two kinds of composition: the composition of systems, and the
composition of behaviors. By revealing that \cref{def.behavior_discrete} can be
expressed as a square in a double category of \emph{arenas}, we will find a
generalization of this definition of behavior which applies to the differential
doctrine as well. It is at this point that we will introduce the formal
definition of a \emph{dynamical system doctrine}.

\begin{definition}\label{def.double_category}
  A \emph{double category} $\cat{D}$ has:
  \begin{itemize}
    \item A class $\const{ob}\cat{D}$ of \emph{objects}.
    \item A \emph{horizontal} category $h \cat{D}$ whose objects are those of
      $\cat{D}$. We call the maps in $h\cat{D}$ the \emph{horizontal} maps of $\cat{D}$.
    \item A \emph{vertical} category $v \cat{D}$ whose objects are those of
      $\cat{D}$. We call the maps in $v\cat{D}$ the \emph{vertical} maps of $\cat{D}$.
    \item For vertical maps $j : A \to B$ and $k : C \to D$ and horizontal maps
      $f : A \to C$ and $g : B \to D$, there is a set of
      \emph{squares}
      \[
        \begin{tikzcd}[sep=small]
          A \ar[dd, "j"'] \ar[rr, "f"] & & B \ar[dd, "k"] \\
           & \alpha & \\
          C \ar[rr, "g"'] & & D
        \end{tikzcd}
      \]
    \item Squares can be composed both horizontally and vertically:
        \[
        \begin{tikzcd}[sep=small]
          A_1 \ar[dd, "j"'] \ar[rr, "f_1"] & & A_2 \ar[dd, "k"]  \ar[rr, "f_2"]&
          & A_3 \ar[dd, "\ell"]\\
           & \alpha &  & \beta &\\
          B_1 \ar[rr, "g_1"'] & & B_2 \ar[rr, "f_2"'] & & B_3
        \end{tikzcd} \mapsto
        \begin{tikzcd}[sep=small]
          A_1 \ar[dd, "j"'] \ar[rr, "f_2 f_1"] & & A_3 \ar[dd, "\ell"] \\
           & \alpha \mid \beta & \\
          B_1 \ar[rr, "g_2 g_1"'] & & B_3
        \end{tikzcd}
        \]
        \[
        \begin{tikzcd}[sep=small]
          A_1 \ar[dd, "j_1"'] \ar[rr, "f"] & & A_2 \ar[dd, "k_1"]  \\
           & \alpha &  \\
           B_1 \ar[dd, "j_1"'] \ar[rr, "g"] & & B_2 \ar[dd, "k_2"]\\
           & \beta & \\
           C_1 \ar[rr, "h"']& & C_2
        \end{tikzcd} \mapsto
        \begin{tikzcd}[sep=small]
          A_1 \ar[dd, "j_2 j_1"'] \ar[rr, "f"] & & A_3 \ar[dd, "k_2 k_1"] \\
           & \frac{\alpha}{\beta} & \\
          C_1 \ar[rr, "h"'] & & B_3
        \end{tikzcd}
        \]
      \item For every vertical map $j : A \to B$, there is an identity square
        \[
        \begin{tikzcd}[sep=small]
          A \ar[dd, "j"'] \ar[rr, equals] & & A \ar[dd, "j"] \\
           & j & \\
          B \ar[rr, equals] & & B
        \end{tikzcd}
        \]
        which we will also refer to as $j$, for convenience. Similarly, for
        every horizontal map $f : A \to B$, there is an identity square
        \[
        \begin{tikzcd}[sep=small]
          A \ar[rr, "f"] \ar[dd, equals] & & A \ar[dd, equals] \\
           & f & \\
          B \ar[rr, "f"'] & & B
        \end{tikzcd}
        \]
        which we will also refer to as $f$, for convenience.
      \item Vertical and horizontal composition is associative and unital, and
        the \emph{interchange law} holds. That is:
        \begin{itemize}
          \item For horizontally composable squares $\alpha$, $\beta$, and
            $\gamma$, $$(\alpha \mid \beta) \mid \gamma = \alpha \mid (\beta \mid
            \gamma).$$
          \item For vertically composable squares $\alpha$, $\beta$, and
            $\gamma$,%
\footnote{If you're seeing this and feeling worried about fractions, you can put your mind at ease; we promise there will be no fractions. Only squares next to squares.}
\[
\begin{array}{c}
\left(  \begin{array}{c}
\alpha \\ \hline
\beta
\end{array}\right)\\ \hline
\gamma 
\end{array}
= 
\begin{array}{c}
  \alpha \\ \hline
\left(  \begin{array}{c}
\beta \\ \hline
\gamma
\end{array}\right)
\end{array}
\]
\item For a square $\alpha$ with left and right vertical edges $j$ and $k$
  respectively, 
$$j \mid \alpha = \alpha = \alpha | k.$$
\item For a square $\alpha$ with top and bottom horizontal edges $f$ and $g$,
$$\frac{f}{\alpha} = \alpha = \frac{\alpha}{g}.%
\footnote{There aren't any fractions here either.}$$
\item For four appropriately composable squares $\alpha$, $\beta$, $\gamma$, and
  $\delta$, the following interchange law holds:
$$\frac{\alpha \mid \beta}{\gamma \mid \delta} = \left.
  \frac{\alpha}{\beta} \middle|  \frac{\gamma}{\delta} \right. .$$
        \end{itemize}
  \end{itemize}
\end{definition}

Phew, that was quite the definition! The reason the definition of a double
category is so much more involved than the definition of a category is that
there is more than twice the data: there's the vertical category and the
horizontal category, but also how they interact through the squares. 

\begin{remark}
  Just like we notate the identity square on a vertical morphism $j$ by $j$ and
  the identity square on a horizontal morphism $f$ by $f$, we will often denote
  composition of vertical morphisms by $\frac{f}{g}$ and of horizontal morphisms
  by $j \mid k$. This notation agrees with the composition of their respective
  identity squares, and will be much more pleasant to look at when writing equations.
\end{remark}

Let's see two important examples of double categories.

\subsection{The double category of arenas in the deterministic doctrine}

Finally, we are ready to meet the double category of arenas in the deterministic
doctrine. This is where our dynamical
systems live, and where they behave.

\begin{definition}\label{def.double_category_of_arenas_discrete}
  The \emph{double category of arenas} in the deterministic doctrine is a double
  category which has:
  \begin{itemize}
  \item Its objects are the \emph{arenas}, pairs of sets $\lens{A^-}{A^+}$.
  \item Its horizontal category is the category of charts.
  \item Its vertical category is the category of lenses.
  \item There is a square of the following form
    \begin{equation}\label{eqn.dbl_cat_arena_square}
      \begin{tikzcd}
        \lens{A^-}{A^+} \ar[r, shift left, "\lens{f_{\flat}}{f}"] \ar[r, shift
        right] \ar[d, shift right, "\lens{j^{\sharp}}{j}"'] \ar[d, shift left,
        leftarrow] & \lens{B^-}{B^+} \ar[d, shift left, leftarrow,
        "\lens{k^{\sharp}}{k}"] \ar[d, shift right]\\
        \lens{C^-}{C^+} \ar[r, shift right, "\lens{g_{\flat}}{g}"'] \ar[r,
        shift left] & \lens{D^-}{D^+}
      \end{tikzcd}
    \end{equation}
    if and only if the following equations hold:
    \begin{align}\label{eqn.dbl_cat_arena_square_commuting}
      g(j(a^+)) &= k(f(a^+)) \\
      k^{\sharp}(f(a^+), g_{\flat}(j(a^+), c^-)) &= f_{\flat}(a^+, j^{\sharp}(a^+, c^-)) 
    \end{align}
    for all $a^+ \in A^+$ and $c^- \in C^-$.
  \end{itemize}
\end{definition}

It's not obvious from this definition that we actually get a double category
with this definition. It's not even clear that we have defined a way to compose
the squares vertically and horizontally.

It turns out we don't need to know anything else to know that we can compose
these squares, at least in principle. This is because there is at most one
square filling any two charts and two lenses that line up as in
\cref{eqn.dbl_cat_arena_square}; to compose these squares just means that if we
have two such squares lining up, the defining equations
\cref{eqn.dbl_cat_arena_square_commuting} hold also for the appropriate
composites. We call double categories with this property \emph{thin}.

\begin{definition}\label{def.thin_double_cat}
  A double category is \emph{thin} if there is at most one square of any
  signature.
\end{definition}

So long as composition is well defined in a thin double category, the laws of
associativity and interchange for square composition come for free; there is at
most one square of the appropriate signature, so any two you can write down are
already equal. We do still have to show that composition is well defined in this
way, which we'll do a bit more generally in \cref{sec.groth_double_construction}

\begin{remark}\label{rmk.double_category_direction}
While the definition of double category we gave treated both horizontal and
vertical directions the same, we will often want to see a square
\[
  \begin{tikzcd}[sep=small]
    A \ar[dd, "j"'] \ar[rr, "f"] & & B \ar[dd, "k"] \\
    & \alpha & \\
    C \ar[rr, "g"'] & & D
  \end{tikzcd}
\]
as a sort of map $\alpha : j \to k$ from its left to its right side, or a map
$\alpha : f \to g$ from its top to its bottom side. For example, the systems
themselves are certain lenses (vertical maps), and the behaviors are squares
between them. On the other hand, we can also see a square as a way of wiring
together charts. 
\end{remark}

\begin{example}\label{ex.understanding_squares_in_double_cat_of_arenas}
  A square
  \begin{equation}\label{eqn.understanding_squares_in_double_cat_of_arenas}
    \begin{tikzcd}
      \lens{A^-}{A^+} \ar[r, shift left, "\lens{f_{\flat}}{f}"] \ar[r, shift
      right] \ar[d, shift right, "\lens{j^{\sharp}}{j}"'] \ar[d, shift left,
      leftarrow] & \lens{B^-}{B^+} \ar[d, shift left, leftarrow,
      "\lens{k^{\sharp}}{k}"] \ar[d, shift right]\\
      \lens{C^-}{C^+} \ar[r, shift right, "\lens{g^{\sharp}}{g}"'] \ar[r, shift
      left] & \lens{D^-}{D^+}
    \end{tikzcd}
    \end{equation}
  can be seen as a chart between lenses, that is, two charts which are
  compatible according to the wiring pattern the lenses describe. For example, consider a square of the
  following form where $\lens{w^{\sharp}}{w}$ is a wiring diagram:
  \[
    \begin{tikzcd}
      \lens{\ord{1}}{\ord{1}} \ar[r, shift left, "\lens{b^-}{b^+}"] \ar[r, shift
      right] \ar[d, shift right, equals] \ar[d, shift left, equals] &
      \lens{B^-}{B^+} \ar[d, shift left, leftarrow,
      "\lens{w^{\sharp}}{w}"] \ar[d, shift right]\\
      \lens{\ord{1}}{\ord{1}} \ar[r, shift right, "\lens{d^-}{d^+}"'] \ar[r,
      shift left] & \lens{D^-}{D^+}
    \end{tikzcd}
  \]
  By \cref{ex.special_charts}, we know that the charts in this diagram are pairs
  of elements $\lens{b^-}{b^+}$ and $\lens{d^-}{d^+}$ in the arenas
  $\lens{B^-}{B^+}$ and $\lens{D^-}{D^+}$ respectively. The square then says
  that $\lens{d^-}{d^+}$ are the values you would get if you passed
  $\lens{b^-}{b^+}$ along the wires in the wiring diagram
  $\lens{w^{\sharp}}{w}$.
\end{example}

Taking for granted that the double category of arenas is indeed a double
category, what does this mean for systems? Well, behaviors are particular
squares in the double category of arenas.

\begin{proposition}\label{prop.behavior_as_square_of_arenas_discrete}
  Let $\Sys{T}$ and $\Sys{S}$ be dynamical systems. A behavior $\phi : \Sys{T}
  \to \Sys{S}$ is equivalently a square of the following form in the double
  category of arenas:
  \begin{equation}\label{eqn.behavior_as_square_of_arenas_discrete}
    \begin{tikzcd}
      \lens{\State{T}}{\State{T}} \ar[r, shift left, "\lens{\phi \circ
        \pi_2}{\phi}"] \ar[r, shift right] \ar[d, shift right,
      "\lens{\update{T}}{\expose{T}}"'] \ar[d, shift left, leftarrow] &
      \lens{\State{S}}{\State{S}} \ar[d, shift left, leftarrow,
      "\lens{\update{S}}{\expose{S}}"] \ar[d, shift right]\\
      \lens{\In{T}}{\Out{T}} \ar[r, shift right, "\lens{f^{\sharp}}{f}"'] \ar[r,
      shift left] & \lens{\In{S}}{\Out{S}}
    \end{tikzcd}
  \end{equation}
\end{proposition}
\begin{proof}
  This is a simple matter of checking the definitions against eachother. The
  defining equations of \cref{def.double_category_of_arenas_discrete} specialize
  to the defining equations of \cref{def.behavior_discrete}.
\end{proof}

This re-expression of the notion of behavior in terms of the double category of
arenas will let us generalize from the deterministic doctrine to other
doctrines.




\subsection{The double category of categories, profunctors, and functors}

Now we come to the primordial double category: the double category of
categories, \emph{profunctors}, and functors. This is an important double
category because it is in some sense the setting in which all category theory
takes place. Before we describe this double category, let's define the notion of
profunctor and their category.
\begin{definition}
  A \emph{profunctor} $P : \cat{A} \tickar \cat{B}$ is a functor $P : \cat{A}\op
  \times \cat{B} \to \smset$. Given objects $A \in \cat{A}$ and $B \in \cat{B}$,
  we write an element $p \in P(A, B)$ as $p : A \tickar B$. 

In terms of this,
  the functoriality of $P$ can be seen as letting us compose $p : A \tickar B$
  on the left and right by $f : A' \to A$ and $g : B \to B'$ to get $fpg : A'
  \tickar B'$. In other words, we can interpret a diagram of this form
$$A' \xto{f} A \xtickar{p} B \xto{g} B'$$
as an element of $P(A', B')$.
\end{definition}

If we call maps $f : A' \to A$ in a category $\cat{A}$ \emph{homomorphisms}
because they go between objects of the same form, we could call elements $p : A
\xtickar B$ --- that is, $p \in P(A, B)$ --- as \emph{heteromorphisms}, maps
going between objects of different forms.

We can't necessarily compose these heteromorphisms, which we can see right away
from their signature: for $p : A \tickar B$, there is always an object of $\cat{A}$ on
the left and an object of $\cat{B}$ on the right, so we'll never be able to line
two of them up. However, if we have another
profunctor $Q : \cat{B} \tickar \cat{C}$ --- another notion of heteromorphism
--- then we can ``compose'' heteromorphisms $A \xtickar{p} B$ in $P$ with $B
\xtickar{q} C$ in $Q$ to get a heteromorphism $A \xtickar{p} B \xtickar{q} C$ in a new
profunctor $P \odot Q : \cat{A} \tickar \cat{C}$.

\begin{definition}
  The composite $P \odot Q$ of a profunctor $P : \cat{A} \tickar \cat{B}$ with a
  profunctor $Q : \cat{B} \tickar \cat{C}$ is defined to be the following quotient:
\begin{equation}\label{eqn.profunctor_composition}
  (P \odot Q)(A, C) := \frac{\sum_{B \in \cat{B}}P(A, B) \times Q(B, C)}{(pf, q) \sim (p, fq)}
\end{equation}
We write an element $[(p, q)] \in (P \odot Q)(A, C)$ as $A \xtickar{p} B
\xtickar{q} C$, so that the relation we quotient by says that 
$$A \xtickar{p} B \xto{f} B' \xtickar{q} C$$
has a unique interpretation as an element of $P \odot Q$. 

The identity profunctor $\cat{A} : \cat{A} \tickar \cat{A}$ is the hom-functor
sending $A$ and $A'$ to the set $\cat{A}(A, A')$ of maps $A \to A'$. 
\end{definition}

We can see that composition of profunctors is associative (up to isomorphism)
because the objects of $P \odot (Q \odot R)$ and $(P \odot Q) \odot R$ can both
be written as 
$$A \xtickar{p} B \xtickar{q} C \xtickar{r} D.$$
The reason the hom profunctor $\cat{A} : \cat{A} \tickar \cat{A}$ is the
identity profunctor is because the elements of $\cat{A} \odot P$ would be
written as
$$A' \xto{f} A \xtickar{p} B$$
but by the functoriality of $P$, this is already an element of $P(A', B)$, 
which is to say more precisely that every equivalence class $[(f, p)] \in
(\cat{A} \odot P)(A', B)$ is equally presented as $[(\id_{A'}, fp)]$. 

\begin{exercise}\label{ex.identity_profunctor}
  Let $P : \cat{A} \tickar \cat{B}$ be a profunctor.
  \begin{enumerate}
    \item Show that there is a natural transformation $\cat{A} \odot P \to P$
      given by the naturality of $P$ on the left.
    \item Show that there is a natural transformation $P \odot \cat{B} \to P$
      given by the naturality of $P$ on the right.
    \item Show that both of these natural transformations are isomorphisms.
  \end{enumerate}
\end{exercise}

\begin{example}
  A profunctor $\ord{1} \tickar \cat{A}$ is the same thing as a functor $\cat{A}
  \to \smset$, and a profunctor $\cat{A} \tickar \ord{1}$ is the same thing as a
  functor $\cat{A}\op \to \smset$. Profunctors are therefore intimately related
  with presheaves.
\end{example}

Now, we are ready to put functors and profunctors together into a double category.
\begin{definition}\label{def.double_cat_of_cats}
The double category $\Cat{Cat}$ of categories, profunctors, and functors
has
\begin{itemize}
\item Objects the categories.
\item Horizontal category the category of categories and profunctors.
\item Vertical category the category of categories and functors between them.
\item A square 
\[
\begin{tikzcd}[sep = small]
  \cat{A} \ar[rr, tick, "P"] \ar[dd, "F"'] & & \cat{B} \ar[dd, "G"] \\
  & \alpha & \\
\cat{C} \ar[rr, tick, "Q"'] & & \cat{D}
\end{tikzcd}
\]
Is a natural transformation $\alpha : P \rightarrow Q(F, G)$, where $Q(F, G)$ is
the profunctor $\cat{A}\op \times \cat{B} \xto{F\op \times G} \cat{C} \times
\cat{D} \xto{Q} \smset$. For $p : A \tickar B$, we have $\alpha(p) : FA \tickar
GB$, and naturality says that $\alpha(fpg) = (Ff)\alpha(p)(Gg)$.
\item Vertical composition of squares is given by composing the natural transformations.
\item Given squares $\alpha : P_1 \rightarrow Q_1(F_1, F_2)$ and $\beta : P_2
  \to Q_2(F_2, F_3)$, we define their horizontal composite $\alpha \mid \beta :
  P_1 \cdot P_2 \to (Q_1 \cdot Q_2)(F_1, F_3)$ by 
$$(\alpha \mid \beta)(A_1 \xtickar{p_1} A_2 \xtickar{p_2} A_3) = F_1 A_1
\xtickar{\alpha(p_1)} F_2 A_2 \xtickar{\beta(p_2)} F_3 A_3$$
and checking that this descends correctly to the quotient.
\end{itemize}
\end{definition}

\begin{remark}
  We are using ``$\Cat{Cat}$'' to refer to the category of categories and
  functors and to the double category of categories, profunctors, and functors.
  The one we mean will be clear from context, and the category of categories and
  functors is the vertical category of the double category of categories.
\end{remark}

\begin{remark}\label{rmk.up_to_coherent_iso}
  We omit full proofs of associativity and unitality for profunctor composition
  because they are best done with the \emph{coend calculus}, and this would take
  us quite far afield. 
  %%
  %% :CUSTOM-ID:cite-me.profunctors
  %%

  However, we will note that there is always a unique coherent isomorphism
  between any two sequences of profunctors which would be equal if unity and
  associativity held on the nose. We will do an example, since the general
  principle is always the same.

  Consider $P : \cat{A}
  \tickar \cat{B}$ and $Q : \cat{B} \tickar \cat{C}$. We will give the canonical
  isomorphism $(\cat{A} \odot P) \odot (Q \odot \cat{C}) \xto{\sim} P \odot (\cat{B}
  \odot (\cat{B} \odot Q))$.

  First, we begin with an isomorphism $(\cat{A} \odot P) \odot (Q \odot \cat{C})
  \xto{\sim} P \odot Q$ and then an isomorphism $P \odot Q \xto{\sim} P \odot (\cat{B}
  \odot (\cat{B} \odot Q))$. The first will be given by naturality, composition and
  re-associating; the second by inserting appropriate identities and re-associating.

  An element of $(\cat{A} \odot P) \odot (Q \odot \cat{C})(A, C)$ is an equivalence
  class $[((f, p), (q, g))]$. We may therefore use the naturality of $P$ and $Q$
  to give the class $[(f \cdot p, q \cdot g)]\in (P \odot Q)(A, C)$. It is
  routine to check that this is indeed an isomorphism. It is
  hopefully clear how to do this in general.

  Now, we go the other way. An element of $(P \odot Q)(A, C)$ is an equivalence
  class $[(p, q)]$. We may then insert identities to give the class $[(p, (\id,
  (\id, q)))] \in P \odot (\cat{B}
  \odot (\cat{B} \odot Q))(A, C)$.

A crucial point about canonical isomorphisms constructed in this manner is that
they compose: the composite of a canonical isomorphism is the canonical
isomorphism of that signature. 
\end{remark}

\begin{exercise}
  Describe the canonical isomorphisms between the following composites of
  profunctors. First, flatten them out by removing all hom profunctors using
  naturality; then expand them again by inserting identities. Let $P : \cat{A} \tickar \cat{B}$, $Q : \cat{B} \tickar \cat{C}$,
  and $R : \cat{C} \tickar \cat{D}$.
  \begin{enumerate}
    \item $(P \odot \cat{B}) \odot (\cat{B} \odot Q) \xto{\sim} \cat{A} \odot ( (P
      \odot \cat{B}) \odot \cat{C} )$.
    \item $P \odot ((Q \odot \cat{C}) \odot (\cat{C} \odot R)) \xto{\sim} ((P
      \odot Q) \odot \cat{C}) \odot (R \odot \cat{D})$.
  \end{enumerate}
\end{exercise}

\begin{remark}\label{rmk.up_to_coherent_iso_symbol}
  We will often need equalities between squares in the double category
  $\cat{Cat}$ whose boundaries are not precisely equal, but which are
  canonically isomorphic. The coming
  \cref{lem.transformation_as_square_horizontal} is an example of this common
  scenario.

  It would clutter already intricate proofs to keep track of the canonical
  isomorphisms which are being introduced and cancelled at each step. For this
  reason, we'll introduce notation for ``equal up to canonical isomorphism on
  the boundary''. We will write 
$$\alpha \coheq \beta$$
to mean that although $\alpha$ and $\beta$ have different boundaries, these
boundaries are canonically isomorphic and whenever they are made to be the same
by any canonical isomorphism (pre- or post-composing $\alpha$ and $\beta$ as
necessary), the resulting squares will be honestly equal. We will see our first
example in \cref{lem.transformation_as_square_horizontal}.
\end{remark}

Before we move on from the double category $\Cat{Cat}$, let's record an
important relationship between its squares (natural transformations between
profunctors) and natural transformations between functors. We will show that
natural transformations are the same thing as squares in $\Cat{Cat}$ whose top
and bottom sides are hom profunctors.
\begin{proposition}\label{prop.transformation_as_square_in_cat}
  Let $F$ and $G : \cat{A} \to \cat{B}$ be functors. Then there is a
  (natural) bijection
  \[
\{\mbox{Natural transformations }F \Rightarrow G\} \cong \left\{\mbox{Squares} \begin{tikzcd}[sep = tiny]
  \cat{A} \ar[rr, tick, equals] \ar[dd, "F"'] & & \cat{A} \ar[dd, "G"] \\
  & \alpha & \\
\cat{B} \ar[rr, tick, equals] & & \cat{B}
\end{tikzcd} \right\}
  \]
  given by sending the natural transformation $\alpha : F \Rightarrow G$ to the
  transformation $\bar{\alpha} : \cat{A}(X, Y) \Rightarrow \cat{B}(FX, GY)$ that
  sends any $f : X \to Y$ the diagonal $\alpha_f$ of the naturality square:
  \[
    \begin{tikzcd}
      FX \ar[r, "\alpha_X"] \ar[d, "Ff"'] \ar[dr, "\alpha_f"]& GX \ar[d, "Gf"] \\
      FY \ar[r, "\alpha_Y"] & GY
    \end{tikzcd}
  \]
\end{proposition}
\begin{proof}
First, let's check that the transformation $\bar{\alpha}(f) = \alpha_f$ is
natural. If $x : X' \to X$ and $y : Y \to Y'$, then we can form the following
commutative diagram of naturality squares:
\[
    \begin{tikzcd}
      FX' \ar[r, "\alpha_{X'}"] \ar[d, "Fx"'] & GX' \ar[d, "Gx"] \\
      FX \ar[r, "\alpha_X"] \ar[d, "Ff"'] \ar[dr, "\alpha_f"]& GX \ar[d, "Gf"] \\
      FY \ar[r, "\alpha_{Y}"] \ar[d, "Fy"'] & GY \ar[d, "Gy"] \\
      FY' \ar[r, "\alpha_{Y'}"] & GY'
    \end{tikzcd}
\]
The diagonal of the outer square is by definition $\bar{\alpha}(x\then f\then y)$, but we
can see from the commutativity of the diagram that it equals $Fx \then \alpha_f
\then Gy$.

It remains to show that any natural transformation $\beta : \cat{A}(X, Y)
\Rightarrow \cat{B}(FX, GY)$ arises uniquely as $\bar{\alpha}$ for a natural
transformation $\alpha : F \Rightarrow G$. Given such a $\beta$, define $\alpha_X
:= \beta(\id_{X})$. We need to prove the naturality of $\alpha$ by showing that
any solid square
\[
    \begin{tikzcd}
      FX \ar[r, "\alpha_X"] \ar[d, "Ff"'] \ar[dr, dashed, "\beta(f)"] & GX \ar[d, "Gf"] \\
      FY \ar[r, "\alpha_Y"] & GY
    \end{tikzcd}
\]
commutes. But note that if we put in the dashed $\beta(f)$, we can see that both
triangles commute by the naturality of $\beta(f)$:
$$\beta(f) = \beta(\id_X \then f) = \beta(\id_X) \then Gf = \alpha_X \then Gf.$$
$$\beta(f) = \beta(f \then \id_Y) = Ff \then \beta(\id_Y) = Ff \then \alpha_Y.$$
This also shows that $\beta(f) = \bar{\alpha}(f)$, which completes the proof.
\end{proof}

There are two ways to compose natural transformations: vertically, and
horizontally. The above bijection respects both of these compositions. In the
following lemmas we take the notation from \cref{prop.transformation_as_square_in_cat}.

\begin{lemma}\label{lem.transformation_as_square_horizontal}
  Let $\alpha : F \Rightarrow G$ and $\beta : G \Rightarrow H$. Then 
$$\overline{\alpha \then \beta} \coheq \bar{\alpha} \mid \bar{\beta}.$$
\end{lemma}
\begin{proof}
Here we are using the symbol ``$\coheq$'' from \cref{rmk.up_to_coherent_iso_symbol} for the first time; this is
because the two sides do not have equal signature, only isomorphic signature. To
correctly compare them, we must conjugate by the appropriate isomorphisms. Here,
with signature included, is the actual equality we will prove:
\[
\begin{tikzcd}[sep = tiny]
  \cat{A} \ar[r, tick, equals] \ar[dd, equals] & \cat{A} \ar[r, tick, equals] &
  \cat{A} \ar[dd, equals] \\
& \cong & \\
  \cat{A} \ar[rr, tick, equals] \ar[dd, "F"'] & & \cat{A} \ar[dd, "H"] \\
  & \overline{\alpha \then \beta} & \\
\cat{B} \ar[rr, tick, equals] & & \cat{B}
\end{tikzcd}
\quad = \quad 
\begin{tikzcd}[sep = tiny]
  \cat{A} \ar[rr, tick, equals] \ar[dd, "F"'] & & \cat{A} \ar[rr, tick, equals]
  \ar[dd, "G"'] & & \cat{A} \ar[dd, "H"] \\
  & \bar{\alpha} & & \bar{\beta} & \\
\cat{B} \ar[rr, tick, equals] \ar[dd, equals] & & \cat{B} \ar[rr, tick, equals] & &
 \cat{B} \ar[dd, equals]\\
 & & \cong & & \\
\cat{B} \ar[rrrr, tick, equals] & & & & \cat{B}
\end{tikzcd}
\]
We leave the canonical isomorphisms without names. They can be described by the
process outlined in \cref{rmk.up_to_coherent_iso}. We note that both of these canonical isomorphisms
are given by composing two arrows, so in order to prove the equality above we
will show that given $f : X \to Y$ and $g : Y \to Z$,
\[
(\alpha \then \beta)_{f \then g} = \alpha_f \then \beta_g.
\]

We will do this by contemplating the following diagram:
\[
\begin{tikzcd}
FX \ar[d, "Ff"'] \ar[r, "\alpha_X"] \ar[dr, "\alpha_f"] & GX\ar[d, "Gf"] \ar[r, "\beta_X"] & HX\ar[d, "Hf"]  \\
FY\ar[d, "Fg"'] \ar[r, "\alpha_Y"] & GY\ar[d, "Gg"] \ar[r, "\beta_Y"] \ar[dr, "\beta_g"]& HY\ar[d, "Hg"]  \\
FZ \ar[r, "\alpha_Z"] & GZ \ar[r, "\beta_Z"] & HZ
\end{tikzcd}
\]

The naturality square for the composite $f \then g$ under the composite $\alpha
\then \beta$ is the outer square, and
therefore its diagonal $(\alpha \then \beta)_{f \then g}$ is the composite of
the diagonals in the diagram, which is $\alpha_f \then \beta_g$.
\end{proof}

\begin{lemma}\label{lemma.transformation_as_square_vertical}
 Let $F_1,\, G_1 : \cat{A} \to \cat{B}$ and $F_2,\, G_2 : \cat{B} \to \cat{C}$
 be functors, and let $\alpha : F_1 \Rightarrow G_1$ and $\beta : F_2
 \Rightarrow G_2$ be natural transformations. We may define their composite $\alpha \ast \beta$ by
\[
(\alpha \ast \beta)_X \coloneqq F_2\alpha_X \then \beta_{G_1 X}.
\]
With this definition, we have
\[
\overline{\alpha \ast \beta} = \frac{\bar{\alpha}}{\bar{\beta}}
\]
\end{lemma}
\begin{remark}
  Note that the equality claimed here is a bona-fide equality, and not an
  ``equality up to canonical isomorphism'' ($\coheq$). This is because the two
  squares involved have the exact same boundary, not merely a canonically
  isomorphic boundaries.
\end{remark}
\begin{proof}
This time, we may prove the equality as stated. It comes down to showing that 
\[
(\alpha \ast \beta)_f = \beta_{\alpha_f} 
\]
for any $f : X \to Y$. Consider the following diagram:
\[
\begin{tikzcd}& & & F_2F_1 X \ar[ddrr,color=red, "F_2\alpha_f"]
  \arrow[dl,color=blue,"F_2\alpha_X \then \beta_{G_1 X}" description] \arrow[dlll, color=red,
  "\beta_{F_1X}"']\arrow[rr]\arrow[dd, color=blue,"F_2F_1f"] & & F_2G_1 X
  \arrow[dlll, crossing over]\arrow[dd] \\ G_2F_1X \ar[ddrr, crossing over,
  color=red, "G_2\alpha_f"] \arrow[rr,crossing over]\arrow[dd] & & G_2G_1
  X & & \\& & & F_2F_1Y\arrow[dl,color=blue,"F_2\alpha_Y \then \beta_{G_1 Y}" description] \arrow[dlll]\arrow[rr] &  & F_2G_1 Y\arrow[dlll,
  color=red,"\beta_{G_1 Y}"] \\ G_2F_1 Y \arrow[rr] & & G_2G_1Y\arrow[from=uu,crossing
  over, color=blue,"G_2G_1f"] & & 
\end{tikzcd}
\]
The back and front faces are the $\alpha$ naturality square of $f$ pushed
through $F_2$ and $G_2$ respectively. The $\beta$ naturality square
of $\alpha_f$ is in the middle, colored in red. The $\alpha \ast \beta$
naturality square of $f$ is in the middle, colored in blue. We note that the
diagonal of both these squares is the diagonal of the whole cube $F_2 F_1 X \to
G_2 G_1 Y$, which means that they are equal. But this is what we were trying to show. 

\end{proof}

\section{Dynamical System Doctrines}\label{sec.doctrines}

In \cref{sec.wiring_together_non_det}, we saw how from the data of an indexed
category $\cat{A} : \cat{C}\op \to \Cat{Cat}$ we could define a category of $\cat{A}$-lenses via the Grothendieck
construction:
$$\Cat{Lens}_{\cat{A}} := \int^{C : \cat{C}}\cat{A}(C)\op.$$

From this, we learned we could wire non-deterministic systems together because a
system could be expressed as a monadic lens of the form
$\lens{\update{S}}{\expose{S}} : \lens{\State{S}}{\State{S}} \fromto
\lens{\In{S}}{\Out{S}}$. 

Now, the form $\lens{S}{S} \fromto \lens{I}{O}$ is \emph{not} something that be
expressed for a general $\cat{A}$-lens because in an $\cat{A}$-lens
$\lens{A^-}{A^+}$, $A^+ \in \cat{C}$ while $A^- \in \cat{A}(C)$. In general,
$\cat{C}$ and $\cat{A}(C)$ might have different objects. This suggests that we need
a way to assign an object $TC \in \cat{A}(C)$ to each object of $C$, so that we
can define a system, in general, to be an $\cat{A}$-lens of the form 
\[
\lens{\update{S}}{\expose{S}} : \lens{T\State{S}}{\State{S}} \fromto
\lens{\In{S}}{\Out{S}}
\]
At this point, your categorical nose should be twitching. We've given a
assignment on objects; how is this assignment functorial? We can discover what
sort of functoriality we need from considering the expression in \cref{prop.behavior_as_square_of_arenas_discrete} of behaviors as
squares of arenas in the deterministic doctrine:
\[
    \begin{tikzcd}
      \lens{\State{T}}{\State{T}} \ar[r, shift left, "\lens{\phi \circ
        \pi_2}{\phi}"] \ar[r, shift right] \ar[d, shift right,
      "\lens{\update{T}}{\expose{T}}"'] \ar[d, shift left, leftarrow] &
      \lens{\State{S}}{\State{S}} \ar[d, shift left, leftarrow,
      "\lens{\update{S}}{\expose{S}}"] \ar[d, shift right]\\
      \lens{\In{T}}{\Out{T}} \ar[r, shift right, "\lens{f^{\sharp}}{f}"'] \ar[r,
      shift left] & \lens{\In{S}}{\Out{S}}
    \end{tikzcd}
  \]
To express this square, we did not just use the fact that we could find $\State{S}$
in both the base $\cat{C}$ and in the category $\cat{A}(\State{S})$ (recall that
here, $\cat{A} = \smctx{-} : \cat{C}\op \to \Cat{Cat}$). We also used the fact
that to any map $\phi : \State{T} \to \State{S}$ we can build a chart
$\lens{\phi \circ \pi_2}{\phi} : \lens{State{T}}{\State{T}} \to
\lens{\State{S}}{\State{S}}$. This is the sort of functoriality we need to
define the notion of behavior in general.

\begin{definition}
  Let $\cat{A} : \cat{C}\op \to \Cat{Cat}$ be a strict indexed category. A
  \emph{section} $T$ of $\cat{A}$ consists of the following assignments:
  \begin{itemize}
    \item To every object $C \in \cat{C}$, an object $TC \in \cat{A}(C)$.
    \item To every $\phi : C' \to C$, a map $T\phi : TC' \to \phi^{\ast}C$.
  \end{itemize}
  These are required to satisfy the following laws:
  \begin{itemize}
    \item For any $C \in \cat{C}$, $T\id_C = \id_{TC}$.
    \item For $\phi : C' \to C$ and $\psi : C'' \to C'$,
      $$T\psi \then \psi^{\ast}(T\phi) = T(\psi \then \phi).$$
  \end{itemize}
\end{definition}

We can express a section of an indexed category in terms of a functor into its
Grothendieck construction.
\begin{proposition}\label{prop.section_indexed_category}
  Let $\cat{A} : \cat{C}\op \to \Cat{Cat}$ be a strict indexed category. A section $T$
  of $\cat{A}$ is equivalently given by the data of a functor $\hat{T} : \cat{C}
  \to \int^{C : \cat{C}} \cat{A}(C)$ for which the composite $\cat{C} \xto{\hat{T}}
  \int^{C : \cat{C}}\cat{A}(C) \xto{\const{proj}} \cat{C}$ with the projection
  is the identity on $\cat{C}$.

  Given a section $T$, we may more suggestively refer to $\hat{T}$ by $\lens{T(-)}{(-)}$.
\end{proposition}
\begin{proof}
  Given a section $T$, we can form the functor
  $$C \mapsto \lens{TC}{C} : \cat{C} \to \int^{C : \cat{C}}\cat{A}(C)$$
sending $\phi$ to $\lens{T\phi}{\phi}$. The laws of the section show that this
is a functor. 

On the other hand, given a $\hat{T} : \cat{C} \to \int^{C : \cat{C}}\cat{A}(C)$
whose composite with the projection is the identity, we see that $\hat{T}(C)$
must be of the form $\lens{TC}{C}$ and that $\hat{T}(\phi)$ must be of the form
$\lens{T\phi}{\phi}$, where $TC$ and $T\phi$ are defined to be the components of
$\hat{T}$ which live in the categories $\cat{A}(C)$. It is straightforward to
check that functoriality implies the laws of a section.
\end{proof}

We can see that the assignmen $\phi \mapsto \phi \circ \pi_2$ is a section of
$\smctx{-} : \cat{C}\op \to \Cat{Cat}$.
\begin{proposition}\label{prop.section_discrete}
Let $\cat{C}$ be a cartesian category. Then the assigment $C \mapsto C$ and
$\phi \mapsto \phi \circ \pi_2$ gives a section of $\smctx{-} : \cat{C}\op \to \Cat{Cat}$.
\end{proposition}
\begin{proof}
  We check that the two laws are satisfied.
  \begin{enumerate}
     \item $\id \circ \pi_2 = \pi_2$, which is the identity in $\smctx{C}$.
     \item We may calculate:
       \begin{align*}
         (\psi \circ \pi_2) \then \psi^{\ast}(\phi \circ \pi_2)(c, x) &= \psi^{\ast}(\phi \circ \pi_2)(c, \psi(x)) \\
                                                                      &= \phi \circ \pi_2 (\psi(c), \psi(x)) \\
                                                                      &= \phi(\psi(x)) \\
         &= (\psi \then \phi) \circ \pi_2 (c, x)
       \end{align*}
  \end{enumerate}
\end{proof}

In order to define lenses, we need the data of an indexed category $\cat{A} :
\cat{C}\op \to \Cat{Cat}$. In order to define dynamical systems as
$\cat{A}$-lenses, and to define the behaviors between them, we need the data of
a section $T$ of $\cat{A}$. Putting these two bits of data together, we get the
notion of \emph{dynamical system doctrine}.

\begin{definition}\label{def.doctrine}
A \emph{dynamical system doctrine} consists of an indexed category $\cat{A} :
\cat{C}\op \to \Cat{Cat}$ together with a section $T$.
\end{definition}

Having this definition of doctrine in mind, we can now define the notion of
dynamical system and behavior in complete generality.

\begin{definition}\label{def.doctrine_system}
A dynamical system in a doctrine $\mathbb{D} = (\cat{A}, T)$ is an $\cat{A}$-lens of the form
\[
\lens{\update{S}}{\expose{S}} : \lens{T\State{S}}{\State{S}} \fromto \lens{\In{S}}{\Out{S}}.
\]
Explcitly, this consists of:
\begin{itemize}
  \item An object $\State{S} \in \cat{C}$ of \emph{states}.
  \item An object $\Out{S} \in \cat{C}$ of \emph{possible outputs}.
  \item An object $\In{S} \in \cat{A}(\Out{S})$ of \emph{possible inputs} or
    \emph{parameters}. What parameters are sensible may therefore depend on the
    output (in the sense of being an object of a category which depends for its
    definition on $\Out{S}$).
  \item A map $\expose{S} : \State{S} \to \Out{S}$ which \emph{exposes} the
    output of a given state.
  \item A map $\update{S} : \expose{S}^{\ast} \In{S} \to T\State{S}$ which
    assigns to any parameter valid for the output of a given state to a possible
    change in state.
\end{itemize}
\end{definition}

In order to define the notion of behavior, we will need to generalize the double
category of arenas from the deterministic doctrine to an arbitrary doctrine. To
do this, we will define the \emph{Grothendieck double construction}, which
produces a double category of arenas from an indexed category $\cat{A}$.

\begin{definition}\label{def.groth_double_construction}
  Let $\cat{A} : \cat{C}\op \to \Cat{Cat}$ be an indexed category. The
  \emph{Grothendieck double construction}
$$\sqiint^{C \in \cat{C}} \cat{A}(C)$$
is the double category defined by:
\begin{itemize}
\item Its objects are the pairs $\lens{A}{C}$ of an object $C \in \cat{C}$ and
  an object $A \in \cat{A}(C)$.
\item Its horizontal category is the Grothendieck construction $\int^{C \in
    \cat{C}}\cat{A}(C)$ of $\cat{A}$.
\item Its vertical category is the Grothendieck construction $\int^{C \in
    \cat{C}} \cat{A}(C)\op$ of the opposite of $\cat{A}$.
\item There is a square of the following form:
  \begin{equation}\label{eqn.groth_double_construction}
    \begin{tikzcd}
      \lens{A_1}{C_1} \arrow[r, shift left, "\lens{g_{1\flat}}{g_1}"]\arrow[r, shift right] \arrow[d, leftarrow,  shift left] \arrow[d, shift right, "\lens{f_1^{\sharp}}{f_1}"'] & \lens{A_2}{C_2} \arrow[d, leftarrow, shift left, "\lens{f_2^{\sharp}}{f_2}"] \arrow[d, shift right] \\
      \lens{A_3}{C_3} \arrow[r, shift left]\arrow[r, shift right,
      "\lens{g_{2\flat}}{g_2}"'] & \lens{A_4}{C_4}
    \end{tikzcd}
  \end{equation}
  if and only if the following diagrams commute:
  \begin{equation} \label{eqn.groth_double_diagram}
    \begin{tikzcd}
      C_1 \arrow[r, "g_1"] \arrow[d, "f_1"'] & C_2 \arrow[d, "f_2"] &  & f_1^{\ast}A_3 \arrow[rr, "f_1^{\sharp}"] \arrow[d, "f_1^{\ast}g_{2\flat}"'] &                                                              & A_1 \arrow[d, "g_{1\flat}"] \\
      C_3 \arrow[r, "g_2"'] & C_4 & & f_1^{\ast}g_2^{\ast}A_4 \arrow[r, equals]
      & g_1^{\ast}f_2^{\ast}A_4 \arrow[r, "g_1^{\ast}f_2^{\sharp}"'] &
      g_1^{\ast}A_2
    \end{tikzcd}
  \end{equation}
  We will call the squares in the Grothendieck double construction
  \emph{commuting squares}, since they represent the proposition that the
  ``lower'' and ``upper'' squares appearing in their boundary commute.
\item Composition is given as in the appropriate Grothendieck constructions.
\end{itemize}

\end{definition}

It just remains to show that commuting squares compose.
\begin{itemize}
\item For vertical composition we appeal to the following diagram:
  \[
    \begin{tikzcd}
      f_1^{\ast}f_3^{\ast} A_5 \arrow[rr, "f_1^{\ast}f_3^{\sharp}"] \arrow[d, "f_1^{\ast}f_3^{\ast}g_{3\flat}"'] &                                                                                 & f_1^{\ast}A_3 \arrow[r, "f_1^{\sharp}"] \arrow[d, "f_1^{\ast}g_{2\flat}"] & A_1 \arrow[dd, "g_{1\flat}"] \\
      f_1^{\ast}f_3^{\ast}g_3^{\ast}A_6 \arrow[d,equals] \arrow[r,equals]                                                       & f_1^{\ast}g_2^{\ast}f_4^{\ast}A_6 \arrow[r, "f_1^{\ast}g_2^{\ast}f_4^{\sharp}"] & f_1^{\ast}g_2^{\ast}A_4 \arrow[d,equals]                                           &                               \\
      g_1^{\ast}f_2^{\ast}f_4^{\ast}A_6 \arrow[rr,
      "g_1^{\ast}f_2^{\ast}f_4^{\sharp}"'] & & g_1^{\ast}f_2^{\ast}A_4 \arrow[r,
      "g_1^{\ast}f_2^{\sharp}"'] & g_1^{\ast}A_2
    \end{tikzcd}
  \]
  The outer diagram is the ``upper'' square of the composite, while the
  ``upper'' squares of each factor appear in the top left and right
  respectively.
\item For horizontal composition we appeal to the following diagram:
  \[
    \begin{tikzcd}
      f_1^{\ast}A_3 \arrow[rr, "f_1^{\sharp}"] \arrow[d, "f_1^{\ast}g_{2\flat}"']     &                                                                                                           & A_1 \arrow[d, "g_{1\flat}"]                      \\
      f_1^{\ast}g_2^{\ast}A_4 \arrow[r,equals] \arrow[dd, "f_1^{\ast}g_2^{\ast}g_{4\flat}"'] & g_1^{\ast}f_2^{\ast}A_4 \arrow[r, "g_1^{\ast}f_2^{\sharp}"] \arrow[d, "g_1^{\ast}f_2^{\ast}g_{4\flat}"'] & g_1^{\ast}A_2 \arrow[dd, "g_1^{\ast}g_{3\flat}"] \\
      & g_1^{\ast}f_2^{\ast}g_4^{\ast}A_6 \arrow[d,equals]                                                               &                                                   \\
      f_1^{\ast}g_2^{\ast}g_4^{\ast}A_6 \arrow[r,equals] &
      g_1^{\ast}g_3^{\ast}f_3^{\ast} \arrow[r,
      "g_1^{\ast}g_3^{\ast}f_3^{\sharp}"'] & g_1^{\ast}g_3^{\ast}
    \end{tikzcd}
  \]
\end{itemize}

We can now check that this does indeed abstract the double category of arenas.

\begin{proposition}\label{prop.arenas_double_groth_contextual_maps}
  The double category of arenas in the deterministic doctrine is the
  Grothendieck double construction of the indexed category of sets and functions
  in context $\smctx{-} : \smset\op \to \Cat{Cat}$:
$$\Cat{Arena} = \sqiint^{C \in \smset} \smctx{C}.$$ 
\end{proposition}
\begin{proof}
  By \cref{prop.charts_as_groth_construction,prop.lenses_as_groth_construction},
  the horizontal and vertical categories are the same. It remains to show that
  the diagrams of \cref{eqn.groth_double_construction} mean the same things as
  \cref{eqn.dbl_cat_arena_square_commuting}.

  Consider a square of the form
  \[
    \begin{tikzcd}
      \lens{A^-}{A^+} \ar[r, shift left, "\lens{f_{\flat}}{f}"] \ar[r, shift
      right] \ar[d, shift right, "\lens{j^{\sharp}}{j}"'] \ar[d, shift left,
      leftarrow] & \lens{B^-}{B^+} \ar[d, shift left, leftarrow,
      "\lens{k^{\sharp}}{k}"] \ar[d, shift right]\\
      \lens{C^-}{C^+} \ar[r, shift right, "\lens{g^{\sharp}}{g}"'] \ar[r, shift
      left] & \lens{D^-}{D^+}
    \end{tikzcd}
  \]
  The first diagram and first equation say:
  \[
    \begin{tikzcd}
      A^+ \ar[d, "j"'] \ar[r, "f"] & B^+ \ar[d, "k"] \\
      C^+ \ar[r, "g"'] & D^+
    \end{tikzcd}
    \quad\quad g(j(a^+)) = k(f(a^+)) \quad\mbox{for all $a^+ \in A^+$,}
  \]
  which mean the same thing. The second diagram, which takes place in
  $\smctx{A^+}$, is more interesting. Here's that diagram with the names we're
  currently using:
  \[
    \begin{tikzcd}
      j^{\ast} C^- \arrow[rr, "j^{\sharp}"] \arrow[d, "j^{\ast}g_{\flat}"'] &
      &  A^- \arrow[d, "f_{\flat}"] \\
      j^{\ast}g^{\ast}D^- \arrow[r, equals] & f^{\ast}k^{\ast}D^- \arrow[r,
      "f^{\ast}k^{\sharp}"'] & f^{\ast}B^-
    \end{tikzcd}
  \]
  Let's compute the two paths from the top left to the bottom right. First is
  $f_{\flat} \circ j^{\sharp} : j^{\ast} C^- \to f^{\ast}B^-$, which sends
  $(a^+, c^-)$ to $f_{\flat}(a^+, j^{\sharp}(a^+, c^-))$. This is the right hand
  side of the second equation, so we're on the right track. The other path is
  $f^{\ast}k^{\sharp} \circ j^{\ast}g_{\flat}$. Recall that $j^{\ast}g_{\flat}$
  sends $(a^+, c^-)$ to $g_{\flat}(j(a^+), c^-)$, and similarly
  $f^{\ast}k^{\sharp}$ sends $(a^+, d^-)$ to $k^{\sharp}(f(a^+), d^-)$. Putting
  them together, we send $(a^+, c^-)$ to $k^{\sharp}(f(a^+), g_{\flat}(j(a^+),
  c^-))$. Therefore the commutation of this diagram means the same thing as the
  second equation in the definition of a square of arenas.
\end{proof}

Building off of this proposition, we can think of the Grothendieck double
construction as giving us a double category of arenas out of \emph{any} indexed
category.

\begin{definition}\label{def.double_cat_of_arenas_general}
  Let $\cat{A} : \cat{C}\op \to \Cat{Cat}$ be an indexed category. Then the
  category of $\cat{A}$-arenas is defined to be the Grothendieck double
  construction of $\cat{A}$:
$$\Cat{Arena}_{\cat{A}} := \sqiint^{C \in \cat{C}} \cat{A}(C).$$

  Note that the horizontal category of $\Cat{Arena}_{\cat{A}}$ is the category
  $\Cat{Chart}_{\cat{A}}$ of $\cat{A}$-charts (\cref{def.chart_general}), and the vertical category of
  $\Cat{Arena}_{\cat{A}}$ is the category $\Cat{Lens}_{\cat{A}}$ of
  $\cat{A}$-lenses (\cref{def.lens_general}).
\end{definition}

With this definition of the double category of arenas in hand, we can define a
behavior in a general doctrine.
\begin{definition}
 Let $\mathbb{D} = (\cat{A}, T)$ be a doctrine, and $\Sys{T}$ and $\Sys{S}$ two
 systems in this doctrine. Given an $\cat{A}$-chart
 \[
\lens{f_{\flat}}{f} : \lens{\In{T}}{\Out{T}} \rightrightarrows \lens{\In{S}}{\Out{S}},
 \]
 A $\lens{f_{\flat}}{f}$-\emph{behavior} $\phi : \Sys{T} \to \Sys{S}$
 is a map $\phi : \State{T} \to \State{S}$ so that the following is a square in
 the double category $\Cat{Arena}_{\cat{A}}$ of $\cat{A}$-arenas:
 \[
    \begin{tikzcd}
      \lens{T\State{T}}{\State{T}} \ar[r, shift left, "\lens{T\phi}{\phi}"] \ar[r, shift right] \ar[d, shift right,
      "\lens{\update{T}}{\expose{T}}"'] \ar[d, shift left, leftarrow] &
      \lens{T\State{S}}{\State{S}} \ar[d, shift left, leftarrow,
      "\lens{\update{S}}{\expose{S}}"] \ar[d, shift right]\\
      \lens{\In{T}}{\Out{T}} \ar[r, shift right] \ar[r,
      shift left, "\lens{f_{\flat}}{f}"'] & \lens{\In{S}}{\Out{S}}
    \end{tikzcd}
 \]
 We will often refer to this square by $\phi$ as well.
\end{definition}

In \cref{sec.behaviors_general}, we will see what composition in the double
category $\Cat{Arena}_{\cat{A}}$ of $\cat{A}$-arenas let's us conclude about
composition of systems and behaviors. For now, in the rest of this section, we will formally introduce the dynamical system
doctrines we have been working with throughout the book, with some precise
variations we can now make clear.

But before we do that, let's see how the above
rather terse formal definition captures the intuitive and informal definition
given in \cref{informal.doctrine}:
\begin{informal}
  A \emph{doctrine} of dynamical systems is a particular way to answer the following
  questions about what it means to be a dynamical system:
  \begin{enumerate}
  \item What does it mean to be a state?
  \item How should the output vary with the state --- discretely,
    continuously, linearly?
  \item Can the kinds of input a
    system takes in depend on what it's putting out, and how do they depend on it?
  \item What sorts of changes are possible in a given state?
  \item What does it mean for states to change. 
  \item How should the way the state changes vary with the input?
  \end{enumerate}
\end{informal}

Let's see how choosing an indexed category $\cat{A} : \cat{C}\op \to \Cat{Cat}$
and a section $T$ constitutes a series of answers to each of these questions.
\begin{enumerate}
  \item We had to choose the base category $\cat{C}$. Our space of states will
    be an object of this category, and so choosing the objects of this category means choosing
    what it means to be a state.
  \item Our exposed variable $\expose{S} : \State{S} \to \Out{S}$ will be a
    morphism of $\cat{C}$, so choosing the morphisms of $\cat{C}$ will mean
    choosing how the output will vary with the state.
  \item The input $\In{S}$ will be an object of $\cat{A}(\Out{S})$, and
    therefore defining the objects of $\cat{A}(\Out{S})$ --- in particular, how
    does they 
    depend on $\Out{S}$ --- will determine how a system's space of inputs may
    depend on its outputs.
  \item The our update map $\update{S} : \expose{S}^{\ast} I \to T\State{S}$
    has codomain $T\State{S}$. Therefore, choosing object assignment of the
    section $T$ tells us space of possible changes which the system may make (as
    depending on the state it is in, in the sense that $T\State{S}$ lives in a
    category $\cat{A}(\State{S})$ which depends for its definition on $\State{S}$).
  \item Since a behavior will involve the chart $\lens{T\phi}{\phi} :
    \lens{T\State{T}}{\State{T}} \rightrightarrows
    \lens{T\State{S}}{\State{S}}$, choosing the action of $T$ on maps $\phi$
    will tell us what it means to interpret changes of state that arise from the
    dynamics of the system into whole behaviors of the system. We will see an
    elaboriation of this idea when we discuss behaviors in doctrines other than
    the deterministic doctrine.
  \item By choosing the maps of $\cat{A}$, we will determine what sort of map
    $\update{S}$ is. This will determine in what sort of way the changes in
    state vary with parameters.
\end{enumerate}

\subsection{The deterministic doctrines}

We have been speaking of the deterministic doctrine throughout this book to mean
the doctrine of machines with discrete time whose next state is entirely
determined by its current state and choice of parameters. But really, there have
been many deterministic doctrines, one for each cartesian category $\cat{C}$.

\begin{definition}\label{def.deterministic_doctrines}
  Let $\cat{C}$ be a cartesian category. The \emph{deterministic doctrine}
  $\determ_\cat{C}$ in $\cat{C}$ is defined to be the indexed category
  $\smctx{-}: \cat{C}\op \to \Cat{Cat}$ together with the section $C \mapsto C$
  and $\phi \mapsto \phi \circ \pi_2$ defined in \cref{prop.section_discrete}.
\end{definition}

\begin{remark}
 \cref{prop.behavior_as_square_of_arenas_discrete} Shows that behaviors in a
 deterministic doctrine are precisely what we studied (and saw examples of) in \cref{sec.behaviors}.
\end{remark}

There are many different deterministic doctrines, one for each choice of
cartesian category $\cat{C}$. For example:
\begin{itemize}
  \item If $\cat{C} = \Cat{Set}$ is the category of sets, we have discontinuous, discrete-time,
    determinisitic systems. These are often called ``Moore machines''.
  \item If $\cat{C} = \Cat{Top}$ is the category of topological spaces, we have
    continuous, discrete-time, deterministic systems.
  \item If $\cat{C} = \Cat{Man}$ is the category of smooth manifolds, then we
    have smooth, discrete-time, deterministic systems.
  \item If $\cat{C} = \Cat{Meas}$ is the category of measurable spaces and
    measurable maps, then we have discrete-time, deterministic systems whose
    $\update{}$ is measurable.
  \item And so on...
\end{itemize}

Let's see how to interpret the determistic doctrine in the case that $\cat{C} =
\smset$ answers the questions of \cref{informal.doctrine}.
\begin{enumerate}
 \item A state is an element of a set.
 \item The output varies as a function of the state, with constraints on what
   sort of function.
 \item No, the kinds of inputs do not depend on the state.
 \item From a state, one may transition to any other state (since $T\State{S} = \State{S}$).
 \item We treat the changes and the states in the same way, interpreting a
   change as the next state.
 \item The change in state is a function of the previous state and the input.
\end{enumerate}


\begin{exercise}
Answer the questions of \cref{informal.doctrine} in for the following doctrines:
\begin{enumerate}
   \item $\determ_{\Cat{Top}}$.
   \item $\determ_{\Cat{Man}}$.
   \item $\determ_{\Cat{Arity}}$.
\end{enumerate}
\end{exercise}

\subsection{The differential doctrines}

We can now define the differential doctrines, which will finally let us see the
definitions of differential behavior given in \cref{sec.kinds_of_behavior} as
different incarnations of a single, general definition.

Unlike the case with deterministic doctrines, we will not be giving a single,
general definition of ``differential'' doctrine. We will be defining our
different differential doctrines ad-hoc.\footnote{One can give a general
  definition of differential doctrine that specializes to these various notions
  with the notion of \emph{tangent category with display maps}. But we prefer to
just describe the various categories as they come.}

We begin with the differential doctrine used to define the notion of
differential system in \cref{def.differential_system}. 
\begin{definition}\label{def.euclidean_diff_doctrine}
The \emph{Euclidean differential doctrine} $\eucdiff$ is defined by the indexed
category $\smctx{-} : \Cat{Euc}\op \to \Cat{Cat}$ together with the section $T$
given by 
\begin{itemize}
  \item $T\rr^n := \rr^n$, thinking of $\rr^n$ as tangent space of a point in $\rr^n$.
  \item For a differentiable map $ f : \rr^n \to \rr^m$, we define $Tf : \rr^n
    \times \rr^n \to \rr^m$ to be 
\[
Tf(p, v) \coloneqq Df_p v
\]
where $Df_p$ is the matrix of partial derivatives \(\begin{pmatrix}
\left.\frac{\partial f_i}{\partial x_j}\right|_{x = p}
\end{pmatrix}\). 
In other words, $Tf(p, v)$ is the directional derivative in direction $v$ of $f$
at $p$. The functoriality law for the section is precisely the multivariable
chain law.
\end{itemize}
\end{definition}

\begin{exercise}
Check that $T$ as defined is indeed a section by referring to the
multidimensional chain law.
\end{exercise}
\begin{remark}
Note that if $f : \rr \to \rr^n$ is a function, then 
$$Tf(t, v) = \frac{df}{dt}(t) \cdot v.$$
\end{remark}

The Euclidean differential doctrine $\eucdiff$ answers the questions of
\cref{informal.doctrine} in the following way:
\begin{enumerate}
  \item A state is a $n$-tuple of real numbers, which is to say a point in $\rr^n$.
  \item The output is a differentiable function of the state.
  \item The kind of input does not depend on the output.
  \item A possible change in a state is given by a displacement vector, also in $\rr^n$.
  \item For a state to change means that it is tending in this direction. That
    is, it has a given derivative.
  \item The changes in state vary differentiably with the input.
\end{enumerate}

Let's see what behaviors look like in the Euclidean differential doctrine. Note
that since the indexed category of $\eucdiff$ is $\smctx{-} : \Cat{Euc}\op \to
\Cat{Cat}$, its double category of arenas is the same as for the deterministic
doctrine $\determ_{\Cat{Euc}}$. However, the definition of behavior will be
different because the section is different. Let's work out what a general
behavior is in $\eucdiff$ explicitly.
\begin{proposition}
Let $\Sys{T}$ and $\Sys{S}$ be systems in the Euclidean differential doctrine. 

A
chart $\lens{f_{\flat}}{f} : \lens{\In{T}}{\Out{T}} \rightrightarrows
\lens{\In{S}}{\Out{S}}$ consists of a pair of smooth functions $f : \Out{T} \to \Out{S}$ and
$f_{\flat} : \Out{T} \times \In{T} \to \In{S}$. 

A $\lens{f_{\flat}}{f}$-behavior is a smooth function $\phi : \State{T} \to
\State{S}$ such that 
\begin{align*}
  \expose{S}(\phi(t)) &= f(\expose{T}(t)).\\
  \update{S}(\phi(t), f_{\flat}(\expose{T}(t), j)) &= D\phi_{t} \update{T}(t, j) \\
\end{align*}
\end{proposition}
\begin{proof}
  This is a matter of interpreting the square
\[
    \begin{tikzcd}
      \lens{T\State{T}}{\State{T}} \ar[r, shift left, "\lens{T\phi}{\phi}"] \ar[r, shift right] \ar[d, shift right,
      "\lens{\update{T}}{\expose{T}}"'] \ar[d, shift left, leftarrow] &
      \lens{T\State{S}}{\State{S}} \ar[d, shift left, leftarrow,
      "\lens{\update{S}}{\expose{S}}"] \ar[d, shift right]\\
      \lens{\In{T}}{\Out{T}} \ar[r, shift right] \ar[r,
      shift left, "\lens{f_{\flat}}{f}"'] & \lens{\In{S}}{\Out{S}}
    \end{tikzcd}
\]
using by specializing \cite{eqn.dbl_cat_arena_square_commuting} to the above
case, using the definition of $T\phi$ in $\eucdiff$.  
\end{proof}

\begin{example}
  Consider the following system $\Sys{Time}$ in $\eucdiff$:  \begin{itemize}
    \item $\State{Time} = \rr = \Out{Time}$, and $\expose{Time} = \id$.
    \item $\In{Time} = \rr^0$, and
      $$\update{Time}(s, \ast) \coloneqq 1.$$
  \end{itemize}
  This system represents the simple differential equation
  $$\frac{ds}{dt} = 1.$$

  Let $\Sys{S}$ be another system in $\eucdiff$. A chart $\lens{p}{v} :
  \lens{\rr^0}{\rr} \rightrightarrows \lens{\In{S}}{\Out{S}}$ consists of a
  function $v : \rr \to \Out{S}$ and a function $p : \rr \times \rr^0 \to
  \In{S}$, which is to say $p : \rr \to \In{S}$. This is precisely the sort of
  chart we need for a trajectory.


  A $\lens{p}{v}$-behavior $\phi : \Sys{Time}
  \to \Sys{S}$ consists of a differentiable function $\phi : \rr \to \State{S}$
  such that the following is a square in the double category of arenas:
  \[
    \begin{tikzcd}
      \lens{\rr}{\rr} \ar[r, shift left, "\lens{T\phi}{\phi}"] \ar[r, shift right] \ar[d, shift right,
      "\lens{1}{\id}"'] \ar[d, shift left, leftarrow] &
      \lens{T\State{S}}{\State{S}} \ar[d, shift left, leftarrow,
      "\lens{\update{S}}{\expose{S}}"] \ar[d, shift right]\\
      \lens{{\ast}}{\rr} \ar[r, shift right] \ar[r,
      shift left, "\lens{p}{v}"'] & \lens{\In{S}}{\Out{S}}
    \end{tikzcd}
  \]
  For this to be a square means that the following two equations hold:
  \begin{align*}
    \expose{S}(\phi(t)) &= v(t)\\
    \update{S}(\phi(t), p(t)) &= \frac{d\phi}{dt}(t).
  \end{align*}
  That is, a behavior of this sort is precisely a trajectory as defined in
  \cref{def.trajectory_diff}
\end{example}

\begin{example}
  Consider the following simple system $\Sys{Fix}$:
  \begin{itemize}
\item $\State{Fix} = \rr^0 = \Out{Fix}$ and $\expose{Fix} = \id$.
\item $\In{Fix} = \rr^0$ and $\update{Fix}(\ast, \ast) = \ast$.
\end{itemize}
This system has no state variables. Nevertheless, a chart $\
lens{i}{o} : \lens{\In{Fix}}{\Out{Fix}} \rightrightarrows
\lens{\In{S}}{\Out{S}}$ into some other system $\Sys{S}$ is not trivial; it is a
pair of elements $i \in \In{S}$ and $o \in \Out{S}$.

A $\lens{i}{o}$-behavior $s : \Sys{Fix} \to \Sys{S}$ consists of a
differentiable function $s : \rr^0 \to \State{S}$ --- which is to say a state
$s \in \State{S}$ -- such that the following is a square in the double category of
arenas:
\[
    \begin{tikzcd}
      \lens{\rr^0}{\rr^0} \ar[r, shift left, "\lens{Ts}{s}"] \ar[r, shift right] \ar[d, shift right,
      "\lens{0}{\id}"'] \ar[d, shift left, leftarrow] &
      \lens{T\State{S}}{\State{S}} \ar[d, shift left, leftarrow,
      "\lens{\update{S}}{\expose{S}}"] \ar[d, shift right]\\
      \lens{{\ast}}{\rr} \ar[r, shift right] \ar[r,
      shift left, "\lens{p}{v}"'] & \lens{\In{S}}{\Out{S}}
    \end{tikzcd}
\]
Now, $s : \rr^0 \to \State{S}$ is a constant function, so $T(s, \ast) = 0$.
Therefore, for this to be a square means that the following two equations hold:
\begin{align*}
\expose{S}(s) = o. \\
\update{S}(s, i) = 0.
\end{align*}
This says that $\Sys{S}$ is not changing in state $s$ on input $i$, or that $s$
is a steady state of $\Sys{S}$ for input $i$ as in \cref{def.steady_state_differential}.
\end{example}

Now, we would like to also show that periodic orbits are behaviors in a
differential doctrine, but we're a bit stuck. In the Euclidean doctrine, there's
no way to ensure that a trajectory $\phi : \rr \to \rr^n$ is periodic. Recall
that $\phi$ being periodic means that 
$$\phi(t) = \phi(t + k)$$
for some $k \in \rr$ called the period. If $\phi$ is periodic, then it descends
to the quotient $\rr / k\zz$, which is a circle of radius $\frac{k}{2\pi}$. If
we could define $\State{Orbit_k}$ to be $\rr / k\zz$, then a trajectory $\hat{\phi} :
\State{Orbit_k} \to \State{S}$ would be precisely a periodic trajectory $\phi :
\rr \to \State{S}$. To make this expansion of representable behaviors, we will
need to move beyond Euclidean spaces.

Our first guess might be to simply change out the category $\Cat{Euc}$ of
Euclidean spaces for the category $\Cat{Man}$ of smooth manifolds in the
definition of $\eucdiff$. Certainly, $\Cat{Man}$ is a cartesian category and so
$\smctx{} : \Cat{Man}\op \to \Cat{Cat}$ is a perfectly good indexed category.
But the tangent bundle of a general smooth manifold is not necessarily a product
like it is for $\rr^n$. So we would need to change our indexed category as well!

\iffalse
We could certainly do this, but if we only want to add circles then we can do
something less drastic. We can consider the cartesian category whose objects are
of the form $\rr^n \times T^m$, where $T^m = S^1 \times \cdots \times S^1$ is
the $m$-fold torus, and whose maps are smooth maps. Since the circle is
\emph{parallelizable}, which is to say that one can define a vector field on the
circle as a function in $\rr$, the tangent bundle of $\rr^n \times T^m$ is a
product of this space with $\rr^{n + m}$. 

\begin{definition}
  Let $\Cat{Tori}$ denote the category whose objects are $\rr^n \times T^m$ and
  smooth maps between them. We note that this is a cartesian category, with
  $$(\rr^n \times T^m) \times (\rr^k \times T^q) = \rr^{n + k} \times T^{m + q}.$$
\end{definition}

\begin{definition}\label{def.toric_diff_doctrine}
 The \emph{toric differential doctrine} consists of $\smctx{-} : \Cat{Tori}\op
 \to \Cat{Cat}$ together with the section $T$ given by $T(\rr^n \times T^m) =
 \rr^{n + m}$ and for $f : \rr^n \times T^m \to \rr^k \times T^q$, $Tf : (\rr^n
 \times T^m) \times \rr^{n + m} \to \rr^k \times T^q$ defined to be
 \[
Tf((p,t),(v, u)) \coloneqq ()
 \]
\end{definition}
\fi

Now, strictly speaking we don't \emph{have} to do this if we only want to add
circles, because circles have a trivial tangent bundle. But it will turn out
that defining the section $T$ will involve choosing, once and for all, a
particular trivialization of the tangent bundle of the circle and expressing all
derivatives in terms of this. It will end up much easier to simply jump over to manifolds.

We recall that to any manifold $M$ there is an associated tangent bundle $\pi :
TM \to M$. A vector field on a manifold $M$ is a section $v : M \to TM$ of the
tangent bundle. We recall a bit about tangent bundles now.
\begin{proposition}
The assignment of a manifold $M$ to its tangent space $TM$ is functorial in that
it extends to an assignment
\[
f : M \to N \mapsto Tf : TM \to TN
\]
which, on Euclidean spaces gives $Tf(p, v) = Df_p v$. Furthermore, the tangent
bundle $\pi : TM \to M$ is natural in the the diagram
\[
  \begin{tikzcd}
    TM \ar[d, "\pi"'] \ar[r, "Tf"]& TN \ar[d, "\pi"] \\
    M \ar[r, "f"'] & N
  \end{tikzcd}
\]
commutes.
\end{proposition}

There is something special about the tangent bundle which allows it to be
re-indexed to a different manifold: it is a \emph{submersion}. Not all pullbacks
of manifolds exist, but all pullbacks of submersions exist and are submersions.
\begin{definition}
  A \emph{submsersion} $\phi : M \to N$ is a map of manifolds for which $T_p\phi
  : T_p M \to T_{\phi(p)} N$ is surjective for each $p \in M$.

  We note that every isomorphism is a submersion, and that the composite of
  submersions is a submersion.
\end{definition}

\begin{lemma}\label{lem.submersion_closed_pullback}
 Let $\phi : A \to B$ be a submersion. Then for any $f : C \to B$, the set
 theoretic pullback $A \times_B C = \{(a, c) \in A \times C \mid \phi(a) =
 f(c)\}$ may be given the structure of a smooth manifold so that the two
 projections $A \times_B C \to A$ and $A \times_B C \to C$ are smooth, and so
 that the resulting square is a pullback square in the category of manifolds.
 Furthermore, the projection $ f^{\ast}\phi : A \times_B C \to C$ is also a submersion.
 
 In short, we say that pullbacks of submersions exist and are themselves submersions.
\end{lemma}

This situation arises enough that we can give an abstract definition of it. 
\begin{definition}
Let $\cat{C}$ be a category. A class of \emph{display maps} in
$\cat{C}$ is a class of maps $\cat{D}$ which satisfies the following: 
\begin{itemize}
  \item Every isomorphism is in $\cat{D}$.
  \item $\cat{D}$ is closed under composition. If $f$ and $g$ are composable arrows in $\cat{D}$, then $f \then g$ is
    in $\cat{D}$.
  \item $\cat{D}$ is closed under pullback. If $f : A \to B$ is in $\cat{D}$ and
    $g : C \to B$ is any map, then the pullback $g^{\ast} f : A \times_B C \to
    C$ exists and is in $\cat{D}$.
\end{itemize}
A \emph{category with display maps} $(\cat{C}, \cat{D})$ is a category $\cat{C}$ equipped with a
class $\cat{D}$ of display maps.
\end{definition}

We have seen that $(\Cat{Man}, \Cat{Subm})$ is a category with display maps by
\cref{lem.submersion_closed_pullback}. There are two other common classes of display map categories.
  \begin{itemize}
   \item If $\cat{C}$ has all pullbacks, then we may take all maps to be display maps.
   \item If $\cat{C}$ is cartesian, then we may take the product projections
     to be the display maps. 
  \end{itemize}
The first of these obviously works, but the second requires a bit of proof (and
to be a bit more carefully defined).
\begin{proposition}\label{prop.cartesian_display_maps}
 Let $\cat{C}$ be a cartesian category. Let $\cat{D}$ denote the class of maps
 $f : A \to B$ for which there exists a $C \in \cat{C}$ and an isomorphism $i :
 A \to C \times B$ for which $f = i \then \pi_2$. That is, $\cat{D}$ is the
 class of maps which are product projections up to an isomorphism. Then
 $(\cat{C}, \cat{D})$ is a display map category.
\end{proposition}
\begin{proof}
  We verify the conditions
\begin{itemize}
  \item If $i : A \to B$ is an isomorphism, then $f \then \pi_2\inv : A \to
    \ord{1} \times B$ is also an isomorphism. By construction, $f = f \then
    \pi_2 \inv \then \pi_2$, so every isomorphism is a product projection up to isomorphism. 
  \item Suppose that $f : A \to B$ is isomorphic to a product projection $\pi_2 : C \times
    B \to B$ in that $f = i \then \pi_2$, and $g : B \to X$ is isomorphic to a product projection $\pi_2 : Y
    \times X \to X$ in that $g = j \then \pi_2$. We may then see that $f \then
    g$ is a product projection up to isomorphism by contemplating the following
    commutative diagram:
\[
\begin{tikzcd}
A \arrow[d, "f"'] \arrow[r, "i"] & C \times B \arrow[ld, "\pi_2"] \arrow[r, "C \times j"] & C \times (Y \times X) \arrow[r, "\alpha"] \arrow[ld, "\pi_2"] & (C \times Y) \times X \arrow[llldd, "\pi_2", bend left] \\
B \arrow[d, "g"'] \arrow[r, "j"] & Y \times X \arrow[ld, "\pi_2"]                        &                                                               &                                                         \\
X                                &                                                        &                                                               &                                                        
\end{tikzcd}
\]
\item Let $f : A \to B$ be a equal to $i \then \pi_2$ with $i : A \to C \times
  B$ an isomorphism. Let $k : X \to B$ be any other map. We will show that
  $\pi_2 : C \times X \to X$ fits in a pullback diagram as follows:
  \[
\begin{tikzcd}
Z \arrow[rdd, "x"', bend right] \arrow[rrrd, "a", bend left] \arrow[rd, "{(a \then i \then \pi_1, x)}", dashed] &                                                       &                               &                  \\
                                                                                                                & C \times X \arrow[r, "C \times k"] \arrow[d, "\pi_2"] & C \times B \arrow[r, "i\inv"] & A \arrow[d, "f"] \\
                                                                                                                & X \arrow[rr, "k"']                                    &                               & B               
\end{tikzcd}
\]
The square commutes since $i\inv \then f = \pi_2 : C \times B \to B$. We see that it satisfies the universal property by making the definition of the
dashed arrow given in the diagram. The lower triangle commutes by definition, so
consider the upper triangle, seeking to show that 
$a = (a\then i \then \pi_1, x) \then (C \times k) \then i\inv$. We calculate:
\begin{align*}
(a\then i \then \pi_1, x) \then (C \times k) \then i\inv &= (a\then i \then \pi_1, x \then k) \then i\inv \\
 &= (a\then i \then \pi_1, a \then f) \then i\inv \\
 &= (a\then i \then \pi_1, a \then i \then \pi_2) \then i\inv \\
 &= a \then i \then i \inv \\
&= a.
\end{align*}
\end{itemize}
Now, if $z : Z \to C$ were any other map so that $(z, x) \then C \times k \then
i\inv = a$, we would have $(z, a \then i \then \pi_2) \then i\inv = a$, or $(z,
a \then i \then pi_2) = a \then i$, from which we may deduce that $z = a \then i
\then \pi_1$. This proves uniqueness of the dashed map.
\end{proof}


We can now construct the indexed category that will form the basis of our new
differential doctrine. We will do so at the general level of display map
categories, since the construction relies only on this structure.
\begin{definition}
Let $(\cat{C}, \cat{D})$ be a category with display maps. The indexed category $\cat{D} : \cat{C}\op \to \Cat{Cat}$ is defined as
follows:
\begin{itemize}
  \item To each object $C \in \cat{C}$, $\cat{D}(M)$ is the category with
    objects the display maps
    $\phi : E \to C$ and maps $f : E \to E'$ such that $f \then \phi' = \phi$.
    That is, it is the full subcategory of the slice category over $C$ spanned
    by the display maps.
  \item To each map $f : C' \to C$, we associate the functor $f^{\ast} :
    \cat{D}(N) \to \cat{D}(M)$ given by taking the pullback along $f$.
\end{itemize}
We note that this is functorial up to coherent isomorphism by the uniqueness (up
to unique isomorphism) of the pullback.
\end{definition}
 
We can specialize this to the category of smooth manifolds with submersions the
display maps
\begin{definition}
The indexed category $\Cat{Subm} : \Cat{Man}\op \to \Cat{Cat}$ is defined as follows:
\begin{itemize}
  \item To each manifold $M$, $\Cat{Subm}(M)$ is the category of submersions
    $\phi : E \to M$ and maps $f : E \to E'$ such that $f \then \phi' = \phi$.
  \item To each map $f : M \to N$, we associate the functor $f^{\ast} :
    \Cat{Subm}(N) \to \Cat{Subm}(M)$ given by taking the pullback along $f$.
\end{itemize}
\end{definition}

\begin{exercise}
 Let $\cat{C}$ be a cartesian category, and equip it with the class $\cat{D}$ of maps
 which are isomorphic to product projections, as in
 \cref{prop.cartesian_display_maps}. Prove that $\cat{D} : \cat{C}\op \to
 \Cat{Cat}$ is equivalent, as an indexed category, to $\smctx{-} : \cat{C}\op
 \to \Cat{Cat}$.
\end{exercise}

If $(\cat{C}, \cat{D})$ is a category with display maps, then the category of
charts of $\cat{D} : \cat{C}\op \to \Cat{Cat}$ is easy to understand in terms of $\cat{D}$.
\begin{proposition}\label{prop.display_maps_charts}
Let $(\cat{C}, \cat{D})$ be a category with display maps. Then the category
$\Cat{Chart}_{\cat{D}} = \int^{C \in \cat{C}}\cat{D}(C)$ 
of charts for $\cat{D} : \cat{C}\op \to \Cat{Cat}$ is equivalent to the category
whose objects are display maps and whose morphisms are commutative squares
between them. 
\end{proposition}
\begin{proof}
An object $\lens{a^-}{A^+}$ of the category of charts is a pair consisting of an
object $A^+ \in \cat{C}$ and a display map $a^- : A^- \to A^+$ in
$\cat{D}(A^+)$. But $A^+$ is determined, as the codomain, by $a^-$; so the
objects of the category of charts are in bijection with the display maps. We
then show that the charts are similarly in bijection with
the squares between display maps.

A chart $\lens{f_{\flat}}{f} : \lens{a^-}{A^+} \rightrightarrows
\lens{b^-}{B^+}$ for this indexed category is a pair consisting of a map $f :
A^+ \to B^+$ in $\cat{C}$ and a triangle
\[
\begin{tikzcd}
A^- \arrow[rd, "a^-"'] \arrow[rr, "f_{\flat}"] &     & f^{\ast}B^- \arrow[ld, "f^{\ast}b^-"] \\
                                               & A^+ &                                      
\end{tikzcd}
\]
By the universal property of the pullback, this data is equivalently given by
the data of a square
\[
\begin{tikzcd}
A^- \arrow[d, "a^-"'] \arrow[r, "\hat{f_{\flat}}"] & B^- \arrow[d, "b^-"] \\
A^+ \arrow[r, "f"']                                & B^+                 
\end{tikzcd}
\]
Now, consider a composite square
\[
\begin{tikzcd}
A^- \arrow[d, "a^-"'] \arrow[r, "\hat{f_{\flat}}"] & B^- \arrow[d, "b^-"] \arrow[r, "\hat{g_{\flat}}"] & C^- \arrow[d, "c^-"] \\
A^+ \arrow[r, "f"']                                & B^+ \arrow[r, "g"']                               & C^+                 
\end{tikzcd}
\]
We can see that the arrow $f_{\flat} \then f^{\ast}g_{\flat} : A^- \to
f^{\ast}g^{\ast}B^-$ composes with the projections from the pullbacks to give
the top half of the outer square, and therefore it is the unique map into the
pullback induced by the outer square.
\end{proof}

\begin{corollary}
Let $(\cat{C}, \cat{D})$ be a category with display maps. To give a section of
$\cat{D} : \cat{C}\op \to \Cat{Cat}$, it suffices to give an endofunctor $T :
\cat{C} \to \cat{C}$ together with a natural transformation $\pi : T \to
\id_{\cat{C}}$ whose components are all display maps.
\end{corollary}
\begin{proof}
Such an endofunctor $T$ with natural transformation $\pi$ gives us a functor $C
\mapsto \pi : TC \to C$ going from $\cat{C}$ to the category of display maps in
$\cat{C}$ and squares between them. This functor will assign to each $f : C' \to
C$ the naturality square 
\[
\begin{tikzcd}
TC' \arrow[d, "\pi"'] \arrow[r, "Tf"] & TC \arrow[d, "\pi"] \\
C' \arrow[r, "f"']                                & C
\end{tikzcd}
\]
We note that the evident projection of the codomain composes with this functor
to give $\id_{\cat{C}}$. By \cref{prop.display_maps_charts}, this is equivalent
to giving such a functor into $\Cat{Chart}_{\cat{D}}$, which, by
\cref{prop.section_indexed_category} is equivalent to giving a section of
$\cat{D}$.
\end{proof}

We may therefore define a doctrine associated to any category with display maps
$(\cat{C}, \cat{D})$ with such an endofunctor $T : \cat{C} \to \cat{C}$ and
natural transformation $\pi : T \to \id_{\cat{C}}$ whose components are all
display maps.

\begin{definition}
Let $(\cat{C}, \cat{D})$ be a category with display maps and let $T : \cat{C}
\to \cat{C}$ be an endofunctor and
$\pi : T \to \id_{\cat{C}}$ a natural transformation whose components are all
display maps. Then this data forms a doctrine $\dispdoc_{\cat{D}, T}$ given by $\cat{D} : \cat{C}\op \to
\Cat{Cat}$ and the section induced by sending $C$ to $\pi : TC \to C$ in $\cat{D}(C)$.
\end{definition}

With one last lemma, we will finally be able to define our general differential doctrine.
\begin{lemma}
The tangent bundle $\pi : TM \to M$ of a manifold $M$ is a submersion.
\end{lemma}

\begin{definition}\label{def.general_diff_doctrine}
The \emph{general differential doctrine} $\diffdoc$ is defined to be the display
category doctrine
$\dispdoc_{\Cat{Subm}, T}$ associated to the $\Cat{Subm} : \Cat{Man}\op \to
\Cat{Cat}$ and the tangent bundle functor $T$.
\end{definition}

A dynamical system in the general differential doctrine $\diffdoc$ consists of a
state space $\State{S}$, and output space $\Out{S}$, but then a \emph{submersion
of inputs} $\pi_{\In{S}} : \In{S} \to \Out{S}$. We can think of $\pi$ as assigning to
each input the output that it is valid for. The update then has signature 
$$\update{S} : \expose{S}^{\ast} \pi_{\In{S}} \to \pi_{T\State{S}}$$
which is to say that it is a triangle of the form
\[
\begin{tikzcd}
\State{S} \times_{\Out{S}} \In{S} \ar[dr, "\pi_1"'] \ar[rr, "\update{S}"] & & T\State{S} \ar[dl, "\pi"]
\\
& \State{S} &
\end{tikzcd}
\] 
which assigns to each state-input pair $(s, i)$ where $i$ is valid given the
state $s$ in the sense that $\expose{S}(s) = \pi_{\In{S}}(i)$ to a tangent
vector at $s$. There will be a lot more to say about dynamical systems in which
the sorts of inputs allowed depend on the current output in later chapters, but
for now we'll have to content ourselves with a simple example.

The general differential doctrine $\diffdoc$ answers the questions of
\cref{informal.doctrine} in the following way:
\begin{enumerate}
  \item A state is a point in a smooth manifold.
  \item The output is a smooth function of the state.
  \item The kind of input can dependend on the output, but it does so
    continuously (in the sense that the assignment sending an input to the
    output it is valid for is a submersion).
  \item A possible change in a state is given by a tangent vector.
  \item For a state to change means that it is tending in this direction. That
    is, it has derivative equal to the given tangent vector.
  \item The changes in state vary differentiably with the input.
\end{enumerate}

\begin{example}
Let's see an example of a situation where the inputs may differ over different
outputs. Suppose we have a robot on a distant planet, and we are directing it.
When we tell it to move in a direction, the robot will move in the given
direction at a given speed $k$. We want to keep
track of the position of the robot as it moves around the planet.

We can model this situation as follows: since the surface of the planet is a
sphere and we want to keep track of where the robot is, we will let $\State{S} =
S^2$ be a sphere. We will also have the robot reveal its position to us, so that
$\Out{S} = S^2$ and $\expose{S} = \id$.

Now, in any given position $p \in \Out{S}$, we want the space of inputs
$\In{S_p}$ valid for $p$ to be the directions we can give to the robot: that is
to say, $\In{S_p} \cong S^1$ should form a circle. However, we want these
directions to be directions that the robot could actually travel, so we will let
$\In{S_p} = \{v \in T_p \Out{S}\mid |v| = 1\}$ be the unit circle in the tangent
space at $p$. Then we may describe the fact that the robot moves in the
direction we tell it by defining
$$\update{S}(s, i) = ki.$$
\end{example}

We note that any system $\Sys{S}$ in the Euclidean differential doctrine can be
considered as
a system in the general differential doctrine by defining the bundle of inputs
to the $\pi_1 : \Out{S} \times \In{S} \to \Out{S}$ and noting that the pullback
of a product projection is a product projection, so that we may take the domain
of the new update $u : \expose{S}^{\ast} \pi_1 \to \pi_{T\State{S}}$ to be
$\State{S} \times \In{S}$, just as it was. We may then define $u(s, i) = (s,
\update{S}(s, i))$, equating $T\State{S} = T\rr^n$ with $\rr^n \times \rr^n$.
Later, when we discuss change of doctrine, we will see that this follows from a
\emph{morphism of doctrines} $\eucdiff \to \diffdoc$.

We can now describe periodic orbits as behaviors in the general differential doctrine.
\begin{example}
  Let $\Sys{Clock_k}$ be the system in $\diffdoc$ with:
\begin{itemize}
 \item State space $ \State{Clock_k} = \rr / k \zz$,
 \item Output space $\Out{Clock_k} = \rr / k \zz$ with $\expose{Clock_k} = \id$.
 \item Input bundle the identity $\In{Clock_k} = \id_{\Out{Clock_k}}$.
 \item Update $\update{Clock_k} : \State{Clock_k} \to T\State{Clock_k}$ the
   assigning each state $s$ to the vector $Tq(1)$, the pushforward of the
   constant vector $1$ on $\rr$ by the quotient
   $q : \rr \to \rr / k \zz$.
\end{itemize}
The universal property of $\rr / k \zz$ says that a smooth function $\gamma :
\rr \to M$ factors through $q : \rr \to \rr / k \zz$ if and only if $\gamma(kn)
= \gamma(0)$ for all $n \in \zz$.

A chart for $\Sys{Clock_k}$ into a system $\Sys{S}$ is a square
\[
  \begin{tikzcd}
    \rr / k \zz \ar[r, "p"] \ar[d, equals] & \In{S} \ar[d, "\pi"] \\
    \rr / k \zz \ar[r, "v"'] & \Out{S}
  \end{tikzcd}
\]
By the various universal properties involved, this is the same data as a pair of maps $\hat{p} : \rr \to
\In{S}$ and $\hat{v} : \rr \to \Out{S}$ for which $\hat{p}(nk) = 0$ and
$\hat{v}(nk) = 0$ for all $n \in \zz$, and for which $\pi \circ \hat{i} =
\hat{v}$.

Now, a behavior $\phi : \Sys{Clock_k} \to \Sys{S}$ is a square
  \[
    \begin{tikzcd}
      \lens{T(\rr / k \zz)}{\rr / k \zz} \ar[r, shift left, "\lens{T\phi}{\phi}"] \ar[r, shift right] \ar[d, shift right,
      "\lens{1}{\id}"'] \ar[d, shift left, leftarrow] &
      \lens{T\State{S}}{\State{S}} \ar[d, shift left, leftarrow,
      "\lens{\update{S}}{\expose{S}}"] \ar[d, shift right]\\
      \lens{{\ast}}{\rr} \ar[r, shift right] \ar[r,
      shift left, "\lens{p}{v}"'] & \lens{\In{S}}{\Out{S}}
    \end{tikzcd}
  \]
  Let $\hat{\phi} : \rr \to \State{S}$ be $\phi \circ q$. First, this being a
  square means that $\expose{S} \circ \phi = v$, or that
  $\expose{S}(\hat{\phi}(t)) = v(t)$. Second, we have that
  \[
\phi^{\ast}\update{S} \circ p = T\phi \circ \update{Clock_k},
  \]
  which is to say
  \[
\update{S}(\phi(t), p(t)) = T\phi(Tq(1)).
\]
Re-expressing this in terms of $\hat{\phi} = \phi \circ q$, we see that this
means that
\[
\update{S}(\hat{\phi}(t), \hat{p}(t)) = \frac{d\hat{\phi}}{dt}.
\]
This says that $\hat{\phi}$ is a trajectory for the system. Since by definition
$\hat{\phi}$ is periodic, and any such periodic map would factor through $q$, we
may conclude that behaviors $\Sys{Clock_k} \to \Sys{S}$ are periodic orbits
(with period dividing $k$) of $\Sys{S}$.
\end{example}



\subsection{Non-deterministic doctrines}

In this section, we will define the non-deterministic doctrines. We've already
done most of the work for this back in \cref{chapter.2}. In particular, in
\cref{thm.bikleisli_indexed_cat}, we showed that to every commutative monad $M :
\cat{C} \to \cat{C}$ on a cartesian category, there is a monoidal strict indexed
category
$$\smctx{}^M : \cat{C}\op \to \Cat{Cat}$$
sending each object $C \in \cat{C}$ to the category $\smctx{C}^M$ of Klesli maps
$C \times X \to MY$ in the context of $C$. We will take this to be the indexed
category underlying the doctrine of non-deterministic systems associated to $M$;
it remains to construct a section $T$.

\begin{proposition}\label{prop.section_bikliesli}
The assigment defined by 
\begin{itemize}
  \item $C \in \cat{C}$ is assigned to $C \in \smctx{C}^M$, and
\item $f : C' \to C$ is assigned to 
$$C' \times C' \xto{\pi_2 \then f \then \eta} MC$$
\end{itemize}
yields a section $T : \cat{C} \to \smctx{-}^M$.
\end{proposition}
\begin{proof}
We see immediately that $T\id_C = \id_{TC}$. It remains to show that for $f : C'
\to C$ and $g : C'' \to C'$, we have 
$$Tg \then g^{\ast} Tf = T(g \then f).$$
In the do notation, the composite on the left is given by 
\[
(c''_1, c''_2) \mapsto \unalignedDo{
  {$c' \from \eta(g(c''_2))$},
  {$\eta(f(c'))$},
} = \eta(f(g(c''_2 )))
\]
which is the right hand side.
\end{proof}

\begin{definition}\label{def.non-det_doctrine}
  Let $M : \cat{C} \to \cat{C}$ be a commutative monad on a cartesian category
  $\cat{C}$. The \emph{$M$-flavored non-deterministic doctrine} $\nondeterm_M$
  is defined to be the indexed category $\smctx{-}^M : \cat{C}\op \to \Cat{Cat}$
  together with section defined in \cref{prop.section_bikliesli}. 
\end{definition}

The non-deterministic doctrine $\nondeterm_M$ answers the questions of
\cref{informal.doctrine} in the following way (taking $\cat{C} = \smset$ for concreteness):
\begin{enumerate}
  \item A state is an element of a set.
  \item The output is a deterministic function of the state.
  \item The kind of input does not depend on the output. 
  \item A possible change in a state is given by an $M$-distribution over states
    (element of
    $M\State{S}$).
  \item A state changes by transitioning into another state. 
  \item The changes in state vary arbitrarily with input.
\end{enumerate}

\begin{exercise}
Answer these questions more specifically for the following doctrines:
\begin{enumerate}
  \item The non-deterministic Moore machine doctrine $\nondeterm_{\powset}$.
  \item The probabilistic doctrine $\nondeterm_{\probset}$.
\item The worst-case cost doctrine $\nondeterm_{\Cat{Cost}}$.
\end{enumerate}
\end{exercise}

Behaviors in non-deterministic doctrines tend to be a little strict. This is
because the notion of trajectory is a bit more subtle in the non-deterministic
case. When does a sequence $s : \nn \to \State{S}$ constitute a trajectory of
the system $\Sys{S}$? Is it when $s_t$ \emph{will} transition to $s_{t +1}$ (in
that $\update{S}(s_t, i_t) = \eta(s_{t+ 1})$)? Or perhaps when it \emph{can}
transition that way --- but how do we express this notion in general?

While the notion of behavior we have given in this chapter works well for
deterministic and differential doctrines, it does not work as well for
non-deterministic doctrines. Instead of asking whether or not a sequence of
states is a trajectory, we might instead want to ask how possible or likely it is
for such a sequence of states to occur through an evolution of the system.
Figuring out how to express this idea nicely and generally remains future work.

On the other hand, simulations of non-deterministic doctrines remain
interesting, because they tell us when we might be able to use a simpler model
for our system without changing the exposed behavior.

\jaz{I should include an example of non-deterministic simulation here.}

\section{Restriction of Doctines}\label{sec.restriction_of_doctrines}

Now that we have a concise, formal definition of dynamical system doctrine, we
can begin to treat doctrines as mathematical objects. In this section, we will
look at a simple way to construct a new doctrine from an old one:
\emph{restriction} along a functor. 

We will use restrictions of doctrines in
order to more precisely control some of the upcoming functoriality results.
Often, we will only be able to prove a theorem by restricting the doctrine beforehand.


Since a doctrine $\dd$ consists of an indexed category $\cat{A} : \cat{C}\op \to
\Cat{Cat}$ together with a section $T$, if we have a functor $F : \cat{D} \to
\cat{C}$ then we should be able to produce a new doctrine by composing $\cat{A}$
and $T$ with $F$. We call this new doctrine $\dd|_F$ the \emph{restriction} of
$\dd$ along $F$.

\begin{definition}\label{def.restriction_doctrine}
Let $\dd = (\cat{A} : \cat{C}\op \to \Cat{Cat}, T)$ be a doctrine of dynamical
systems. For any functor $F : \cat{D} \to \cat{C}$, we have a new doctrine

$$\dd|_F := (\cat{A} \circ F\op, T \circ F)$$

where $T \circ F$ is the section given by
\begin{itemize}
  \item $(T \circ F)(D) := T(FD)$, and
  \item $(T \circ F)(f) := T(Ff)$, which may see has the correct codomain since
    $(\cat{A} \circ F\op)(f)(T \circ F)(D) = \cat{A}(Ff)T(FD)$.
\end{itemize}
\end{definition}

Since an indexed category is no more than a functor into $\Cat{Cat}$, $\cat{A}
\circ F\op$ is an indexed category. It only remains to check that $T \circ F$ as
defined is indeed a section of the Grothendieck construction of $\cat{A} \circ
F\op$; this calculation is a straightforward unfolding of definitions.

\begin{example}\label{ex.affine_restriction_euclidean}
In the next chapter we will see a few approximation methods as ways of changing
doctrine. 

 However, these approximations do not preserve all features of the
doctrines; in general, they are only exact for a restricted class of functions. For example, the Euler method which approximates a differential
equation
$$\frac{ds}{dt} = F(s, p)$$
on a Euclidean space by the discrete-time update function 
$$u(s, p) = s + \epsilon F(s, p)$$
only exactly reproduces \emph{affine} behaviors of systems. Being affine is a
rather severe restriction on the behavior of a dynamical system, but it does
allow the important case of steady states.

In order to capture Euler approximation as a change of doctrine in the exact
manner to be explored in the next chapter, we therefore need to restrict the
Euclidean differential doctrine $\eucdiff$ to \emph{affine} functions. Recall
that the indexing base of $\eucdiff$ is the category $\Cat{Euc}$ of Euclidean
spaces and differentiable functions. We may therefore take our restriction functor to be
the inclusion $\Cat{Aff} \hookrightarrow \Cat{Euc}$ of affine functions between
Euclidean spaces.
\end{example}

Now that we have the formalities out of the way, let's understand what
restricting a doctrine means for the theory of systems in it. Because we have
changed the indexing base for the doctrine, we have changed the objects of
states and exposed variables, and the bottom part of both the lenses and charts. 

In particular, the object of states of a $\dd|_F$-doctrine is now an object of
$\cat{D}$ and not of $\cat{C}$. The exposed variable $\expose{S} : \State{S} \to
\Out{S}$ is now a map in $\cat{D}$. Furthermore, and rather drastically, the underlying map $\phi :
\State{T} \to \State{S}$ of a behavior is also a map in $\cat{D}$.

\begin{example}
Continuing from \cref{ex.affine_restriction_euclidean}, we may consider what a
behavior represented by the system $\Sys{T} = \lens{t \mapsto 1}{\id} : \lens{\rr}{\rr} \tto
\lens{\ord{1}}{\rr}$ is in the restricted doctrine $\eucdiff|_{\Cat{Aff}}$.

Since $\Sys{T}$ represents trajectories in $\eucdiff$, it will represent
trajectories in $\eucdiff|_{\Cat{Aff}}$. However, we have restricted the
underlying map $s : \rr \to \State{S}$ to lie in $\Cat{Aff}$ --- that is, to be
affine. There are not often affine solutions to general differential equations,
so for the most part we will simply find that a system $\Sys{S}$ has \emph{no}
trajectories (in this restricted doctrine), or very few.

However, any constant function is affine; for this reason, all steady states are
affine functions, and so remain behaviors in this restricted doctrine.
\end{example}

\end{document}